<!DOCTYPE html>

<!-- saved from url=(0051)./vistamodels.html -->
<html lang="en"><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
<meta content="" name="description"/>
<meta content="" name="author"/>
<link href="/images/adicionales/favicon.ico" rel="icon"/>
<title>ISP - Software: VistaModels</title>
<!-- Bootstrap core CSS -->
<link href="./vistamodels_files/bootstrap.min.css" rel="stylesheet"/>
<!-- Bootstrap theme -->
<link href="./vistamodels_files/bootstrap-theme.min.css" rel="stylesheet"/>
<!-- Custom styles for this template -->
<!-- <link href="theme.css" rel="stylesheet"> -->
<!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
<!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
<!-- Local styles -->
<link href="./vistamodels_files/styles.css" rel="stylesheet"/>
</head>
<body role="document" style="padding-top: 50px;">
<div class="row" id="Top">
<hr/>
</div>
<div class="container">
<div class="row"> <!------------------------------ TITLE --------------------------------------->
<div class="col-md-3">
</div>
<div class="col-md-6" style="width: 52%;text-align: left; margin-top: 0%; margin-left: 0%; margin-right: auto;">
<p><left></left></p><h1><span class="label label-info">VistaModels: </span></h1><p></p>
<p><left></left></p><h2><span class="label label-info"><span style="font-style: italic;">Computational models of Visual Neuroscience</span></span></h2><p></p><br/>
</div>
<div class="col-md-3">
</div>
</div>
<div class="row"> <!--------------------------------------- PICTURE - ABSTRACT + INDEX - PICTURE --------------------------------------------->
<!--------------------------------------- IZQUIERDA --------------------------------------------->
<!--------------------------------------- IZQUIERDA --------------------------------------------->
<!--------------------------------------- IZQUIERDA --------------------------------------------->
<div class="col-md-3">
<div class="container" style="width: 120%;text-align: left; margin-top: -5%; margin-left: -30%; margin-right: auto;">
<img alt="Jesus" class="img-responsive" src="/images/adicionales/VistaModels1.webp"/>
<p align="justify"><strong>Mechanistic Models:</strong> <small>Following Hubel-Wiesel and McCulloch-Pitts, our models are cascades of two basic elements: (a) a linear transform (not necessarily convolutional set of receptive fields), and (b) a nonlinear saturation (either divisive or subtractive) describing the interactions between the linear units. We have played with different versions of such elements. For the linear part we explored center-surround units, local-DCTs, Orthonormal Wavelets, Overcomplete Wavelets and Laplacian Pyramids. For the nonlinear part played with different adaptive nonlinearities such as the Divisive Normalization and the subtractive Wilson-Cowan equations. See [<a href="https://arxiv.org/abs/1711.00526">PLoS 2018</a>] for a comprehensive account of the maths, and
              [<a href="https://arxiv.org/abs/1804.05964">ArXiV 2018</a>] for the equivalence between the considered nonlinear models. These models have been tuned to reproduce basic psychophysics such as contrast response curves and subjective image distortion.</small></p><br/><br/><br/><br/><br/>
</div>
</div>
<!--------------------------------------- CENTRO --------------------------------------------->
<!--------------------------------------- CENTRO --------------------------------------------->
<div class="col-md-6" style="width: 50%;text-align: left; margin-top: 0%; margin-left: -1%; margin-right: auto;">
<p align="justify"> The Toolboxes in the <strong>VistaModels</strong> site are organized in three categories of different nature: <a href="./vistamodels.html#Empirical"><strong>(a) Empirical-mechanistic Models</strong></a>, tuned to reproduce basic phenomena of color and texture perception, <a href="./vistamodels.html#EfficientCoding"><strong>(b) Principled Models</strong></a>, derived from information theoretic arguments, and <a href="./vistamodels.html#Flow"><strong>(c) Engineering-motivated Models</strong></a>, developed to address applied problems in image and video processing.<br/>

            The algorithms in <strong>VistaModels</strong> require the standard building blocks provided in the (more basic) toolboxes <a href="./vistalab.html"><strong>VistaLab</strong></a> and <a href="./colorlab.html"><strong>ColorLab</strong></a>. However, the necessary functions from these more basic toolboxes are included in the packages listed below for the user convenience.</p><br/>
<div class="list-group">
<a class="list-group-item" href="./vistamodels.html#download">
<h4 class="list-group-item-heading" style="color:rgb(255,50,50)"><strong>Download Toolboxes!</strong></h4>
</a>
<a class="list-group-item" href="./vistamodels.html#Empirical">
<h4 class="list-group-item-heading" style="color:rgb(0,150,200)"><strong>(A) Empirical-mechanistic Models:</strong><br/><br/>
                       * V1_model_DCT_DN_color</h4>
<ul>
<li>Linear transform: YUV chromatic channels and local-DCT</li>
<li>Nonlinear transform: Divisive Normalization (between frequencies in a single spatial region)</li>
</ul>
</a>
<a class="list-group-item" href="./vistamodels.html#Empirical">
<h4 class="list-group-item-heading" style="color:rgb(0,150,200)">* V1_model_wavelet_DN_color</h4>
<ul>
<li>Linear transform: YUV chromatic channels and Orthogonal Wavelets</li>
<li>Nonlinear transform: Divisive Normalization (intraband only)</li>
</ul>
</a>
<a class="list-group-item" href="./vistamodels.html#Empirical">
<h4 class="list-group-item-heading" style="color:rgb(0,150,200)">* BioMultiLayer_L_NL_color</h4>
<ul>
<li>Biologically plausible 4-layer network (linear+nonlinear cascade)</li>
<li>It may start from spectral images (3-d arrays)</li>
<li>Adaptive LMS to ATD transform including Von-Kries adaptation</li>
<li>Achromatic and chromatic Weber-like saturation</li>
<li>Computes contrast and applies CSFs of the Standard Spatial Observer</li>
<li>Global Divisive Normalization in the spatial domain</li>
<li>Overcomplete wavelets</li>
<li>General Divisive Norm. (intra-and-inter band) in overcomplete wavelets</li>
</ul>
</a>
<a class="list-group-item" href="./vistamodels.html#test">
<h4 class="list-group-item-heading" style="color:rgb(0,150,200)">* Psychophysical test-bed for model tuning and comparison</h4>
<ul>
<li>Achromatic and chromatic Contrast Sensitivity Functions</li>
<li>Contrast response curves</li>
<li>Noise visibility on natural color images</li>
</ul>
</a>
<a class="list-group-item" href="./vistamodels.html#EfficientCoding">
<h4 class="list-group-item-heading" style="color:rgb(0,150,200)"><strong>(B) Principled Models:</strong><br/><br/>
                       * Efficient coding in mechanistic models</h4>
<ul>
<li>Univariate equalization in luminance and contrast</li>
<li>PDF factorization using psychophysical Divisive Normalization</li>
<li>Optimization of linear and nonlinear transform for efficient coding</li>
</ul>
</a>
<a class="list-group-item" href="./vistamodels.html#StatLinear">
<h4 class="list-group-item-heading" style="color:rgb(0,150,200)">* Statistically-based linear receptive fields</h4>
<ul>
<li>Spatio-spectral sampling from Principal Component Analysis</li>
<li>Receptive fields in phase quadrature from linear Complex ICA</li>
<li>Adaptive spatio-chromatic receptive fields from Higher-Order CCA</li>
<li>Failure of Topographic ICA in reproducing V1 orientation domains</li>
</ul>
</a>
<a class="list-group-item" href="./vistamodels.html#StatNonlinear">
<h4 class="list-group-item-heading" style="color:rgb(0,150,200)">* Statistically-based nonlinearities</h4>
<ul>
<li>Color, texture and motion sensors (nonlinearities and aftereffects) from Sequential Principal Curves Analysis</li>
<li>Color sensors from Principal Polynomial Analysis</li>
<li>Color discrimination and object-induced receptive fields from Gaussianization</li>
<li>Texture discrimination from Fisher Information</li>
</ul>
</a>
<a class="list-group-item" href="./vistamodels.html#Flow">
<h4 class="list-group-item-heading" style="color:rgb(0,150,200)"><strong>(C) Engineering-motivated Models:</strong><br/><br/>
                       * Perceptually-weighted optical flow: VistaVideoCoding</h4>
</a>
<a class="list-group-item" href="./vistamodels.html#Coding">
<h4 class="list-group-item-heading" style="color:rgb(0,150,200)">* Subjective Image Coding and Restoration</h4>
<ul>
<li>Transform coding using V1_model_DCT_DN_color (<span style="color:rgb(0,150,200)"> VistaCoRe </span>) </li>
<li>Image regularization using V1_model_DCT_DN_color (<span style="color:rgb(0,150,200)"> VistaCoRe </span>) </li>
<li>Image denoising from the statistics at V1 representation (<span style="color:rgb(0,150,200)"> VistaCoRe </span>) </li>
<li>Image enhancement via Laplacian+Div.Norm. (<span style="color:rgb(0,150,200)"> Laplacian+DN </span>) </li>
</ul>
</a>
<a class="list-group-item" href="./vistamodels.html#Quality">
<h4 class="list-group-item-heading" style="color:rgb(0,150,200)">* Subjective Image and Video quality: VistaQualityTools</h4>
</a>
<a class="list-group-item" href="./vistamodels.html#download">
<h4 class="list-group-item-heading" style="color:rgb(255,50,50)"><strong>Download Toolboxes!</strong></h4>
</a>
<a class="list-group-item" href="./vistamodels.html#references">
<h4 class="list-group-item-heading" style="color:rgb(0,150,200)"><strong>Citation and References</strong></h4>
</a>
</div>
</div>
<!--------------------------------------- DERECHA --------------------------------------------->
<!--------------------------------------- DERECHA --------------------------------------------->
<div class="col-md-3">
<div class="container" style="width: 140%;text-align: right; margin-left: -5%; margin-right: auto;">
<div class="row">
<div class="col-md-12" style="width: 100%;text-align: right; margin-left: 0%; margin-right: auto;">
<img alt="" class="img-responsive" src="/images/adicionales/VistaModels2.webp"/><br/>
<p ;style="margin-top: -0%;text-align: left; margin-left: 15%" align="justify"><strong>Statistical Principles:</strong> <small>The emergence of (a) specific sensors (e.g. the red and green curves), or (b) specific discrimination properties (ellipsoids in gray) may be understood as an adaptation to the statistics of natural input (samples in blue). We have used these
                                             <a href="https://www.youtube.com/watch?v=cv9hje42i_E">Barlow-style information-theoretic priciples</a>
                                             in two ways: unfolding the data manifolds [<a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/LaparraMalo15.pdf">Front. Human Neurosci. 15</a>], and Gaussianizing the data manifolds [<a href="https://arxiv.org/abs/1602.00229">IEEE Trans. Neur. Nets. 11</a>]. <br/>
                                             Interestingly, nonlinearities of the Human Visual System (from retina [<a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/JOPT95.PS.gz">J.Opt.95</a>] to cortex [<a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/ivc99.ps.gz">Im.Vis.Comp.00</a>,<a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/Malo_Laparra_Neural_10b.pdf"> Neural Comp.10</a>]) have remarkable statistical effects too!.</small></p>
</div>
</div>
</div>
</div>
</div>
<!-- %%%%%%%%%%%%%%%%%%%%% CUANDO PASE EL CHAPARRON VUELVE A INTRODUCIR EL NIVEL <h2> EN VERDE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
<!-- %%%%%%%%%%%%%%%%%%%%% CUANDO PASE EL CHAPARRON VUELVE A INTRODUCIR EL NIVEL <h2> EN VERDE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
<!-- %%%%%%%%%%%%%%%%%%%%% CUANDO PASE EL CHAPARRON VUELVE A INTRODUCIR EL NIVEL <h2> EN VERDE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
<!-- %%%%%%%%%%%%%%%%%%%%% CUANDO PASE EL CHAPARRON VUELVE A INTRODUCIR EL NIVEL <h2> EN VERDE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
<div class="row">
<div class="col-md-3">
<!-- %%%%%%%%%%%%%%%%%%%%% Nadie a la izquierda %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
</div>
<div class="col-md-6" style="width: 60%;text-align: left; margin-left: -10%; margin-right: auto;">
<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% EMPIRICAL MODELS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
<hr id="Empirical"/><br/><br/>
<a href="./vistamodels.html#Top"><small>Back to top</small></a>
<p align="justify"></p><h2><span class="label label-success">(A)    Empirical-mechanistic Models</span></h2><p></p>
<p align="justify">Cascades of linear transforms and nonlinear saturations are ubiquitous in neuroscience and artificial intelligence ever since the <a href="http://www.scholarpedia.org/article/Models_of_visual_cortex">McCulloch-Pitts model</a>.
               More recently this has been exemplified in subtractive and divisive models of cortical interaction [Wilson&amp;Cowan, Kybernetik 73; Carandini and Heeger, Nature Rev. Neurosci. 12].<br/>
               Over the years we have developed progressively better versions of such cascades to be applicable to color images and video sequences. In this way they can be used to predict complex phenomena happening in natural scenes and to improve results in image processing and computer vision. These parametric models were empirically tuned to give a rough description of different color and texture perception phenomena (see the <a href="./vistamodels.html#test">psychophysical test-bed</a> below for model tuning and comparison).<br/>
               See a visual example of the effect of the local spatial-frequency transforms and the divisive normalization below (illustration of the 2018 model)</p>
<p align="justify"></p><h3><span class="label label-warning">1995 - 2008: Linear opponent color channels, local-DCT and Divisive Normalization</span></h3><p></p>
<p align="justify">This model is invertible and was originally tuned to reproduce contrast response curves obtained from contrast incremental thresholds [Pons PhD Thesis, 1997].
               It was applied to reproduce subjective distortion opinion [<a href="https://www.sciencedirect.com/science/article/abs/pii/S0262885696000042">Im.Vis.Comp.97</a>,<a href="https://www.sciencedirect.com/science/article/abs/pii/S0141938299000098">Displays 99</a>] and to improve the perceptual quality of JPEG and MPEG through (a) transform coding of the achromatic channel [<a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/ELECT95.PS.gz">Elect.Lett.95</a>, <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/ELECT99.PS.gz">Elect.Lett.99</a>, <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/ivc99.ps.gz">Im.Vis.Comp.00</a>,
               <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/ieeeoct01.pdf">IEEE TIP 01</a>,
               <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/patt_rec03.pdf">Patt.Recog.03</a>,
               <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/SVM_JND8_ACCEPTED.pdf">IEEE TNN 05</a>,
               <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/manuscript4.pdf">IEEE TIP 06a</a>,
               <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/camps_JMLR_08.pdf">JMLR08</a>], (b) the color channels [<a href="https://www.eurekaselect.com/96168/article">RPSP12</a>], and (c) by improving the motion estimation
               [<a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/LNCS97.PS.gz">LNCS97</a>,
               <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/ELECT98.PS.gz">Elect.Lett.98</a>,
               <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/seg_ade2.ps">Elect.Lett.00a</a>,
               <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/elect00.ps">Elect.Lett.00b</a>,
               <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/ieeeoct01.pdf">IEEE TIP 01</a>].</p>
<p align="justify"><strong>Download the Toolbox!:</strong><strong><a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/V1_model_DCT_DN_color.zip"> V1_model_DCT_DN_color.zip (74MB) </a></strong><a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/V1_model_DCT_DN_color.zip"> </a> <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/V1_model_DCT_DN_color.zip"><img alt="mat" src="/images/adicionales/matlab_ico.gif" style="border: 0px solid ; width: 21px; height: 20px;"/></a></p>
<p align="justify"></p><h3><span class="label label-warning">2009 - 2010: Linear opponent color channels, Orthogonal Wavelet and Divisive Normalization</span></h3><p></p>
<p align="justify">Even though we developed our own Matlab code for some specific overcomplete wavelets in the mid 90's [<a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/msc_jmalo.zip">MSc Thesis 95</a>,
               <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/JMO97.PS.gz">J.Mod.Opt.97</a>], it took some time until we applied the Divisive Normalization interaction to Simoncelli's wavelets in MatlabPyrTools (which are substantially more efficient). The model was fitted to reproduce subjective image distortion opinion
               [<a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/Laparra_JOSA_10.pdf">JOSA A 10</a>] following exhaustive grid search as in
               [<a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/icip02.pdf">IEEE ICIP 02</a>]. This model (which relies on the orthogonal wavelets of the MatlabPyrTools) was found to have excellent redundancy reduction properties
               [<a href="https://link.springer.com/chapter/10.1007/978-3-642-11509-7_3">LNCS10</a>,<a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/Malo_Laparra_Neural_10b.pdf">Neur.Comp.10</a>].</p>
<p align="justify"><strong>Download the Toolbox!:</strong><strong><a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/V1_model_wavelet_DN_color.zip"> V1_model_wavelet_DN_color.zip (14MB) </a></strong><a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/V1_model_wavelet_DN_color.zip"> </a> <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/V1_model_wavelet_DN_color.zip"><img alt="mat" src="/images/adicionales/matlab_ico.gif" style="border: 0px solid ; width: 21px; height: 20px;"/></a></p>
<p align="justify"></p><h3><span class="label label-warning">2013 - 2018: Multi-Layer network with nonlinear opponent color, Overcomplete Wavelet and Divisive Normalization</span></h3><p></p>
<p align="justify">Even though we developed a comprehensive color vision toolbox in the early 2000's (see <a href="./colorlab.html">ColorLab </a>), it took some time until we included a fully adaptive chromatic front-end before the spatial processing models based on overcomplete wavelets. Note that the older toolboxes rely on (too rough) linear RGB to YUV transforms. This multi-layer model (or biologically-plausible deep network) performs the following chain of perceptually meaningful operations
               [<a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0201326">PLoS 18</a>]: </p>
<table border="0" cellpadding="2" cellspacing="2" style="width: 100%; text-align: left; margin-left: auto; margin-right: auto;">
<tbody>
<tr align="center"><td colspan="2" rowspan="1">
<img alt="a" class="img-responsive" src="/images/adicionales/modelB.webp"/>
</td>
</tr>
</tbody>
</table>
<br/>
<p align="justify">The parameters of the different layers were fitted in different ways: while the 2nd and 3rd layers (contrast and CSF+masking) were determined using MAximum Differentiation [<a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/malo15a-reprint.pdf">Malo and Simoncelli SPIE 15</a>], layers 1st and 4th (chromatic front-end and wavelet layer) were fitted to reproduce subjective image distortion data [<a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0201326">PLoS 18</a>], and then fine-tuned to reproduce classical masking [<a href="https://www.frontiersin.org/articles/10.3389/fnins.2019.00008/full">Front. Neurosci. 19</a>].</p>
<p align="justify"><strong>Download the Toolbox!:</strong><strong><a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/BioMultiLayer_L_NL_color.zip"> BioMultiLayer_L_NL_color.zip (49MB) </a></strong><a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/BioMultiLayer_L_NL_color.zip"> </a> <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/BioMultiLayer_L_NL_color.zip"><img alt="mat" src="/images/adicionales/matlab_ico.gif" style="border: 0px solid ; width: 21px; height: 20px;"/></a></p>
<p align="justify"></p><h3><span class="label label-warning">2019 - 2021: Convolutional and differentiable implementations</span></h3><p></p>
<p align="justify">The matrix formulation developed in [<a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0201326">PLoS 18</a>, <a href="https://www.frontiersin.org/articles/10.3389/fnins.2019.00008/full">Front. Neurosci. 19</a>] and implemented in <strong>BioMultiLayer_L_NL_color</strong> is elegant but not applicable to large images nor appropriate to be included in python deep-learning schemes since it is implemented in Matlab. Recently we worked to solve these issues and confirm the choices of the chromatic part. This led to the deep Percepnet [<a href="https://ieeexplore.ieee.org/document/9190691">IEEE ICIP 20</a>], and to the convolutional version the above MultiLayer L+NL cascade [J.Vision, Proc. VSS 2021]. While <strong>Percepnet</strong> has the advantage of being implemented in python and hence ready for automatic differentiation (state-of-the-art in image quality), it has the disadvantage of being based on a restricted version of Divisive Normalization (no explicit interactions in space/scale) [<a href="https://openreview.net/forum?id=rJxdQ3jeg">ICLR 17</a>]. On the other hand, the <strong>BioMultiLayer_L_NL_color_convolutional</strong> has a more general and interpretable version of the Divisive Normalization (in includes full range of interactions in space/scale/orientation). Moreover, the color adaptation choices and the scaling of the achromatic and chromatic channels has been confirmed by positive psychophysical and statistical behaviors [<a href="https://journals.physiology.org/doi/abs/10.1152/jn.00487.2019">J. Neurophysiol.19</a>, <a href="https://mathematical-neuroscience.springeropen.com/articles/10.1186/s13408-020-00095-8">J. Math.Neurosci.20</a>]. However, derivatives are implemented in matlab, so it is not ready to be included in deep-learning schemes right away. There is a lot of room for improvement of its parameters!.</p>
<p align="justify"><strong>Download the Toolbox!:</strong><strong><a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/BioMultiLayer_L_NL_color_convolutional.zip"> BioMultiLayer_L_NL_color_convolutional.zip (76MB) </a></strong><a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/BioMultiLayer_L_NL_color_convolutional.zip"> </a> <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/BioMultiLayer_L_NL_color_convolutional.zip"><img alt="mat" src="/images/adicionales/matlab_ico.gif" style="border: 0px solid ; width: 21px; height: 20px;"/></a></p>
<p align="justify"><strong>Visit Github!:</strong><strong><a href="https://github.com/alexhepburn/perceptnet"> Perceptnet </a></strong><a href="https://github.com/alexhepburn/perceptnet"> </a> <a href="https://github.com/alexhepburn/perceptnet"><img alt="python" src="/images/adicionales/python.webp" style="border: 0px solid ; width: 21px; height: 20px;"/></a></p>
<p align="justify"><strong>Statistical and Psychophysical support for the chromatic choices (I/II):</strong> The small scale model in this paper uses the same chromatic choices as the BioMultiLayer_L_NL models.<strong><a href="./infoWilsonCowan.html"> Code and Data for small scale recurrent Wilson-Cowan network [J.Neurophysiol. 2020] </a></strong><a href="./infoWilsonCowan.html"> </a> <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/infoWilsonCowan.zip"><img alt="mat" src="/images/adicionales/matlab_ico.gif" style="border: 0px solid ; width: 21px; height: 20px;"/></a></p>
<p align="justify"><strong>Statistical and Psychophysical support for the chromatic choices (II/II):</strong>The small scale model in this paper uses the same chromatic choices as the BioMultiLayer_L_NL models.<strong><a href="./infoDN.html"> Code and Data for small scale Div. Norm. [J.Math.Neurosci. 2020] </a></strong><a href="./infoDN.html"> </a> <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/infoWilsonCowan.zip"><img alt="mat" src="/images/adicionales/matlab_ico.gif" style="border: 0px solid ; width: 21px; height: 20px;"/></a></p>
<hr id="test"/><br/><br/>
<a href="./vistamodels.html#Top"><small>Back to top</small></a>
<p align="justify"></p><h3><span class="label label-warning">Psychophysical test-bed for model tuning and comparison</span></h3><p></p>
<p align="justify">The figure below (computed using <a href="./vistalab.html"><strong>VISTALAB</strong></a> and <a href="./colorlab.html"><strong>ColorLab</strong></a>) illustrates distinctive features of early vision:
               (a) the bandwidth of the achromatic and the chromatic channels is markedly different,
               (b) the response to contrast is a saturating nonlinearity, its slope (sensitivity) depends on the frequency and the response attenuates as a function of the properties of the background (note how the test is more salient -highlighted in green- on top of a very different background while it is masked -highlighted red- on top of similar backgrounds),
               and (c) the visibility of i.i.d. noise seen on top of a natural image is not uniform: e.g. visibility is smaller in high contrast regions.<br/>
</p>
<table border="0" cellpadding="2" cellspacing="2" style="width: 150%; text-align: left; margin-left: -20%; margin-right: auto;">
<tbody>
<tr align="center"><td colspan="2" rowspan="1">
<img alt="a" class="img-responsive" src="/images/adicionales/facts1.webp"/>
</td>
</tr>
</tbody>
</table>
<br/>
<table border="0" cellpadding="2" cellspacing="2" style="width: 130%; text-align: left; margin-left: -8%; margin-right: auto;">
<tbody>
<tr align="center"><td colspan="2" rowspan="1">
<img alt="a" class="img-responsive" src="/images/adicionales/visib_noise.webp"/>
</td>
</tr>
</tbody>
</table>
<br/>
<p align="justify">These quite visible facts can be used to tune the parameters of the mechanistic models considered above. One could play with the parameters by hand until the response curves qualitatively reproduce what one actually sees.
               We suggested this idea to improve model fit in natural image databases [Front.Neurosci.18] and (for the first time!) here is data and code to perform such tune-it-yourself experiments:
               <strong><a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/experiments_VistaModels.zip"> experiments_VistaModels.zip (400MB) </a></strong><a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/experiments_VistaModels.zip"> </a> <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/experiments_VistaModels.zip"><img alt="mat" src="/images/adicionales/matlab_ico.gif" style="border: 0px solid ; width: 21px; height: 20px;"/></a></p>
<p align="justify">File is huge because it contains thousands of tests to compute detailed contrast response curves and distortion measures on the TD database. Moreover, it also has the corresponding responses of the three mechanistic models!.</p>
<p align="justify">Results below suggest that models are equivalent but the most recent displays better behavior (on top of having more plausible receptive fields [PLoS18]). More importantly, while the results on Image Quality are way better than the popular Structural Similarity Index SSIM (see <strong><a href="../../soft_imvideo/subpages/vista_toolbox.html"> VistaQualityTools </a></strong><a href="../../soft_imvideo/subpages/vista_toolbox.html"> </a>) there is still a lot of room for improvement through these tune-it-yourself experiments!.</p>
<p align="justify"></p><h3><span class="label label-warning">Model Comparison</span></h3><p></p>
<table border="0" cellpadding="2" cellspacing="2" style="width: 150%; text-align: left; margin-left: -20%; margin-right: auto;">
<tbody>
<tr align="center"><td colspan="2" rowspan="1">
<img alt="a" class="img-responsive" src="/images/adicionales/compCSFs.webp"/>
</td>
</tr>
</tbody>
</table>
<hr/><hr/>
<table border="0" cellpadding="2" cellspacing="2" style="width: 170%; text-align: left; margin-left: -20%; margin-right: auto;">
<tbody>
<tr align="center"><td colspan="2" rowspan="1">
<img alt="a" class="img-responsive" src="/images/adicionales/compResponses.webp"/>
</td>
</tr>
</tbody>
</table>
<hr/><hr/>
<table border="0" cellpadding="2" cellspacing="2" style="width: 150%; text-align: left; margin-left: -20%; margin-right: auto;">
<tbody>
<tr align="center"><td colspan="2" rowspan="1">
<img alt="a" class="img-responsive" src="/images/adicionales/compNoise.webp"/>
</td>
</tr>
</tbody>
</table>
<hr id="EfficientCoding"/><br/><br/>
<a href="./vistamodels.html#Top"><small>Back to top</small></a>
<p align="justify"></p><h2><span class="label label-success">(B)   Principled Models</span></h2><p></p>
<p align="justify"></p><h3><span class="label label-warning">Efficient coding in mechanistic models</span></h3><p></p>
<p align="justify">We have shown that models including point-wise Weber-like saturation for brightness lead to decreasing signal-to-noise ratio as a function of the luminance [J.Opt.95].
               Moreover, taking into account more general cascades of linear+nonlinear layers (e.g. local-frequency transforms and divisive normalization after Weber-brightness) we have seen that
               the efficiency of such systems (in terms of redundancy reduction) decreases with luminance and contrast, which is consistent with the distribution of natural images in local frequency domains [PLoS 18].
               We have seen that the discrimination ability of Local-DCT+Div.Norm. models is bigger in the more populated regions of the frequency-amplitude domain [Im.Vis.Comp.97].
               Additionally, we have seen that the mutual information between the coefficients of the image representation progressively reduces from the retina to the
               normalized representation, both in the local-DCT + DN case [IEEE TIP 06] and in the Orthogonal wavelet+DN case [Neur.Comp.10].</p>
<table border="0" cellpadding="2" cellspacing="2" style="width: 150%; text-align: left; margin-left: -20%; margin-right: auto;">
<tbody>
<tr align="center"><td colspan="2" rowspan="1">
<img alt="a" class="img-responsive" src="/images/adicionales/principled.webp"/>
</td>
</tr>
</tbody>
</table>
<br/>
<p align="justify">The above body of results means that the Mechanistic Models considered above display remarkable adaptation to the natural image statistics.</p>
<p align="justify">In the same line, in collaboration with NYU (Balle and Simoncelli) we have optimized the described linear+nonlinear architectures for optimal autoencoding.
               By including both the linear and the nonlinear parts in the optimization we get unprecedented rate-distortion performances (see paper and code here [<a href="http://www.cns.nyu.edu/~lcv/iclr2017">ICLR 17</a>]),
               way better than our previous image coders based on V1 models with fixed linear stages (See the <strong><a href="../../soft_imvideo/subpages/kecode.html">VistaCoRe</a></strong> Toolbox).</p>
<table border="0" cellpadding="2" cellspacing="2" style="width: 80%; text-align: center; margin-left: 10%; margin-right: auto;">
<tbody>
<tr align="center"><td colspan="2" rowspan="1">
<img alt="a" class="img-responsive" src="/images/adicionales/autoencoder.webp"/>
</td>
</tr>
</tbody>
</table>
<br/>
<hr id="StatLinear"/><br/><br/>
<a href="./vistamodels.html#Top"><small>Back to top</small></a>
<p align="justify"></p><h3><span class="label label-warning">Statistically-based linear receptive fields</span></h3><p></p>
<p align="justify">Statistical goals such as decorrelation (Principal Component Analysis, PCA), and Independent Component Analysis (ICA) many time lead to sensible linear receptive fields when trained with natural scenes.
               For instance, spatio-spectral PCA leads to compact representations to disentangle reflectance and spectral illumination from retinal irradiance and lead to spatial-frequency sensors with smooth spectral response [IEEE TGRS 13]
               (see <strong><a href="../../soft_imvideo/subpages/spatiospectral.html">VistaSpatioSpectral</a></strong>).
               In collaboration with Helsinki University (Gutman and Hyvarinen) we explored ICA-related techniques.
               Complex ICA led to local and oriented receptive fields in phase quadrature [LNCS11] (download the <strong><a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/CICA_toolbox.zip">Complex ICA</a></strong> Toolbox).
               Higher Order Canonical Correlation Analysis (HOCCA) combines the sparsity goal with optimal correspondence between identified features in domain adaptation problems leading to biologically plausible spatiochromatic receptive fields which adapt to changes in the illumination (PLoS 14, see the <strong><a href="../../soft_feature/subpages/hocca.html">HOCCA</a></strong> Toolbox)<br/>
               This analysis of ICA methods concluded with a refutation of a classical result in cortical organization based on Topographica ICA: in fact
               (as opposed to Hyvarinen &amp; Hoyer Vis. Res. 2001) it <strong>does not</strong> lead to orientation domains [PLoS 17].
               See code and results to <strong><a href="./TICAdomains.html">analyze TICA receptive fields</a></strong>.</p>
<table border="0" cellpadding="2" cellspacing="2" style="width: 150%; text-align: center; margin-left: -30%; margin-right: auto;">
<tbody>
<tr align="center"><td colspan="2" rowspan="1">
<img alt="a" class="img-responsive" src="/images/adicionales/LinearStats.webp"/>
</td>
</tr>
</tbody>
</table>
<br/>
<hr id="StatNonlinear"/><br/><br/>
<a href="./vistamodels.html#Top"><small>Back to top</small></a>
<p align="justify"></p><h3><span class="label label-warning">Statistically-based nonlinearities</span></h3><p></p>
<p align="justify">Instead of optimizing the mechanistic models for efficient coding we tried a stronger approach to test the Efficient Coding Hypothesis:
               use pure data-driven techniques instead of assuming models which already have the right functional form.
               We developed a family of invertible techniques for manifold unfolding and for manifold Gaussianization.<br/>

               The unfolding techniques identify nonlinear sensors that follow curved manifolds. These include Sequential Principal Curves Analysis <strong><a href="../../soft_feature/subpages/spca.html">SPCA</a></strong><a href="../../soft_feature/subpages/spca.html"> </a>
               and sequels:  Principal Polynomial Analysis <strong><a href="../../soft_feature/subpages/ppa.html">PPA</a></strong><a href="../../soft_feature/subpages/ppa.html"> </a> and
               Dimensionality Reduction based on Regression, <strong><a href="../../soft_feature/subpages/drr.html">DRR</a></strong><br/>
 
               The Gaussianization technique (Rotation-Based Iterative Gaussianization, <strong><a href="../../soft_feature/subpages/rbig.html">RBIG</a></strong>)
               does not identify sensors but it allows to compute the PDF. Therefore it is useful to define discrimination
               regions according to information maximization or error minimization.

               See the kind of predictions made by these unfolding techniques (SPCA [Network 06, NeCo12, Front. Human Neurosci.15, ArXiv 16, https://arxiv.org/pdf/1606.00856.pdf],
               and PPA-DRR [SPIE13, Int.J.Neur.Syst.14, IEEE Sel.Top.Sig.Proc.15]) and by the Gaussianization technique
                [<a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/2013_Courant_features_RBIG.pdf">Talk at LeCun Lab NYU 13</a>, IEEE TNN 11].</p>
<table border="0" cellpadding="2" cellspacing="2" style="width: 150%; text-align: center; margin-left: -30%; margin-right: auto;">
<tbody>
<tr align="center"><td colspan="2" rowspan="1">
<img alt="a" class="img-responsive" src="/images/adicionales/ResponsesSPCA1.webp"/>
</td>
</tr>
</tbody>
</table>
<table border="0" cellpadding="2" cellspacing="2" style="width: 150%; text-align: center; margin-left: -30%; margin-right: auto;">
<tbody>
<tr align="center"><td colspan="2" rowspan="1">
<img alt="a" class="img-responsive" src="/images/adicionales/ResponsesSPCA2.webp"/>
</td>
</tr>
</tbody>
</table>
<table border="0" cellpadding="2" cellspacing="2" style="width: 150%; text-align: center; margin-left: -30%; margin-right: auto;">
<tbody>
<tr align="center"><td colspan="2" rowspan="1">
<img alt="a" class="img-responsive" src="/images/adicionales/neuro_rbig.webp"/>
</td>
</tr>
</tbody>
</table>
<p align="justify">Closely related to optimal discrimination (or optimal metric) for error minimization is the concept of Fisher Information.
               Our lab has a tradition in the study of Riemannian metrics induced by nonlinear perception systems [J. Malo PhD 99, Displ.99].
               Over the years, the ideas about the geometrical transforms induced by the system and their effect on information processing have
               evolved from distance computation to the consideration of the transformation of neural noise [Displ.99, Patt.Recog.03, IEEE TIP 06, JOSA A 10, SPIE 15, NIPS 17, PLoS 18].</p>
<table border="0" cellpadding="2" cellspacing="2" style="width: 100%; text-align: center; margin-left: -0%; margin-right: auto;">
<tbody>
<tr align="center"><td colspan="2" rowspan="1">
<img alt="a" class="img-responsive" src="/images/adicionales/metricFisher.webp"/>
</td>
</tr>
</tbody>
</table>
<hr id="Flow"/><br/><br/>
<a href="./vistamodels.html#Top"><small>Back to top</small></a>
<p align="justify"></p><h2><span class="label label-success">(C)   Engineering-motivated</span></h2><p></p>
<p align="justify"></p><h3><span class="label label-warning">Perceptually-weighted motion estimation: VistaVideoCoding</span></h3><p></p>
<p align="justify">What can be predicted is not worth transmitting!. This simple idea is the core of predictive coding used in most successful video coders (e.g. MPEG).
               In predictive coding motion information is the key to predict future-from-past. MPEG-like coders first compute the optical flow (or displacement field) and encode
               the prediction error in a transformed domain which (not surprisingly!) is similar to the  <a href="./vistamodels.html#Empirical"> V1 mechanistic models</a> described above.<br/>

               In this video-coding context we improved motion estimation by connecting the optical flow computation with the perceptual relevance of the
               prediction error: we proposed to improve the resolution of the motion estimate only if the prediction error was hard to encode for our
               improved V1 models [LNCS97, Electr.Lett.98, J.Vis.01].
               This gave rise to smoother motion flows more appropriate for motion-based segmentation [Electr.Lett.00a], and to better video
               coders [Electr.Lett.00b, IEEE TIP 01].<br/><br/>
<strong>Download the motion estimation and video coding toolbox! <a href="../../soft_visioncolor/subpages/video_coding.html">VistaVideoCoding</a></strong>.
               </p>
<table border="0" cellpadding="2" cellspacing="2" style="width: 220%; text-align: center; margin-left: -60%; margin-right: auto;">
<tbody>
<tr align="center"><td colspan="2" rowspan="1">
<img alt="a" class="img-responsive" src="/images/adicionales/flow_800_10_excel.gif"/>
</td>
</tr>
</tbody>
</table>
<br/>
<hr id="Coding"/><br/><br/>
<a href="./vistamodels.html#Top"><small>Back to top</small></a>
<p align="justify"></p><h3><span class="label label-warning">Image Coding: VistaCoRe</span></h3><p></p>
<p align="justify">Image compression requires vision models that rank visual features according to their perceptual relevance
                so that extra bits can be allocated to encode the subjectively important aspects of the image.<br/>
                The vision model based on DCT and Divisive Normalization considered above leads to better decoded images at the same compression ratio than JPEG and variants based on simpler models of masking.<br/>
                See the <strong><a href="../../soft_imvideo/subpages/kecode.html"> VistaCoRe</a></strong> (Coding and Restoration Toolbox), and
                the references [Eletr.Lett95, Eletr.Lett99, Im.Vis.Comp.00 Patt.Recog.03, IEEE TNN 05, IEEE TIP 06a, IEEE TIP 06b, JMLR08].</p>
<table border="0" cellpadding="2" cellspacing="2" style="width: 150%; text-align: center; margin-left: -20%; margin-right: auto;">
<tbody>
<tr align="center"><td colspan="2" rowspan="1">
<img alt="a" class="img-responsive" src="/images/adicionales/coding.webp"/>
</td>
</tr>
</tbody>
</table>
<br/>
<hr id="Quality"/><br/><br/>
<a href="./vistamodels.html#Top"><small>Back to top</small></a>
<p align="justify"></p><h3><span class="label label-warning">Image and Video Quality: VistaQualityTools</span></h3><p></p>
<p align="justify">Computing perceptual distances between images requires vision models that identify relevant and negligible visual features.
                Distortions in features that will be neglected by the observers should induce no effect in the distance. And the other way around for visually relevant features.
                The different models can be quantitatively compared by their accuracy in reproducing the opinion of viewers in subjectively rated databases.<br/>
                The three vision models considered above (based on DCTs, orthonormal wavelets, and overcomplete wavelets) have been used to propose distortion metrics
                that overperform SSIM. See <strong><a href="../../soft_imvideo/subpages/vista_toolbox.html"> VistaQualityTools</a></strong>, and
                the references [Im.Vis.Comp.97, Displays99, Patt.Recog.03, IEEE Trans.Im.Proc.06] for the DCT metric, [JOSA 10, Neur.Comp.10] for the orthogonal wavelet metric, and [PLoS 18, Frontiers Neurosci.18] for the metric based on overcomplete wavelets.</p>
<!--------------------


                 <a href="#Flow" class="list-group-item">
                       <h4 class="list-group-item-heading" style="color:rgb(0,150,200)"><strong>(C) Engineering-motivated Models:</strong><br><br>
                       * Perceptually-weighted optical flow: VistaVideoCoding</h4>
                 </a>
                 <a href="#Coding" class="list-group-item">
                       <h4 class="list-group-item-heading" style="color:rgb(0,150,200)">* Subjective Image Coding and Restoration</h4>
                                <ul>
                                  <li>Transform coding using V1_model_DCT_DN_color (<span style="color:rgb(0,150,200)"> VistaCoRe </span>) </li>
                                  <li>Image regularization using V1_model_DCT_DN_color (<span style="color:rgb(0,150,200)"> VistaCoRe </span>) </li>
                                  <li>Image denoising from the statistics at V1 representation (<span style="color:rgb(0,150,200)"> VistaCoRe </span>) </li>
                                  <li>Image enhancement via Laplacian+Div.Norm. (<span style="color:rgb(0,150,200)"> Laplacian+DN </span>) </li>
                                </ul>
                 </a>
                 <a href="#Quality" class="list-group-item">
                       <h4 class="list-group-item-heading" style="color:rgb(0,150,200)">* Subjective Image and Video quality: VistaQualityTools</h4>
                 </a>

               ----->
<hr id="download"/><br/><br/>
<a href="./vistamodels.html#Top"><small>Back to top</small></a>
<p align="justify"></p><h2><span class="label label-danger">Download VistaModels!</span></h2><p></p>
<ul>
<li> <strong><span style="color:rgb(255,0,0)">Updated Matlab Toolbox (VISTALAB 4.0):</span> <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/Vistalab.zip"> Vistalab.zip (30MB) </a></strong><a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/Vistalab.zip"> </a> <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/Vistalab.zip"><img alt="mat" src="/images/adicionales/matlab_ico.gif" style="border: 0px solid ; width: 21px; height: 20px;"/></a>
</li><li> <strong>Outdated toolbox  (VISTALAB 1.0): <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/BasicVideoTools_code.zip"> BasicVideoTools_code.zip (15MB) </a></strong><a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/BasicVideoTools_code.zip"> </a> <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/BasicVideoTools_code.zip"><img alt="mat" src="/images/adicionales/matlab_ico.gif" style="border: 0px solid ; width: 21px; height: 20px;"/></a> <br/> The first stand alone version of VISTALAB was known as BasicVideoTools. This outdated version is included here only for compatibility with the code in the experiments of the motion-aftereffect <a href="https://www.frontiersin.org/articles/10.3389/fnhum.2015.00557/full">Front. Human Neurosci. 15 paper</a>.
             </li><li> <strong><span style="color:rgb(255,0,0)">Extensions of VISTALAB I: VistaVideoCoding</span> <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/VistaVideoCoding.zip">VistaVideoCoding.zip (60MB) </a></strong><a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/VistaVideoCoding.zip"> </a> <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/VistaVideoCoding.zip"><img alt="mat" src="/images/adicionales/matlab_ico.gif" style="border: 0px solid ; width: 21px; height: 20px;"/></a>
</li><li> <strong><span style="color:rgb(255,0,0)">Extensions of VISTALAB II: VistaModels</span> <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/BioMultiLayer_L_NL_color.zip">BioMultiLayer_L_NL_color.zip (40MB) </a></strong><a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/BioMultiLayer_L_NL_color.zip"> </a> <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/BioMultiLayer_L_NL_color.zip"><img alt="mat" src="/images/adicionales/matlab_ico.gif" style="border: 0px solid ; width: 21px; height: 20px;"/></a>
</li><li> <strong><span style="color:rgb(255,0,0)">Extensions of VISTALAB III: ColorLab</span> <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/Colorlab.zip">Colorlab.zip (15MB) </a></strong><a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/Colorlab.zip"> </a> <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/Colorlab.zip"><img alt="mat" src="/images/adicionales/matlab_ico.gif" style="border: 0px solid ; width: 21px; height: 20px;"/></a>
</li></ul>
<hr id="references"/><br/>
<a href="./vistamodels.html#Top"><small>Back to top</small></a>
<p align="justify"></p><h2><span class="label label-success">Citation and References</span></h2><p></p>
<p align="justify">VistaLab is released free of charge for the scientific community: please cite us when using the software (both the web site and first journal paper that used VistaLab)<br/><br/>
</p><p align="justify"><strong><span style="color:rgb(0,160,0)">WEB: </span></strong></p>
<strong>J. Malo &amp; J. Gutierrez. <br/> VistaLab: the Matlab toolbox for Spatio-Temporal Vision. Univ. Valencia 1997<br/>
<a href="./vistalab.html">./vistalab.html</a></strong><p></p>
<br/>
<p align="justify"><strong><span style="color:rgb(0,160,0)">FIRST PAPER: </span></strong></p>
<strong> Malo, Gutirrez, Epifanio, Ferri,<br/> <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/ELECT98.PS.gz">Perceptually weighted optical flow for motion-based  segmentation in MPEG-4 paradigm.</a> Electr. Lett. 36 (20):1693-1694 (2000)</strong> <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/ELECT98.PS.gz"><img alt="mat" src="/images/adicionales/pdf16x16.gif" style="border: 0px solid ; width: 16px; height: 16px;"/></a><br/><br/>
<p align="justify"><strong><span style="color:rgb(0,160,0)">Other papers: </span></strong></p>
       V. Laparra &amp; J. Malo.<br/> <a href="https://www.frontiersin.org/articles/10.3389/fnhum.2015.00557/full">Visual aftereffects and sensory nonlinearities from a single statistical framework</a> Frontiers in Human Neuroscience 9:557 (2015) <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/LaparraMalo15.pdf"><img alt="mat" src="/images/adicionales/pdf16x16.gif" style="border: 0px solid ; width: 16px; height: 16px;"/></a><br/><br/>
<br/><br/><br/><br/><br/>
<div class="col-md-3">
<!-- %%%%%%%%%%%%%%%%%%%%% Nadie a la derecha %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
</div>
</div>
</div><!-- /.container -->
<!-- Bootstrap core JavaScript
    ================================================== -->
<!-- Placed at the end of the document so the pages load faster -->
<script src="./vistamodels_files/jquery-1.12.4.min.js"></script>
<script src="./vistamodels_files/bootstrap.min.js"></script>
</div></body></html>