<!doctype html><html lang=en-us dir=ltr><head><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>ISP - Image and Signal Processing group</title>
<link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH crossorigin=anonymous><link rel="shortcut icon" href=/images/favicon.ico type=image/x-icon><link rel=stylesheet href=/github/style/style.min.f7dd1c60a6cdba402b3b03b09262e8921a1b461c6157b54a640584ac10ed246e.css integrity="sha256-990cYKbNukArOwOwkmLokhobRhxhV7VKZAWErBDtJG4=" crossorigin=anonymous><script src=/github/js/mode.f2979a93a325fecf9605263bd141398a311c8e23388ed7dcff74f92f7e632866.js integrity="sha256-8peak6Ml/s+WBSY70UE5ijEcjiM4jtfc/3T5L35jKGY=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/marked/marked.min.js></script></head></head><nav class="navbar navbar-expand-lg bg-body-tertiary fixed-top"><div class=container-fluid><a href=/ class="d-lg-none d-flex align-items-center a_logonav"><img src=/images/isp_logo_sinfondo.webp alt="ISP Icon" height=30 class=logo_nav>
<span class="ms-2 text-isp">ISP</span>
</a><button class="navbar-toggler ms-auto" type=button data-bs-toggle=collapse data-bs-target=#navbarTogglerDemo01 aria-controls=navbarTogglerDemo01 aria-expanded=false aria-label="Toggle navigation" style=height:40px>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarTogglerDemo01><ul class="navbar-nav mx-auto mb-lg-0"><li class="nav-item px-2 nav-item-highlight d-none d-lg-block"><a class="nav-link a" aria-current=page href=/github/>ISP</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/github/people/>People</a></li><li class="nav-item dropdown px-2 nav-item-highlight"><a class="nav-link dropdown-toggle a" href=/github/research/philosophy id=navbarDropdownResearch role=button aria-expanded=false>Research</a><ul class=dropdown-menu aria-labelledby=navbarDropdownResearch><li><a class="dropdown-item a" href=/github/research/machine_learning/>Machine learning</a></li><li><a class="dropdown-item a" href=/github/research/visual_neuroscience/>Visual neuroscience</a></li><li><a class="dropdown-item a" href=/github/research/visual_brain/>Visual brain</a></li><li><a class="dropdown-item a" href=/github/research/earth_science/>Earth science</a></li><li><a class="dropdown-item a" href=/github/research/social_science>Social science</a></li></ul></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/github/projects/>Projects</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/github/facilities/>Facilities</a></li><li class="nav-item dropdown px-2 nav-item-highlight"><a class="nav-link dropdown-toggle a" href=/github/publications/journals/ id=navbarDropdownPublications role=button aria-expanded=false>Publications</a><ul class=dropdown-menu aria-labelledby=navbarDropdownPublications><li><a class="dropdown-item a" href=/github/publications/journals/>Journals</a></li><li><a class="dropdown-item a" href=/github/publications/conferences/>Conferences</a></li><li><a class="dropdown-item a" href=/github/publications/books/>Books</a></li><li><a class="dropdown-item a" href=/github/publications/talks/>Talks</a></li><li><a class="dropdown-item a" href=/github/publications/technical_reports/>Technical Reports</a></li><li><a class="dropdown-item a" href=/github/publications/theses/>Theses</a></li></ul></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/github/code/>Code</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/github/data/>Data</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/github/seminars/>Seminars</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/github/courses/>Courses</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/github/collaborators/>Collaborators</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/github/news/>News</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/github/contact/>Contact</a></li></ul></div></div></nav><style>.navbar{--bs-navbar-padding-y:0rem !important;background-color:#222!important}.a{color:#949494!important}.a:hover{color:#fff!important}.nav-item-highlight{padding:.4rem}.nav-item-highlight:hover{background-color:#2d70aa!important}.dropdown-menu{background-color:#333!important;color:#fff!important;display:block}.dropdown-item{color:#949494!important}.dropdown-item:hover{background-color:#2d70aa!important;color:#fff!important}.navbar-nav li.nav-item .nav-link,.dropdown-menu .dropdown-item{transition:color .3s,background-color .3s}.nav-link.active,.dropdown-item.active{color:#fff!important;background-color:#007bff!important}.navbar-toggler-icon{background-image:url("data:image/svg+xml,%3csvg viewBox='0 0 30 30' xmlns='http://www.w3.org/2000/svg'%3e%3cpath stroke='white' stroke-width='2' stroke-linecap='round' stroke-miterlimit='10' d='M4 7h22M4 15h22M4 23h22'/%3e%3c/svg%3e")}.navbar-toggler{border:none}.navbar-toggler:focus{outline:none}@media(min-width:992px){.dropdown-menu{display:none}.dropdown:hover .dropdown-menu{display:block}}@media(max-width:991px){.navbar-nav{max-height:calc(100vh - 56px);overflow-y:auto}.navbar-nav::-webkit-scrollbar{width:9px}.navbar-nav::-webkit-scrollbar-thumb{background-color:#888;border-radius:10px}.navbar-nav::-webkit-scrollbar-thumb:hover{background-color:#555}.navbar-nav .nav-link{font-size:1.2rem}.dropdown-menu .dropdown-item{font-size:1rem}.dropdown-toggle::after{display:inline-block;margin-left:.255em;vertical-align:.255em;content:"";border-top:.3em solid;border-right:.3em solid transparent;border-bottom:0;border-left:.3em solid transparent}.navbar .dropdown-toggle::after{content:none!important}.navbar-toggler-icon{width:1.2em;height:1.2em}.navbar-toggler{border:none!important}.navbar-toggler:focus{box-shadow:none!important}.d-flex{height:45px}.a_logonav{text-decoration:none!important}.text-isp{font-size:1.3rem;color:#9d9d9d;display:inline-block;vertical-align:middle;text-decoration:none!important}.navbar-nav{max-height:calc(50vh - 56px);overflow-y:auto}}</style><script>document.addEventListener("DOMContentLoaded",function(){let e=document.querySelectorAll(".navbar .dropdown");e.forEach(function(e){let t=e.querySelector(".dropdown-toggle");t.addEventListener("click",function(e){window.innerWidth<992&&e.target===t&&(window.location.href=t.href)});let n=e.querySelectorAll(".dropdown-item");n.forEach(function(e){e.addEventListener("click",function(){window.location.href=e.href})})}),document.addEventListener("click",function(t){window.innerWidth<992&&!t.target.closest(".navbar .dropdown")&&e.forEach(function(e){e.querySelector(".dropdown-menu").classList.remove("show")})})})</script><main><div class=container><div class="content-container grid-layout"><div class=box-header><h1>First words ex-cathedra</h1></div><div class=box-abstract><p><h1 id=jes√∫s-malo-san-francisco-starbucks-at-390-stockton-st-february-2015>Jes√∫s Malo (San Francisco, Starbucks at 390 Stockton St., February 2015)</h1><p>Circa 2015, applications for full professorship in Spanish universities (cathedra) involved writing an essay to describe your career and personal views on science. Here is what I wrote to get the condition of <strong>Accredited University Professor</strong> from the official National Evaluation Agency&mldr;</p><p>Now (after the positive outcome in July 2015), I upload the version with uncensored pictures, full text, and over 150 hyperlinks!. These are my first words ex-cathedra (even though my salary, as well as the salary of over 2500 colleagues in the same situation, will remain the same for a while unless we do something):</p><hr><h1 id=table-of-contents>Table of Contents</h1><ul><li><a href=#1-why-a-physicist-would-ever-care-about-human-vision>Why a physicist would ever care about Human Vision?</a></li><li><a href=#2-chronological-summary-of-my-career>Chronological summary of my career</a></li><li><a href=#3-my-research-contributions>My research contributions</a></li><li><a href=#4-my-teaching-activities-like-richard-dawkins-in-a-republican-convention>My teaching activities: like Richard Dawkins in a Republican Convention</a></li><li><a href=#5-economic-constraints-of-science-in-spain>Economic constraints of science in Spain</a></li></ul><hr><h1 id=1-why-a-physicist-would-ever-care-about-human-vision>1. Why a physicist would ever care about Human Vision?</h1><h2 id=think-again-human-vision-is-cool>Think again: human vision is cool!</h2><p>The leit-motif of my research and teaching activity is the study of visual information processing in the human brain. This is a biological and subjective problem: not very appealing adjectives for a big-bang theory guy. Nevertheless, the aspects of this problem that may be of interest for physicists determined the direction of my scientific career.</p><p>Despite the overuse of the word multidisciplinary, you have to consider that Visual Perception is a truly multidisciplinary problem. On the one hand, the input signal certainly involves plain Physics such as light emission and scattering in every-day scenes (classical Radiometry) and image formation in biological systems (classical Physiological Optics). However, on the other hand, the analysis of such input signal is a problem for Neuroscience: examples of the latter include the study of (natural) neural networks for image understanding. Human Vision is not at all limited to the laws of image formation, that basically date back to Newton classical Optics, but also include the formulation of laws that determine the organization of the sensors that make sense of these signals. And this is a quite different issue!. Regarding this analysis part, a theory that explains the visual cortex phenomena requires concepts coming from Statistics and Information Theory, or in nowadays jargon, Machine Learning. A particularly interesting feature of this problem is the fact that, as opposed to other science problems, the relation between maths and application (here Maths and Neuroscience) is not one-directional: in this case the system to be understood is actually a computing machine that may also inspire original mathematical approaches. Finally, the models coming from Theoretical Neuroscience may be applied in Electrical Engineering and Computer Science.</p><p>From a personal (and hence arguable) point of view, the Human Vision problem is interesting for a physicist not for the aspects related to classical Optics (fundamentally solved long ago), but for the study of the visual brain. Vision is not in the (well known) eye of the beholder, but in his/her (highly unknown) brain. The visual brain is a natural system with complex dynamics (the jargon physicists love), quantitative theories for partial explanations are very recent, and many of them are still under discussion. The study of Vision combines experiments, mathematical theories and technological applications, and this combination is the core of how the physicists approach the problems. It doesn&rsquo;t matter that the experimental methods come from the Psychology, the Optometry or the Neurophysiology (all of them use the so called Psycho-Physics) or that the applications are in Image Processing and Computer Vision: the study of the Human Visual System is certainly quite appropriate for a physicist.</p><p>The fascination for the surprising behavior of the visual system is what determined my scientific exploration: over the last 20 years I made some contributions (or managed to introduce some <strong>colored noise</strong> üòâ in most of the disciplines cited above.</p><hr><h1 id=2-chronological-summary-of-my-career>2. Chronological summary of my career</h1><h2 id=while-khun-and-marx-were-kind-of-wrong-sinatra-was-right-i-did-it-my-way>While Khun and Marx were kind of wrong, Sinatra was right: I did it my way!</h2><p>Selecting a multidisciplinary problem implies having a wide range of collaborators over the years. The topics and the collaborators to address them are the parts of the scientific career that one can actually choose. <a href=https://en.wikipedia.org/wiki/Thomas_Kuhn>Thomas Kuhn</a> (or even <a href=https://en.wikipedia.org/wiki/Karl_Marx>Karl Marx</a>) would certainly say that economic constraints sometimes impose their own choices. In my case, even though money sometimes determined the order in which I visited different aspects of the problem (e.g., applications before foundations), economic constraints didn&rsquo;t imply modifications in the selected direction since I was fortunate enough to get steady funds along these two decades (more details on economic constraints below).</p><p>Constraints are usually harder in the teaching part since it is determined by the duties of the department where you happen to develop your research. Nevertheless, with some dedication, this part can also be modulated. Similarly to the research side (where I started at the <a href=#>Optics Department</a> in the Physics School, but then I looked for collaborators in <a href=https://www.uv.es/ferri/>Maths</a>, <a href=http://www.uv.es/gcamps/>Electrical Engineering</a>, and <a href=http://www.uv.es/jgutierr/index_en.html>Computer Science</a>), in the teaching side I decided to give lectures in PhD and Master programs out of my department (beyond the department-related duties). This was a way to convey the knowledge acquired in research activities to a broader audience.</p><p>Below is the list of multidisciplinary collaborators I found (or looked for) over time. Note that in the formative years and right after getting my first permanent position, I focused on applications (e.g., image coding) to maximize funding probabilities. More recently, particularly after my second <a href=https://www.fecyt.es/>Spanish NSF Project</a> as PI, I turned to the fundamental issues (the theory and the consideration of a higher abstraction level, as for instance in the current <a href=https://explora.fecyt.es/>Explora Project</a> - my 4th as PI), yet still paying attention to technology transfer:</p><ul><li><p>I started my PhD thanks to the funds of the <a href=http://www.acuvueprofessional.com/fellowship-grants>European Vistakon Research Award</a> (Johnson & Johnson) obtained in 1994. My advisor was <a href=http://www.uv.es/artigas/>Jose Mar√≠a Artigas</a> at the Optics Department, but in order to formalize the psychophysics done with <a href=https://es.wikipedia.org/wiki/%C3%81lvaro_Pons>A. Pons</a>, <a href=http://www.uv.es/capilla/>P. Capilla</a>, and <a href=https://www.researchgate.net/profile/Maria_Jose_Luque>M.J. Luque</a>, I decided to look for help in the computational issues at the Computer Science Dept. (with <a href=http://www.uv.es/ferri/>F. Ferri</a> as co-advisor).</p></li><li><p>The <a href=http://www.cies.org/>Fulbright postdoc</a> I got for the period 2000-2001 allowed me to work with world-class researchers in <a href=http://vision.arc.nasa.gov/personnel/watson/watson.html>Vision Science</a>, both from the experimental perspective (Beau Watson at the NASA Ames Research Center) and the theoretical perspective (Eero Simoncelli at the <a href=http://www.cns.nyu.edu/%7Eeero/>Center for Neural Science</a> and <a href=https://www.cims.nyu.edu/>Courant Institute of Mathematics</a> at NYU). I already knew about divisive normalization (<a href=http://www.carandinilab.net/Carandini-1994-science.pdf>Carandini94</a>, <a href="https://www.osapublishing.org/josaa/abstract.cfm?uri=josaa-14-9-2379">Watson97</a>, <a href=http://www.sciencedirect.com/science/article/pii/S0042698997001831>Simoncelli98</a>), but there I better understood its relation with unsupervised learning (<a href=http://www.nature.com/nature/journal/v381/n6583/abs/381607a0.html>Olshausen96</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/schwartz01-reprint.pdf>Schwartz01</a>). These were the inspiration for a lot of later work.</p></li><li><p>When I came back from the US and I got my permanent position, I started my period as PI in public-funded projects with <a href=http://www.io.csic.es/PagsPers/JPortilla/>J. Portilla</a> (CSIC Optics Institute) working on image coding and restoration. Due to my participation in PhD programs of Applied Mathematics and Computer Science, I advised the PhDs of <a href=http://www3.uji.es/%7Eepifanio/>I. Epifanio</a> and <a href=http://www.uv.es/%7Ejgutierr/index_en.html>J. Guti√©rrez</a>, now faculty staff in different universities. Moreover, I looked for collaborators at the <a href=http://www.uv.es/gcamps/>Electrical Engineering Department</a> (with <a href=http://www.uv.es/gcamps/>G. Camps</a>, <a href=http://www.uv.es/jordi/>J. Mu√±oz</a>, and <a href=http://www.uv.es/%7Echovago/>L. G√≥mez</a>) to set the <a href=./../../../../>Image and Signal Processing Group</a> where I lead the activities related to <a href=../../../../research/visual_neuroscience/>Vision Science</a> and <a href=http://isp.uv.es/improc.html>Image Processing</a>. In this period, as a result of my activity as PI of different regional and national projects I contacted groups from national and international universities (e.g., the <a href=http://www.cvc.uab.cat/%7Exotazu/>Computer Vision Center of UAB</a> -<a href=http://www.cvc.uab.cat/%7Exotazu/>X. Otazu</a>-, the <a href="http://deic.uab.es/personal/membres.php?idioma=2&amp;id=18">Electrical Eng. Dept. of UAB</a> -<a href="http://deic.uab.es/personal/membres.php?idioma=2&amp;id=18">J. Serra</a>-, or the Mathematics and Statistics Dept. of <a href=http://www.cs.helsinki.fi/u/ahyvarin/>Helsinki University</a> with <a href=http://www.cs.helsinki.fi/u/ahyvarin/>A. Hyvarinen</a> and <a href=https://sites.google.com/site/michaelgutmann/>M. Gutmann</a>).</p></li><li><p>Currently, particularly after my 3rd advised PhD (<a href=http://www.uv.es/lapeva/>V. Laparra</a>, currently PostDoc at <a href=https://www.nyu.edu/>NYU</a>), and my second stay in the US at Stanford and NYU (now as <a href=https://thevisualanalogylab.wix.com/valab>Senior Visiting Researcher</a> in 2013), I am more interested in Theoretical Visual Neuroscience. As a result, I contacted Prof. <a href=http://thevisualanalogylab.wix.com/valab>L. Mart√≠nez Otero</a> (<a href=http://thevisualanalogylab.wix.com/valab>CSIC Neuroscience Institute</a>) to be co-PI of a recently funded national project on modeling cortical circuits for neural response inversion. His experimental know-how and wider neuroscience perspective will also be critical in another national project involving fMRI for neuroaesthetics I am PI of. On top of these theoretical activities, following our industrial patent in 2008, I am also conducting technology transfer through two applied projects I am PI of: one funded by the regional government to cooperate with <a href=http://www.analog.com/en/index.html>Analog Devices Inc.</a>, and the other funded by <a href=http://www.davalorsalud.com/>Davalor Salud</a>, a company devoted to the automated evaluation of visual function. Moreover, <a href=https://portal.upf.edu/web/etic/entry/-/-/15515/409/marcelo-jos%C3%A9-bertalmio>M. Bertalmio</a>&rsquo;s group at <a href=https://portal.upf.edu/web/etic/entry/-/-/15515/409/marcelo-jos%C3%A9-bertalmio>Univ. Pompeu Fabra</a> is helping me to measure multi-stage vision models for digital movies using his ERC funds.</p></li><li><p>An illustration of the multidisciplinarity is the wide range of my editorial services. Beyond the regular reviewing activities in 17 JCR journals (including <a href=http://www.mitpressjournals.org/loi/neco>Neural Computation</a>, <a href=https://www.osapublishing.org/josaa/home.cfm>JOSA</a>, <a href=http://cis.ieee.org/ieee-transactions-on-neural-networks-and-learning-systems.html>IEEE Transactions on Neural Networks and Learning Systems</a>, <a href=http://www.jmlr.org/>JMLR</a>, <a href=http://www.signalprocessingsociety.org/publications/periodicals/jstsp/>IEEE J. Sel. Top. Sig. Proc.</a>, <a href=http://www.journals.elsevier.com/computers-and-mathematics-with-applications/>Computers and Mathematics with Applications</a>, <a href=http://www.grss-ieee.org/publications/transactions/>IEEE Transactions on Geoscience and Remote Sensing</a>, <a href=http://www.journals.elsevier.com/image-and-vision-computing/>Image and Vision Computing</a>, or <a href=http://iopscience.iop.org/2040-8986/>J. Optics</a>) and in major conferences (including <a href=https://nips.cc/>NIPS</a>, <a href=http://e-nns.org/>ICANN</a>, <a href=http://dblp1.uni-trier.de/db/conf/sspr/index.html>IAPR-SSPR</a>, or <a href=http://www.ieee.org/conferences_events/index.html>IEEE ICIP</a>), I was Associate Editor of the <a href=http://www.signalprocessingsociety.org/publications/periodicals/image-processing/>IEEE Transactions on Image Processing</a> (IF=3.11, 1st quartile JCR) in the period 2009-2013, and currently I am Academic Editor of <a href=http://www.plosone.org/>PLoS ONE</a> (IF = 3.55, 1st quartile JCR) for the period 2014-2017, in both cases dealing with manuscripts in the intersection of Human Vision and Machine Learning.</p></li></ul><p>As a summary, despite the troubles of a truly multidisciplinary topic, the path has been (kind of) coherent and successful. At this point, I have to thank <a href=http://www.uv.es/artigas/>Prof. Jose Mar√≠a Artigas</a> for his lectures on Physics of Vision: a small course for the physics students which (unfortunately!) is no longer available in my university. In those lessons, he told us about something completely different. As fresh and educative as the <a href="https://www.youtube.com/watch?v=FGK8IC-bGnU">Monty Python</a> for physics students.</p><hr><h1 id=3-my-research-contributions>3. My research contributions</h1><h2 id=colored-noise-in-vision-sciences-and-some-thoughts-on-the-h-index>Colored noise in vision sciences and some thoughts on the h-index</h2><ul><li>Experiments in vision science</li><li>Theory: empirical models in vision science</li><li>Theory: principled models in vision science (computational visual neuroscience)</li><li>Theory: statistical learning</li><li>Applications in image processing</li><li>Preliminary conclusions</li><li>Impact of the above: h-index or just colored noise?</li></ul><h2 id=experiments-in-vision-science-7-jcr-publications>Experiments in Vision Science (7 JCR publications)</h2><p>I made experimental contributions in three aspects: Physiological Optics, Psychophysics, and Image Statistics. (i) In the field of <strong>Physiological Optics</strong>, we measured the optical transfer function of the lens+cornea system in-vivo <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/OPH97.PS.gz>Opth.Phys.Opt.97</a>. This work received the European Vistakon Research Award 94&rsquo;. (ii) In <strong>Psychophysics</strong>, we proposed simplified methods to measure the Contrast Sensitivity Function in all the frequency domain <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/JOPT94.PS.gz>J.Opt.94</a>, and a fast and accurate method to measure the parameters of multi-stage linear+nonlinear vision models <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/malo15a-reprint.pdf>Proc.SPIE15</a>. Finally, (iii) in <strong>Image Statistics</strong> we gathered spatially and spectrally calibrated image samples to determine the properties of these signals and their variation under changes in illumination, contrast, and motion <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/ivc99.ps.gz>Im.Vis.Comp.00</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/Neco_accepted_2012.pdf>Neur.Comp.12</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/manuscr_TGRS_2012_00431.pdf>IEEE-TGRS14</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/Gutmann_PLOS_ONE_2014.pdf>PLoS-ONE14</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/rem_sens_im_proc_12_ch02.pdf>Rem.Sens.Im.Proc.11</a>, <a href=../../../../code/vision_and_color/aftereffects/content>Front.Neurosci.15</a>.</p><h2 id=theory-empirical-models-in-vision-science-8-jcr-publications>Theory: empirical models in Vision Science (8 JCR publications)</h2><p>We proposed mathematical descriptions of different visual dimensions: <strong>Texture</strong>, <strong>Color</strong>, and <strong>Motion</strong>. (i) We used wavelet representations to propose nonstationary Texture Vision models <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/JMO97.PS.gz>J.Mod.Opt.97</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/msc_jmalo.pdf>MScThesis95</a>. (ii) We developed <strong>Color Vision</strong> models with illumination invariance that allow the reproduction of chromatic anomalies, adaptation, and aftereffects <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/VISRES97.PS.gz>Vis.Res.97</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/JOPT96.PS.gz>J.Opt.96</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/JOPT98.PS.gz>J. Opt.98</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/josa_04.pdf>JOSA04</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/Neco_accepted_2012.pdf>Neur.Comp.12</a>. (iii) <strong>Motion Vision</strong> models <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/Malo_Alheteia_08.pdf>Alheteia08</a> focus on optical flow computation in perceptually relevant moving regions <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/vss_poster.eps>J.Vis.01</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/Redundancy_Reduction_Malo_99.pdf>PhDThesis99</a>, and explain the static motion aftereffect <a href=../../../../code/vision_and_color/aftereffects/content>Front.Neurosci.15</a>.</p><p>All these psychophysical and physiological models have a parallel linear+nonlinear structure where receptive fields and surround-dependent normalization play an important role.</p><h2 id=theory-principled-models-in-vision-science-12-jcr-publications>Theory: principled models in Vision Science (12 JCR publications)</h2><p>This category refers to the proposition of <strong>organization laws of sensory systems</strong> that explain empirical phenomena, showing how neural function is adapted to the statistics of visual stimuli. (i) We worked on the <strong>derivation of the linear properties</strong> of the sensors, finding that spatio-chromatic sensitivity, receptive field changes, and phase properties arise from optimal solutions to the adaptation problem under noise constraints and manifold matching <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/Gutmann_PLOS_ONE_2014.pdf>PLoS-ONE14</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/AdaptVQ_ieeetgars_2012.pdf>IEEE-TGRS13</a>, from statistical independence requirements <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/ICANN_2011_v7.pdf>LNCS11</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/SlidesNeuroImageMeeting11.pdf>NeuroImag.Meeting11</a>, and from optimal estimation of object reflectance <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/manuscr_TGRS_2012_00431.pdf>IEEE-TGRS14</a>. (ii) We also derived the <strong>non-linear behavior</strong> of visual sensors like chromatic, texture, and motion sensors, linking non-linearities to optimal information transmission and/or error minimization in noisy systems <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/V1_from_non_linear_ICA.pdf>Network06</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/Neco_accepted_2012.pdf>Neur.Comp.12</a>, <a href=../../../../code/vision_and_color/aftereffects/content>Front.Neurosci.15</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/JOPT95.PS.gz>J.Opt.95</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/ivc99.ps.gz>Im.Vis.Comp.00</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/spr00.ps>LNCS00</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/patt_rec03.pdf>Patt.Recog.03</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/Malo_Laparra_Neural_10b.pdf>Neur.Comp.10</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/LNAI10_malo_laparra.pdf>LNCS10</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/SlidesNeuroImageMeeting11.pdf>NeuroImag.Meeting11</a>.</p><h2 id=theory-statistical-learning-7-jcr-publications>Theory: Statistical Learning (7 JCR publications)</h2><p>In theoretical neuroscience the derivation of properties of biological sensors from the regularities visual scenes requires novel tools for statistical learning. In this field, we developed new techniques for unsupervised manifold learning, feature extraction (or symmetry detection in datasets), dimensionality reduction, probability density estimation, multi-information estimation, distance learning, and automatic adaptation from optimal dataset matching. Given my interest in applicability in Vision Science problems, I focused on techniques that can be explicitly represented in the image domain to be compared with receptive fields of visual neurons, as opposed to the usual practice in the Machine Learning community. Techniques include Rotation-based Iterative Gaussianization -RBIG- <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/Laparra11.pdf>IEEE TNN 11</a>, Sequential Principal Curves Analysis -SPCA- <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/V1_from_non_linear_ICA.pdf>Network06</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/Neco_accepted_2012.pdf>Neur.Comp.12</a>, <a href=../../../../code/vision_and_color/aftereffects/content>Front. Neurosci.15</a>, Principal Polynomial Analysis -PPA- <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/IJNS_Laparra14_accepted_v5.pdf>Int.J.Neur.Syst.14</a>, Dimensionality Reduction based on Regression -DRR- <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/drr_jstsp2014_final.pdf>IEEE JSTSP15</a>, and Graph Matching for Adaptation <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/AdaptVQ_ieeetgars_2012.pdf>IEEE TGRS13</a>.</p><h2 id=applications-image-processing-24-jcr-publications--1-patent>Applications: Image Processing (24 JCR publications + 1 patent)</h2><p>We proposed original image processing techniques using both perception models and image statistics including (i) improvements of JPEG standard for image coding through nonlinear texture vision models <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/ELECT95.PS.gz>Electr.Lett.95</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/ELECT99.PS.gz>Electr.Lett.99</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/Gomez-Perez05_IEEETNN.pdf>IEEE TNN05</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/manuscript4.pdf>IEEE TIP06a</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/Camps-Valls08_JMLR.pdf>JMLR08</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/2012b_Gutierrez_RPTSP_12c.PDF>RPSP12</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/patente_v5_jesus.pdf>Patent08</a>, (ii) improvements of MPEG standard for video coding with new perceptual quantization scheme and new motion estimation focused on perceptually relevant optical flow <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/LNCS97.PS.gz>LNCS97</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/ELECT98.PS.gz>Electr.Lett.98</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/elect00.ps>Electr.Lett.00a</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/seg_ade2.ps>Electr.Lett.00b</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/ieeeoct01.pdf>IEEE TIP01</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/Redundancy_Reduction_Malo_99.pdf>Redund.Reduct.99</a>, (iii) new image restoration techniques based on nonlinear contrast perception models and the image statistics in local frequency domains <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/manuscript_TIP_00864_2004_R2.pdf>IEEE TIP 06b</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/laparra10a.pdf>JMLR10</a>; (iv) new approaches to color constancy either based on relative chromatic descriptors <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/VISRES97.PS.gz>Vis.Res.97</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/JOPT96.PS.gz>J.Opt.96</a>, statistically-based chromatic adaptation models <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/Neco_accepted_2012.pdf>Neur.Comp.12</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/Gutmann_PLOS_ONE_2014.pdf>PLoS-ONE14</a>, or Bayesian estimation of surface reflectance <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/manuscr_TGRS_2012_00431.pdf>IEEE-TGRS14</a>; (v) new subjective image and video distortion measures using nonlinear perception models <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/IVC97.PS.gz>Im.Vis.Comp.97</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/displays_99.pdf>Disp.99</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/icip02.pdf>IEEE ICIP02</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/Laparra_JOSA_10.pdf>JOSA10</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/malo15a-reprint.pdf>Proc.SPIE15</a>; and (vi) image classification and knowledge extraction (or regression) based on our feature extraction techniques <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/Laparra11.pdf>IEEE-TNN11</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/AdaptVQ_ieeetgars_2012.pdf>IEEE-TGRS13</a>,<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/IJNS_Laparra14_accepted_v5.pdf>Int.J.Neur.Syst.14</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/drr_jstsp2014_final.pdf>IEEE-JSTSP15</a>. See code for image and video processing applications <a href=../../../../code/image_video_processing/>here</a>.</p><h2 id=preliminary-conclusions>Preliminary Conclusions</h2><ul><li><p>The visual brain is astonishingly well adapted to the natural visual world. This sentence shouldn&rsquo;t be surprising for any teenager that heard about <a href=https://en.wikipedia.org/wiki/Charles_Darwin>Charles Darwin</a>. The cool thing in that conclusion was preparing <a href=http://isp.uv.es/data_color.htm>accurate image data</a>, developing the appropriate <a href=../../../../code/feature_extraction/>mathematical tools</a> to derive the behavior described by <a href=../../../../code/vision_and_color/>computational models</a> as seen in <a href=../../../../code/vision_and_color/aftereffects/content>psychophysical illustrations</a>. By putting all this together in a <a href=http://isp.uv.es/code/imvideo/KeCoDe/single_piece.html>single piece of code</a> you realize that the statement is true.</p></li><li><p>Appropriate (mathematical) formulation of visual phenomena is the only way to understand the problem and to derive applications. This statement is not very original either, given the famous <a href=https://en.wikipedia.org/wiki/Galileo_Galilei#Scientific_methods>Galileo Galilei</a> quote [the book of nature is written in mathematical language]. However, in this multidisciplinary world, a special effort has to be done to translate physiological facts into models that work on, lets say, actual video sequences. By doing so, you transcend the specific details of a set of experiments, and think about all the additional problems faced (and solved) by the visual brain. Numerical simulations are useful to put a specific physiological behavior in perspective. Moreover, well-formulated models allow us to explore new experimental questions through the appropriate stimuli. Not to speak about the straightforward use in image processing and computer vision&mldr;</p></li><li><p>Nonlinear techniques are fancy, but it is amazing the percentage of reality that we can explain with linear models. Another old-fashion statement for eigenvector lovers. Besides, linear algebra is easy! For instance, a simple rotation (the Principal Component Analysis of <a href=https://en.wikipedia.org/wiki/Karl_Pearson>Karl Pearson</a>) applied to small patches of natural sequences, explains the major features of the receptive fields of LGN-V1 visual neurons. This includes opponent color coding, neurons tuned to spatial texture, and motion-sensitive neurons. Different kinds of simple affine transforms (linear scaling and translations) explain basic sensitivity to color, texture, and motion as well as the basic trends of adaptation. Amazing!</p></li><li><p>We roughly understand low-level visual information processing in the brain. However, there is still a long way to understand how we derive abstract concepts from low-level primitives. Not a surprising statement either if you saw the appropriate documentary or heard about <a href=http://rstb.royalsocietypublishing.org/content/370/1666/20140383>David Marr</a>. Despite all the knowledge about color, spatial texture, motion, and depth information processing in LGN, V1, and MT, little is known about how these pieces are put together in other parts of the brain (e.g. IT). What are the organization laws of these higher abstraction mechanisms? What about their relations to language? What about our ability to synthesize images (draw) from a written description?.</p></li></ul><p>An educated teenager that heard about Darwin, Galileo, Pearson, and Marr (evolution, mathematical modeling, eigenvectors, decorrelation, and vision) could be disappointed by the simplicity of these conclusions. However, note that my claims can be louder now than 20 years ago because of the time spent in accumulating evidence (and writing this <a href=http://isp.uv.es/code/imvideo/KeCoDe/single_piece.html>piece of code</a>). I hope that the next 20 years are fruitful enough to make these conclusions stronger or (even better!) to change some of them.</p><h2 id=impact-of-colored-noise-in-science-libraries>Impact of Colored Noise in science libraries</h2><p>As I told Eero Simoncelli once, while few people make a big impact on the scientific community, what others (including myself) do can be seen as injecting colored noise in the science libraries and the internet. Nevertheless, as argued below, that is not a major problem, but even something worth funding.</p><h2 id=some-thoughts-on-jcr-publications-and-the-h-index>Some thoughts on JCR publications and the h-index</h2><p>For ordinary (not-Nobel-laureate) people, research is mainly a personal learning experience. Such process starts with some childish initial curiosity and ends with refereed publication. It involves putting the question in context, saying something coherent about it, and convincing critical reviewers about the accuracy of such statement (no matter it is ground-breaking or not). Given the quality control imposed by peer review (particularly in high impact journals) the publishing-in-fine-journals exercise is one of the most comprehensive learning procedures ever developed. Even though the publications of the average scientist remain unknown or never make a global difference (i.e. low h-index), the rigor of the learning process in JCR-journals ensures this person has the deepest understanding of the issues. And this has a local impact in the dissemination of knowledge to others, either (local) students or (local) industries. A cohort of average scientists well trained through the publication process have to be there, ready to understand, confirm, disseminate and apply what (the few) original scientist happen to discover. In my view, that is the justification of devoting public money to fund average scientific research (or random colored noise generation ;-). <strong>Note that samples from colored noise do not distribute as a sphere, but collectively they point to a certain direction, hopefully the right one!</strong>.</p><p>For those of you who do not share this personal learning view, and love rankings better, here is the impact of my research (by July 2015, i.e. automatically outdated) according to my <a href="https://scholar.google.es/citations?user=0pgrklEAAAAJ&amp;hl=es&amp;oi=ao">Google Scholar profile</a>: my Hirsch index was 19, the total number of citations to my work was 876, so I was the 3rd most-cited scientist in the world in the (<a href="https://scholar.google.es/citations?view_op=search_authors&amp;hl=es&amp;mauthors=label:image_statistics">Google Scholar</a> ;-) category of Image Statistics, the 28th one in <a href="https://scholar.google.es/citations?view_op=search_authors&amp;hl=es&amp;mauthors=label:human_vision">Human Vision</a>, the 87th in <a href="https://scholar.google.es/citations?view_op=search_authors&amp;hl=es&amp;mauthors=label:visual_perception">Visual Perception</a>, and the 263rd in <a href="https://scholar.google.es/citations?view_op=search_authors&amp;hl=es&amp;mauthors=label:vision">Vision</a>. So what?</p><p>To me, the undeniable peak of my scientific career happened when a mild morning of February 2001, I left my office at the NASA Ames Research Center and drove my Toyota through the rocket wind tunnels to attend a talk at a nearby town in Silicon Valley on the vision abilities of <a href=https://en.wikipedia.org/wiki/HAL_9000>HAL-9000</a>, the famous computer of Stanley Kubrick&rsquo;s <em>2001: A Space Odyssey</em>. The combination of NASA, 2001, and HAL-9000 together really felt like big science. Particularly compared to my Spanish postdoc salary and housing prices of the <a href=https://en.wikipedia.org/wiki/Dot-com_bubble>dot-com bubble</a>. Since that glorious morning, I felt like <a href=https://en.wikipedia.org/wiki/Space_Odyssey#Characters>Dr. Dave Bowman</a> for a second; everything else has been a steady decline.</p><hr><h1 id=4-my-teaching-activities-like-richard-dawkins-in-a-republican-convention>4. My teaching activities: like Richard Dawkins in a Republican Convention</h1><h2 id=why-an-optometrist-or-engineer-would-ever-care-about-maths-or-science>Why an optometrist (or engineer) would ever care about Maths (or Science)?</h2><p>My teaching activity at the university spans over 19 years (only one less than my research activity). This means that I had to teach while obtaining my PhD. This undesirable situation happened since at that time (mid 90s) getting a PhD grant was restricted to students of professors having public funds (which was not the case of my advisor). Therefore, I stayed at the university only because (i) I won the <a href=http://www.acuvueprofessional.com/fellowship-grants>European Vistakon Research Award</a> (which I used to pay my PhD research for one year), and (ii) a new degree on Optometry and Vision Science was established at my university and it generated several openings for junior assistant professors.</p><p>The quality of my teaching over these two decades has been rated by my students according to the regulations in my university (in a scale of 5) as 3.6 ¬± 0.3, i.e. they gave me a positive rating with a small variance over the years.</p><p>My teaching activity has been modulated by (1) my interest in Vision Science, and (2) by having most of my docent duties associated to the Degree and Master on Optometry and Vision Science. The correspondence between these two factors has been positive since it gave coherence to the research and teaching activities. However, the problem with Optometry students is that they imagine themselves as Medical Doctors (and you know that <a href=http://www.cebm.net/>Evidence-Based Medicine</a> is a recent field!). As a result, these students are not quite prepared for the practice of quantitative science (is there any non-quantitative science anyway?). This problem represented, (i) a challenge to convey the quantitative message to students with non-quantitative interests, and (ii) an incentive to diversify my teaching activity looking for students not scared by scalar products. The challenge posed by the non-quantitative students lead to the development of Matlab tools such as <a href=../../../../code/vision_and_color/colorlab/content>COLORLAB</a>, <a href=../../../../code/image_video_processing/basicvideotools/content>BasicVideoTools</a>, and <a href=../../../../code/vision_and_color/virtualneurolabs/content>VirtualNeuroLabs</a>, and new docent methodologies <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/proyecto_docente_ch3_2002.pdf>[ProyDocente02]</a> to convey the quantitative credo to students afraid of Maths. In this quantitative effort I found a lot of help and support from <a href=https://www.researchgate.net/profile/Maria_Jose_Luque>M.J. Luque</a> and <a href=http://www.uv.es/capilla/>P. Capilla</a> (respectively ;-) Sometimes I really feel as hopeless as <a href=https://en.wikipedia.org/wiki/Richard_Dawkins>Richard Dawkins</a> at a Republican convention. But having a lot of fun, though!. On second thoughts, my Optometry undergrads are not that bad: deficient education is always a problem of the teachers, not the students. Please excuse the trivial comparison with the <a href=https://en.wikipedia.org/wiki/Creationism#Scientific_criticism>creationists</a>!.</p><p>In order to diversify the audience, I also lectured in PhD and Master programs with Excellence distinction out of my Optics department, as for instance at the Applied Maths and Computer Science departments of my university, at the Institute of Applied Ophtalmo-Biology (Univ. Valladolid), and at the Institut de Rob√≤tica i Inform√†tica Industrial (UPC). You can find slides, lecture notes and computer material for PhD courses <a href=../../../../courses/>here</a>.</p><p>Finally, I have to mention my best (or more patient) students: those who dared to be advised by me in their PhD years: <a href=https://www3.uji.es/~epifanio/>Irene Epifanio</a>, <a href=http://www.uv.es/~jgutierr/index_en.html>Juan Guti√©rrez</a>, and <a href=http://www.uv.es/lapeva/>Valero Laparra</a>. They got doctorate degrees with a number of JCR publications, best PhD and Master Thesis awards in PhD programs with European Excellence distinction, etc&mldr; Nevertheless, the best is what I learned from them: thank you all, it was a lot of fun!.</p><hr><h1 id=5-economic-constraints-of-science-in-spain>5. Economic constraints of science in Spain</h1><h2 id=why-a-positive-evaluation-for-professorship-does-not-imply-an-actual-position-in-spain>Why a positive evaluation for professorship does not imply an actual position in Spain?</h2><p>Saying that &ldquo;I did it my way despite the economic constraints&rdquo; was an obvious literary license (for the evaluation committee). The truth is that my generation has been extremely lucky since Spain experienced an unprecedented window of opportunities for young scientists in the late 90s and early 2000s. In this short time window the economic effort started in the 80s (after we got democracy) to build a European-like science system, led to a mature public research system in the 90s (private sector didn&rsquo;t go that fast). Favorable economic environment in the 90s and European funds steadily fueled this system and average scientific production in Spain achieved world-class level for the first time in history. In this situation it is easier to do it your way. It is fair to acknowledge that scientific freedom is a by-product of favorable conditions.</p><p>May be Marx wasn&rsquo;t that wrong after all. Particularly considering how the situation has changed since the 2008 crisis. Sadly, the favorable time window may be closing in Spain (and in other places in southern Europe). Conservative governments in Spain do not see basic research as an investment for the future, but as a <a href=http://www.nature.com/news/spain-cuts-science-ministry-in-government-changeover-1.9725>luxury you can disregard</a> (Nature, Dec. 2011).</p><p>This short-sighted policy affects both young and senior scientists. Massive budget cuts reduce the possibility to get PhD students, and those who finally complete their PhD have small chances here. Postdocs are scarce and, for some years now, new associate professor positions are extremely rare. In the same vein, no new full professor position has been created since 2011, and retirement-related positions are only covered at a 50% rate. Before the crisis, the official <a href=http://www.aneca.es/Programas/ACADEMIA>Accreditation for University Professor</a> (after a thorough independent review) used to be equivalent to getting an actual Professorship since there were no major funding problems. Now those days are over. The careers of accredited scholars (otherwise professors) are indefinitely truncated.</p><p>An association of <a href=http://acreditadosacatedra.blogspot.com.es/>Accredited University Professors</a> (website in Spanish) was created to demand solutions for this unfair blocked-career situation (see <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/The_Manifest.pdf>the manifest in English</a>). Major worker unions <a href=http://www.fe.ccoo.es/ensenanza/Inicio:891082--CCOO_pide_desatascar_el_conflicto_que_paraliza_la_carrera_del_profesorado_universitario>CCOO</a> and <a href=http://www.ugt.upv.es/2015/07/13/carta-de-apoyo-a-la-coordinadora-nacional-de-profesores-titulares/>UGT</a> support our demands. As in scientific research (see <a href=#impact-of-colored-noise-in-science-libraries>the colored noise concept</a>) it is the collective action what defines the direction to go. <a href="https://docs.google.com/forms/d/1CAI0r2Yx872ApZRWLzvcPyVA_gpnYBl9H2nlupGSAq4/viewform?c=0&amp;w=1">Please sign up!</a></p><hr><p>For further details on each of these sections, including my research contributions and teaching philosophy, I have included over <strong>150 hyperlinks</strong> throughout the text, providing access to my full publications, tools, and additional resources.</p></p></div><aside class=box-gallery><div class=gallery-grid><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/people/jesus.webp","[More about Jesus Malo and his academic career](#1-why-a-physicist-would-ever-care-about-human-vision)"," **Image Processing Lab.**<br> Dept. Optics (Faculty of Physics).<br> Universitat de Val√®ncia <br> Catedr√†tic Escardino <br> 46980, Paterna, Val√®ncia, Spain <br> jesus.malo@uv.es.<br> **ISP:** +34 963 544 099.<br> **Optics Dept:** +34 963 544 041.")'><img src=/images/people/jesus.webp alt="[More about Jesus Malo and his academic career](#1-why-a-physicist-would-ever-care-about-human-vision)"></a><p class=gallery-title><a href=#1-why-a-physicist-would-ever-care-about-human-vision>More about Jesus Malo and his academic career</a></p><div class=gallery-description><p><strong>Image Processing Lab.</strong><br>Dept. Optics (Faculty of Physics).<br>Universitat de Val√®ncia<br>Catedr√†tic Escardino<br>46980, Paterna, Val√®ncia, Spain<br><a href=mailto:jesus.malo@uv.es>jesus.malo@uv.es</a>.<br><strong>ISP:</strong> +34 963 544 099.<br><strong>Optics Dept:</strong> +34 963 544 041.</p></div></div><div class=gallery-item><iframe src=https://www.youtube.com/embed/0G0rXqvwR0Q frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><p class=gallery-title><a href=#1-why-a-physicist-would-ever-care-about-human-vision>Motion illusions and Entropy</a></p><div class=gallery-description><p>An example of this surprising behavior is the Static Motion Aftereffect (or the perception of reverse motion after prolonged exposure to a slowly moving pattern -see video-). Physicists like explanations from first principles (the so called laws), and this illusion can be understood according to a law based on communication theory. Sensors that maximize information transmission from sequences happen to have similar frequency tuning to motion sensitive neurons in V1 cortex. For the same efficiency reason, their response is nonlinear, and attenuates in the presence of high contrast moving patterns. Exposure to such patterns induces an operation regime that leads to the illusion while the system readapts to the new situation. Optimal Information Transmission seems to be a law of Human Vision. [Find out more&mldr;] It took me 20 years to fully understand that sentence.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/people/AA_new_york_university.webp","[More on the link between Physics, Neuroscience, and Statistics](#1-why-a-physicist-would-ever-care-about-human-vision)","If you are not already convinced of the relation (since you only listen to [authority arguments](https://en.wikipedia.org/wiki/Argument_from_authority) ;-) I have something for you. The New York University ([36 Nobel Laureates, wikipedia dixit](https://en.wikipedia.org/wiki/List_of_Nobel_laureates_by_university_affiliation#New_York_University) ;-) organizes its resources in this way: the Physics Department and the Center for Neural Science are in the very same building (both doors in the picture below lead to the same hall, and physiology and theoretical physics labs are interleaved). Moreover, the Courant Institute of Mathematics, famous for its research in Statistical Learning is exactly at the other side of the street (Washington Place).")'><img src=/images/people/AA_new_york_university.webp alt="[More on the link between Physics, Neuroscience, and Statistics](#1-why-a-physicist-would-ever-care-about-human-vision)"></a><p class=gallery-title><a href=#1-why-a-physicist-would-ever-care-about-human-vision>More on the link between Physics, Neuroscience, and Statistics</a></p><div class=gallery-description><p>If you are not already convinced of the relation (since you only listen to <a href=https://en.wikipedia.org/wiki/Argument_from_authority>authority arguments</a> ;-) I have something for you. The New York University (<a href=https://en.wikipedia.org/wiki/List_of_Nobel_laureates_by_university_affiliation#New_York_University>36 Nobel Laureates, wikipedia dixit</a> ;-) organizes its resources in this way: the Physics Department and the Center for Neural Science are in the very same building (both doors in the picture below lead to the same hall, and physiology and theoretical physics labs are interleaved). Moreover, the Courant Institute of Mathematics, famous for its research in Statistical Learning is exactly at the other side of the street (Washington Place).</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/people/family.webp","[Family and Academic Ancestors](#2-chronological-summary-of-my-career)","On the left, physical ancestors: Consuelo L√≥pez and Gaspar Malo (Jesus&#39; parents), Manuel and Remedios (uncle and aunt). On the   right, Prof. Jose Mar√≠a Artigas, a mentor in the Physics of Vision.")'><img src=/images/people/family.webp alt="[Family and Academic Ancestors](#2-chronological-summary-of-my-career)"></a><p class=gallery-title><a href=#2-chronological-summary-of-my-career>Family and Academic Ancestors</a></p><div class=gallery-description><p>On the left, physical ancestors: Consuelo L√≥pez and Gaspar Malo (Jesus&rsquo; parents), Manuel and Remedios (uncle and aunt). On the right, Prof. Jose Mar√≠a Artigas, a mentor in the Physics of Vision.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/people/JoseMaria.webp","[Family and Academic Ancestors](#2-chronological-summary-of-my-career)","My parents Consuelo L√≥pez (left) and Gaspar Malo (the man at the right), and my uncle and aunt (Manuel and Remedios). They could hardly read, however, they had a distinct preference for rational versus religious explanations of facts. Scientific ancestor (right): Prof. Jose Mar√≠a Artigas (here with granddaughter) could hardly operate a computer, however, after his stay at Cambridge in the eighties, he told a generation of physics students about [Horace Barlow](https://en.wikipedia.org/wiki/Horace_Barlow) and [Fergus Campbell](http://www.jstor.org/stable/770136). Similarly to his friend [David Field](http://redwood.psych.cornell.edu/people/david.html), Jose Mar√≠a used basically no math, but he transmitted the right concepts for those who wanted to listen. It is not reading this [Encyclopaedia](http://global.britannica.com/science/vision-physiology) or the [other](https://en.wikipedia.org/wiki/Vision_science), not even operating [this](https://en.wikipedia.org/wiki/MATLAB) or [that](https://en.wikipedia.org/wiki/MATLAB/) computer!. I thank my ancestors the original way of thinking and the freedom they gave me to do it my way.")'><img src=/images/people/JoseMaria.webp alt="[Family and Academic Ancestors](#2-chronological-summary-of-my-career)"></a><p class=gallery-title><a href=#2-chronological-summary-of-my-career>Family and Academic Ancestors</a></p><div class=gallery-description><p>My parents Consuelo L√≥pez (left) and Gaspar Malo (the man at the right), and my uncle and aunt (Manuel and Remedios). They could hardly read, however, they had a distinct preference for rational versus religious explanations of facts. Scientific ancestor (right): Prof. Jose Mar√≠a Artigas (here with granddaughter) could hardly operate a computer, however, after his stay at Cambridge in the eighties, he told a generation of physics students about <a href=https://en.wikipedia.org/wiki/Horace_Barlow>Horace Barlow</a> and <a href=http://www.jstor.org/stable/770136>Fergus Campbell</a>. Similarly to his friend <a href=http://redwood.psych.cornell.edu/people/david.html>David Field</a>, Jose Mar√≠a used basically no math, but he transmitted the right concepts for those who wanted to listen. It is not reading this <a href=http://global.britannica.com/science/vision-physiology>Encyclopaedia</a> or the <a href=https://en.wikipedia.org/wiki/Vision_science>other</a>, not even operating <a href=https://en.wikipedia.org/wiki/MATLAB>this</a> or <a href=https://en.wikipedia.org/wiki/MATLAB/>that</a> computer!. I thank my ancestors the original way of thinking and the freedom they gave me to do it my way.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/people/jopt_cover.webp","[Journal of Optics Cover](#2-chronological-summary-of-my-career)","Cover of Journal of Optics Vol. 25(3), 1994, made out of pictures from my first paper!. In the 90s simulations using 2D vision models working on actual images were unusual and easily deserved the cover of the journal (IF = 0.4, not a big deal but good enough for my first paper coming from a student lab ;-).")'><img src=/images/people/jopt_cover.webp alt="[Journal of Optics Cover](#2-chronological-summary-of-my-career)"></a><p class=gallery-title><a href=#2-chronological-summary-of-my-career>Journal of Optics Cover</a></p><div class=gallery-description><p>Cover of Journal of Optics Vol. 25(3), 1994, made out of pictures from my first paper!. In the 90s simulations using 2D vision models working on actual images were unusual and easily deserved the cover of the journal (IF = 0.4, not a big deal but good enough for my first paper coming from a student lab ;-).</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/people/PremioVistakon.webp","[Vistakon European Research Award](#2-chronological-summary-of-my-career)","The Vistakon European Research Award I got in 1994 (together with other stuff in my bookshelf). Its money is what made me stay in the academia when I was about to leave!.")'><img src=/images/people/PremioVistakon.webp alt="[Vistakon European Research Award](#2-chronological-summary-of-my-career)"></a><p class=gallery-title><a href=#2-chronological-summary-of-my-career>Vistakon European Research Award</a></p><div class=gallery-description><p>The Vistakon European Research Award I got in 1994 (together with other stuff in my bookshelf). Its money is what made me stay in the academia when I was about to leave!.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/people/neural_cover.webp","[Neural Computation Cover](#2-chronological-summary-of-my-career)","Cover of Neural Computation Vol. 22(12), 2010, made out of pictures from papers by Field & Chichilinsky (background) and Malo & Laparra (foreground). Theirs (in pink) is an illustration of the retina structure and ours (in gray) is an example of the probability of the response of a linear V1 cell given the response of another V1 cell.")'><img src=/images/people/neural_cover.webp alt="[Neural Computation Cover](#2-chronological-summary-of-my-career)"></a><p class=gallery-title><a href=#2-chronological-summary-of-my-career>Neural Computation Cover</a></p><div class=gallery-description><p>Cover of Neural Computation Vol. 22(12), 2010, made out of pictures from papers by Field & Chichilinsky (background) and Malo & Laparra (foreground). Theirs (in pink) is an illustration of the retina structure and ours (in gray) is an example of the probability of the response of a linear V1 cell given the response of another V1 cell.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/people/journals.webp","[IEEE Transactions and PLoS ONE Editorial Positions](#2-chronological-summary-of-my-career)","I was invited to be Associate Editor of the Transactions on Image Processing by the Editor in Chief, Thrasos Pappas, and the Editorial Board of the Journal in 2009. However, it was mandatory to become an IEEE member (and pay) in advance. That is the IEEE view of the business. I quit the IEEE as soon as I finished my 5-year period as Associate Editor. Now I shifted to open access journals: I was invited to be Academic Editor of PLoS ONE (Public Library of Science) in 2014.")'><img src=/images/people/journals.webp alt="[IEEE Transactions and PLoS ONE Editorial Positions](#2-chronological-summary-of-my-career)"></a><p class=gallery-title><a href=#2-chronological-summary-of-my-career>IEEE Transactions and PLoS ONE Editorial Positions</a></p><div class=gallery-description><p>I was invited to be Associate Editor of the Transactions on Image Processing by the Editor in Chief, Thrasos Pappas, and the Editorial Board of the Journal in 2009. However, it was mandatory to become an IEEE member (and pay) in advance. That is the IEEE view of the business. I quit the IEEE as soon as I finished my 5-year period as Associate Editor. Now I shifted to open access journals: I was invited to be Academic Editor of PLoS ONE (Public Library of Science) in 2014.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/people/experiment1.webp","[Illustrative experimental equipment](#3-my-research-contributions)","Left: double-pass setting for the measurement of the Modulation Transfer Function of the human eye [Opth.Phys.Opt.97]. Right: Spectrally calibrated light sources, image colorimeter and spectroradiometer to gather accurate color image statistics, see the available color image database [[Neur.Comp.12](https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/OPH97.PS.gz), [PLoS-ONE14](https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/Gutmann_PLOS_ONE_2014.pdf)], and [texture and motion datasets](../../../../code/vision_and_color/aftereffects/content) [[Front.Neurosci.15](../../../../code/vision_and_color/aftereffects/content)], with samples ready to be processed.")'><img src=/images/people/experiment1.webp alt="[Illustrative experimental equipment](#3-my-research-contributions)"></a><p class=gallery-title><a href=#3-my-research-contributions>Illustrative experimental equipment</a></p><div class=gallery-description><p>Left: double-pass setting for the measurement of the Modulation Transfer Function of the human eye [Opth.Phys.Opt.97]. Right: Spectrally calibrated light sources, image colorimeter and spectroradiometer to gather accurate color image statistics, see the available color image database [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/OPH97.PS.gz>Neur.Comp.12</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/Gutmann_PLOS_ONE_2014.pdf>PLoS-ONE14</a>], and <a href=../../../../code/vision_and_color/aftereffects/content>texture and motion datasets</a> [<a href=../../../../code/vision_and_color/aftereffects/content>Front.Neurosci.15</a>], with samples ready to be processed.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/people/method1.webp","[Spectrally Calibrated Light Source](#3-my-research-contributions)","")'><img src=/images/people/method1.webp alt="[Spectrally Calibrated Light Source](#3-my-research-contributions)"></a><p class=gallery-title><a href=#3-my-research-contributions>Spectrally Calibrated Light Source</a></p></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/people/method2.webp","[Spectroradiometer](#3-my-research-contributions)","")'><img src=/images/people/method2.webp alt=[Spectroradiometer](#3-my-research-contributions)></a><p class=gallery-title><a href=#3-my-research-contributions>Spectroradiometer</a></p></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/people/motion.webp","[Empirical Motion Model al Work](#3-my-research-contributions)","Waving hands sequence recorded at my lab (just a remake of the original movie from [Watson & Ahumada](http://vision.arc.nasa.gov/publications/ModelHumanVisualMotion.pdf)), linear filter model of MT neurons as an aggregate of spatio-temporal wavelet-like filters (bottom left) tuned to certain speed for optical flow computation (example at bottom right for a later frame). Note that our remake improved the original by including a striped costume for Fourier-obsessed freaks!")'><img src=/images/people/motion.webp alt="[Empirical Motion Model al Work](#3-my-research-contributions)"></a><p class=gallery-title><a href=#3-my-research-contributions>Empirical Motion Model al Work</a></p><div class=gallery-description><p>Waving hands sequence recorded at my lab (just a remake of the original movie from <a href=http://vision.arc.nasa.gov/publications/ModelHumanVisualMotion.pdf>Watson & Ahumada</a>), linear filter model of MT neurons as an aggregate of spatio-temporal wavelet-like filters (bottom left) tuned to certain speed for optical flow computation (example at bottom right for a later frame). Note that our remake improved the original by including a striped costume for Fourier-obsessed freaks!</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/people/dicromat.webp","[See how it feels to be color blind!](#3-my-research-contributions)","We proposed a way to simulate the perception of color blinds (here with Picasso&#39;s [Dora Maar](https://en.wikipedia.org/wiki/Dora_Maar)). As you see, dichromats are not color blind at all: they simply see different colors. Moreover, this simulation can be used to discriminate between color theories just by asking your dichromat friend which image is more similar to him.")'><img src=/images/people/dicromat.webp alt="[See how it feels to be color blind!](#3-my-research-contributions)"></a><p class=gallery-title><a href=#3-my-research-contributions>See how it feels to be color blind!</a></p><div class=gallery-description><p>We proposed a way to simulate the perception of color blinds (here with Picasso&rsquo;s <a href=https://en.wikipedia.org/wiki/Dora_Maar>Dora Maar</a>). As you see, dichromats are not color blind at all: they simply see different colors. Moreover, this simulation can be used to discriminate between color theories just by asking your dichromat friend which image is more similar to him.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/people/estimulac.webp","[Illustrative organization principle](#3-my-research-contributions)","Optimal adaptation and information transmission with noise constraints (Higher Order Canonical Correlation) predicts shifts in oscillatory responses of gabor-like opponent spatio-chromatic receptive fields when adapted to visual scenes under different illumination (similarly to V1 neurons). See code [here](../../../../code/feature_extraction/hocca/content/).")'><img src=/images/people/estimulac.webp alt="[Illustrative organization principle](#3-my-research-contributions)"></a><p class=gallery-title><a href=#3-my-research-contributions>Illustrative organization principle</a></p><div class=gallery-description><p>Optimal adaptation and information transmission with noise constraints (Higher Order Canonical Correlation) predicts shifts in oscillatory responses of gabor-like opponent spatio-chromatic receptive fields when adapted to visual scenes under different illumination (similarly to V1 neurons). See code <a href=../../../../code/feature_extraction/hocca/content/>here</a>.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/people/resp1.webp","[Response 1](#3-my-research-contributions)","")'><img src=/images/people/resp1.webp alt="[Response 1](#3-my-research-contributions)"></a><p class=gallery-title><a href=#3-my-research-contributions>Response 1</a></p></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/people/resp2.webp","[Response 2](#3-my-research-contributions)","")'><img src=/images/people/resp2.webp alt="[Response 2](#3-my-research-contributions)"></a><p class=gallery-title><a href=#3-my-research-contributions>Response 2</a></p></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/people/data_metric.webp","[Data Metric](#3-my-research-contributions)","")'><img src=/images/people/data_metric.webp alt="[Data Metric](#3-my-research-contributions)"></a><p class=gallery-title><a href=#3-my-research-contributions>Data Metric</a></p></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/people/features_1.webp","[Features 1](#3-my-research-contributions)","")'><img src=/images/people/features_1.webp alt="[Features 1](#3-my-research-contributions)"></a><p class=gallery-title><a href=#3-my-research-contributions>Features 1</a></p></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/people/features_2.webp","[Features 2](#3-my-research-contributions)","")'><img src=/images/people/features_2.webp alt="[Features 2](#3-my-research-contributions)"></a><p class=gallery-title><a href=#3-my-research-contributions>Features 2</a></p></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/people/im_coding.webp","[Image Coding](#3-my-research-contributions)","Using nonlinear perceptual image representations is critical to improve JPEG (see the gain in visual quality at 1 bit/pix). Measuring subjective distortion (the numbers at the bottom) is another vision-related problem we addressed (see below).")'><img src=/images/people/im_coding.webp alt="[Image Coding](#3-my-research-contributions)"></a><p class=gallery-title><a href=#3-my-research-contributions>Image Coding</a></p><div class=gallery-description><p>Using nonlinear perceptual image representations is critical to improve JPEG (see the gain in visual quality at 1 bit/pix). Measuring subjective distortion (the numbers at the bottom) is another vision-related problem we addressed (see below).</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/people/video_coding.webp","[Video Coding](#3-my-research-contributions)","Improved bit allocation according to nonlinear perception model (right vs left) is critical to improve MPEG video coding with regard to improved optical flow computation (bottom vs top).")'><img src=/images/people/video_coding.webp alt="[Video Coding](#3-my-research-contributions)"></a><p class=gallery-title><a href=#3-my-research-contributions>Video Coding</a></p><div class=gallery-description><p>Improved bit allocation according to nonlinear perception model (right vs left) is critical to improve MPEG video coding with regard to improved optical flow computation (bottom vs top).</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/people/ruidos_great.webp","[Image Restoration](#3-my-research-contributions)","Regularization functionals based on nonlinear perception models and signal smoothing according to image statistics in the wavelet domain help in image restoration.")'><img src=/images/people/ruidos_great.webp alt="[Image Restoration](#3-my-research-contributions)"></a><p class=gallery-title><a href=#3-my-research-contributions>Image Restoration</a></p><div class=gallery-description><p>Regularization functionals based on nonlinear perception models and signal smoothing according to image statistics in the wavelet domain help in image restoration.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/people/flor1.webp","[Color Constancy](#3-my-research-contributions)","")'><img src=/images/people/flor1.webp alt="[Color Constancy](#3-my-research-contributions)"></a><p class=gallery-title><a href=#3-my-research-contributions>Color Constancy</a></p></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/people/flor2.webp","[Color Constancy and White Balance](#3-my-research-contributions)","These adaptation problems reduce to manifold matching in different illumination conditions. We proposed linear and nonlinear solutions to this geometric problem.")'><img src=/images/people/flor2.webp alt="[Color Constancy and White Balance](#3-my-research-contributions)"></a><p class=gallery-title><a href=#3-my-research-contributions>Color Constancy and White Balance</a></p><div class=gallery-description><p>These adaptation problems reduce to manifold matching in different illumination conditions. We proposed linear and nonlinear solutions to this geometric problem.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/people/metrics.webp","[Subjective image/video distortion metrics](#3-my-research-contributions)","Observer&#39;s opinion (ground truth in the vertical axis) is better correlated with our Euclidean distance in nonlinear perceptual domains (right) than with the widely used Structural Similarity Index (left).")'><img src=/images/people/metrics.webp alt="[Subjective image/video distortion metrics](#3-my-research-contributions)"></a><p class=gallery-title><a href=#3-my-research-contributions>Subjective image/video distortion metrics</a></p><div class=gallery-description><p>Observer&rsquo;s opinion (ground truth in the vertical axis) is better correlated with our Euclidean distance in nonlinear perceptual domains (right) than with the widely used Structural Similarity Index (left).</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/people/clasi1.webp","[Image Classification](#3-my-research-contributions)","")'><img src=/images/people/clasi1.webp alt="[Image Classification](#3-my-research-contributions)"></a><p class=gallery-title><a href=#3-my-research-contributions>Image Classification</a></p></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/people/clasi2.webp","[Image Classification 2](#3-my-research-contributions)","Classifiers based on flexible features adapted to the data (such as RBIG, SPCA, PPA, DRR) are robust to changes in acquisition conditions (adaptivity implies no retraining is needed).")'><img src=/images/people/clasi2.webp alt="[Image Classification 2](#3-my-research-contributions)"></a><p class=gallery-title><a href=#3-my-research-contributions>Image Classification 2</a></p><div class=gallery-description><p>Classifiers based on flexible features adapted to the data (such as RBIG, SPCA, PPA, DRR) are robust to changes in acquisition conditions (adaptivity implies no retraining is needed).</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/people/NASA.webp","[HAL at SPIE Human Vision and Electronic Imaging 2001](#3-my-research-contributions)","Dr. Rosalind Picard actually hold the HAL-9000 eye in front of us in her invited talk at the conference (eye at the bottom right). She temporarily got it under Metro Goldwyn Mayer permission. Unfortunately Dr. Bowman (top right) was not attending the conference. Nevertheless, LittleHAL (my machine at NASA, bottom-left) showed no &#39;Computer Malfunction&#39; message while I attended the talk. However, not all my current students get the HAL-9000 story... more on this communication challenge in the teaching section below.")'><img src=/images/people/NASA.webp alt="[HAL at SPIE Human Vision and Electronic Imaging 2001](#3-my-research-contributions)"></a><p class=gallery-title><a href=#3-my-research-contributions>HAL at SPIE Human Vision and Electronic Imaging 2001</a></p><div class=gallery-description><p>Dr. Rosalind Picard actually hold the HAL-9000 eye in front of us in her invited talk at the conference (eye at the bottom right). She temporarily got it under Metro Goldwyn Mayer permission. Unfortunately Dr. Bowman (top right) was not attending the conference. Nevertheless, LittleHAL (my machine at NASA, bottom-left) showed no &lsquo;Computer Malfunction&rsquo; message while I attended the talk. However, not all my current students get the HAL-9000 story&mldr; more on this communication challenge in the teaching section below.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/people/sept_11_NY.webp","[Bayesian events by 2001](#3-my-research-contributions)","The scientific achievement preferred by my students is the fact that, being a theoretician, Bayes theory didn&#39;t stopped me from doing some experimental measures (pictures). I was at Time Cafe enjoying this preprint [Larry Maloney](http://www.psych.nyu.edu/maloney/) had told me about some days before. Nevertheless, people was grouping at the other side of Lafayette str. looking at something. I came back to the office to get my camera. It was NYU, september the 11th 2001, about 9:30 AM. [See the measurements here](http://www.uv.es/jmalo/personal/las_torres/las_torres.html).")'><img src=/images/people/sept_11_NY.webp alt="[Bayesian events by 2001](#3-my-research-contributions)"></a><p class=gallery-title><a href=#3-my-research-contributions>Bayesian events by 2001</a></p><div class=gallery-description><p>The scientific achievement preferred by my students is the fact that, being a theoretician, Bayes theory didn&rsquo;t stopped me from doing some experimental measures (pictures). I was at Time Cafe enjoying this preprint <a href=http://www.psych.nyu.edu/maloney/>Larry Maloney</a> had told me about some days before. Nevertheless, people was grouping at the other side of Lafayette str. looking at something. I came back to the office to get my camera. It was NYU, september the 11th 2001, about 9:30 AM. <a href=http://www.uv.es/jmalo/personal/las_torres/las_torres.html>See the measurements here</a>.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/people/alumnii.webp","[Pictures taken by the students of Color Science](#4-my-teaching-activities-like-richard-dawkins-in-a-republican-convention)","[One of the exercises](https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/edit_rakel.m) in those lectures consisted of representing the color of their pictures in CIE XYZ and editing the hue, chroma and lightness in CIE Lab or alternative color appearance models using [COLORLAB](../../../../code/vision_and_color/colorlab/content).")'><img src=/images/people/alumnii.webp alt="[Pictures taken by the students of Color Science](#4-my-teaching-activities-like-richard-dawkins-in-a-republican-convention)"></a><p class=gallery-title><a href=#4-my-teaching-activities-like-richard-dawkins-in-a-republican-convention>Pictures taken by the students of Color Science</a></p><div class=gallery-description><p><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/edit_rakel.m>One of the exercises</a> in those lectures consisted of representing the color of their pictures in CIE XYZ and editing the hue, chroma and lightness in CIE Lab or alternative color appearance models using <a href=../../../../code/vision_and_color/colorlab/content>COLORLAB</a>.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/people/alumna.webp","[Rakel](#4-my-teaching-activities-like-richard-dawkins-in-a-republican-convention)","")'><img src=/images/people/alumna.webp alt=[Rakel](#4-my-teaching-activities-like-richard-dawkins-in-a-republican-convention)></a><p class=gallery-title><a href=#4-my-teaching-activities-like-richard-dawkins-in-a-republican-convention>Rakel</a></p></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/people/doctores.webp","[PhD Students](#4-my-teaching-activities-like-richard-dawkins-in-a-republican-convention)","Dr. Irene Epifanio (top left) walking the red carpet to receive the best-thesis award in Physics and Maths 2003 from the President of the Generaliat Valenciana [IrenePhD03] (co-advised by Guillermo Ayala). Dr. Juan Guti√©rrez (right) at the black board explaining image statistics (top) and Bayesian inference in image wavelet domains (bottom). Note the &#39;tru√±o matem√©tico&#39; ommited in the derivation [JuanPhD05] (co-advised by Paco Ferri). Dr. Valero Laparra and I (bottom left) at the Keops pyramid having a nonlinear feature extraction ecstasy after our visit to the smoking room at the Max Plank Biological Cybernetics Institute invited by M. Bethge (middle left) [ValeroPhD11] (co-advised by Gustau Camps).")'><img src=/images/people/doctores.webp alt="[PhD Students](#4-my-teaching-activities-like-richard-dawkins-in-a-republican-convention)"></a><p class=gallery-title><a href=#4-my-teaching-activities-like-richard-dawkins-in-a-republican-convention>PhD Students</a></p><div class=gallery-description><p>Dr. Irene Epifanio (top left) walking the red carpet to receive the best-thesis award in Physics and Maths 2003 from the President of the Generaliat Valenciana [IrenePhD03] (co-advised by Guillermo Ayala). Dr. Juan Guti√©rrez (right) at the black board explaining image statistics (top) and Bayesian inference in image wavelet domains (bottom). Note the &rsquo;tru√±o matem√©tico&rsquo; ommited in the derivation [JuanPhD05] (co-advised by Paco Ferri). Dr. Valero Laparra and I (bottom left) at the Keops pyramid having a nonlinear feature extraction ecstasy after our visit to the smoking room at the Max Plank Biological Cybernetics Institute invited by M. Bethge (middle left) [ValeroPhD11] (co-advised by Gustau Camps).</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/people/scientist_at_cambridge.webp","[Scientist at Cambridge](#4-my-teaching-activities-like-richard-dawkins-in-a-republican-convention)","[Francisco J. Hernandez](http://www.zoo.cam.ac.uk/directory/francisco-hernandez-heras) physicist at the Zoology Dept. of Cambridge Univ. (right) telling us (Gustau Camps, center, and Jes√∫s Malo, left) about his ideas to [improve science funding in Spain (Nature, jan. 2012)](http://blogs.nature.com/news/2012/01/spanish-researchers-petition-for-taxpayer-donations.html), as we entered the Trinity College (picture taken by Valero Laparra).")'><img src=/images/people/scientist_at_cambridge.webp alt="[Scientist at Cambridge](#4-my-teaching-activities-like-richard-dawkins-in-a-republican-convention)"></a><p class=gallery-title><a href=#4-my-teaching-activities-like-richard-dawkins-in-a-republican-convention>Scientist at Cambridge</a></p><div class=gallery-description><p><a href=http://www.zoo.cam.ac.uk/directory/francisco-hernandez-heras>Francisco J. Hernandez</a> physicist at the Zoology Dept. of Cambridge Univ. (right) telling us (Gustau Camps, center, and Jes√∫s Malo, left) about his ideas to <a href=http://blogs.nature.com/news/2012/01/spanish-researchers-petition-for-taxpayer-donations.html>improve science funding in Spain (Nature, jan. 2012)</a>, as we entered the Trinity College (picture taken by Valero Laparra).</p></div></div><div id=imageModal class=modal><span class=modal-close onclick=closeModal()>&#215;</span>
<img class=modal-content id=modalImage><div id=modalCaption class=modal-caption></div></div></div></aside></div></div></main></body><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js integrity=sha384-C6RzsynM9kWDrMNeT87bh95OGNyZPhcTNXj1NW7RuBCsyN/o0jlpcV8Qyq46cDfL crossorigin=anonymous defer></script><script type=text/javascript src=https://cdn.jsdelivr.net/gh/pcooksey/bibtex-js@1.0.0/src/bibtex_js.min.js defer></script></html>