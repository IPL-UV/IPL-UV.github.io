<html lang="en"><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
<meta content="" name="description"/>
<meta content="" name="author"/>
<link href="/images/adicionales/favicon.ico" rel="icon"/>
<title>ISP - Regression software</title>
<!-- Bootstrap core CSS -->
<link href="./ISP - Regression software_files/bootstrap.min.css" rel="stylesheet"/>
<!-- Bootstrap theme -->
<link href="./ISP - Regression software_files/bootstrap-theme.min.css" rel="stylesheet"/>
<!-- Custom styles for this template -->
<!-- <link href="theme.css" rel="stylesheet"> -->
<!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
<!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
<!-- Local styles -->
<link href="./ISP - Regression software_files/styles.css" rel="stylesheet"/>


</head>


<body role="document" style="padding-top: 50px;">
<div class="container">
<h3><b>Regression and system identification</b></h3>
<div class="panel panel-default">
<div class="panel-heading">
<h1 class="panel-title"></h1><h4><b>simpleR v2.1: simple Regression toolbox</b></h4>
</div>
<div class="panel-body">
<div class="col-md-2">
<a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/blob/main/code/soft_regression/simpler-2-1.zip"> <img height="121" src="/images/adicionales/boxplot.webp" width="160"/> </a>
</div>
<div class="col-md-6">
<h5><p class="text-justify">The simple Regression toolbox, simpleR, contains a set of functions in Matlab to illustrate the capabilities of several statistical regression algorithms. simpleR contains simple educational code for linear regression (LR), decision trees (TREE), neural networks (NN), support vector regression (SVR), kernel ridge regression (KRR), aka Least Squares SVM, Gaussian Process Regression (GPR), and Variational Heteroscedastic Gaussian Process Regression (VHGPR). We also include a dataset of collected spectra and associated chlorophyll content to illustrate the training/testing procedures. This is just a demo providing a default initialization. Training is not at all optimized. Other initializations, optimization techniques, and training strategies may be of course better suited to achieve improved results in this or other problems. We just did it in the standard way for illustration and educational purposes, as well as to disseminate these models.</p>
<p>Last version of the toolbox is in GitHub: <a href="https://github.com/IPL-UV/simpleR">https://github.com/IPL-UV/simpleR</a>.
</p></h5>
</div>
<div class="col-md-4">
<h6><b>References</b><br/>
<li>Retrieval of biophysical parameters with heteroscedastic Gaussian processes
Lázaro-Gredilla, M. and Titsias, M.K. and Verrelst, J. and Camps-Valls, G.
IEEE Geoscience and Remote Sensing Letters 11 (4):838-842, 2014 </li>
<li>Prediction of daily global solar irradiation using temporal Gaussian processes
Salcedo-Sanz, S. and Casanova-Mateo, C. and Muñoz-Marí, J. and Camps-Valls, G.
IEEE Geoscience and Remote Sensing Letters 11 (11):1936-1940, 2014 </li>
</h6>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<h1 class="panel-title"></h1><h4><b>A comprehensive Gaussian processes repository.</b></h4>
</div>
<div class="panel-body">
<div class="col-md-2">
<a href="https://github.com/jejjohnson/gp_model_zoo"> <img height="121" src="/images/adicionales/GP.webp" width="160"/> </a>
</div>
<div class="col-md-6">
<h5><p class="text-justify">A comprehensive repo on Gaussian processes code, literature and model zoo.</p>
</h5>
</div>
<div class="col-md-4">
<h6><b>References</b><br/>
<li>A comprehensive Gaussian processes repository. J. E. Johnson, Tech Rep 2019/12B, Universitat de Valencia, 2019 </li>
</h6>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<h1 class="panel-title"></h1><h4><b>Gaussian processes with input noise</b></h4>
</div>
<div class="panel-body">
<div class="col-md-2">
<a href="https://github.com/jejjohnson/uncertain_gps"> <img height="121" src="/images/adicionales/GP.webp" width="160"/> </a>
</div>
<div class="col-md-6">
<h5><p class="text-justify">Gaussian processes with input noise</p>
</h5>
</div>
<div class="col-md-4">
<h6><b>References</b><br/>
<li> Accounting for Input Noise in Gaussian Process Parameter Retrieval
Johnson, J. E. and Laparra, V. and Camps-Valls, G.
IEEE Geoscience and Remote Sensing Letters 17 (3) :391-395, 2020  </li>
</h6>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<h1 class="panel-title"></h1><h4><b>Joint Gaussian processes</b></h4>
</div>
<div class="panel-body">
<div class="col-md-2">
<a href="https://dhsvendsen.github.io/blogpost/basicjgp.html"> <img height="130" src="/images/adicionales/jgp.webp" width="170"/> </a>
</div>
<div class="col-md-6">
<h5><p class="text-justify">We introduce a nonlinear nonparametric regression model which combines knowledge from real observations and simulated data from physical models. The inversion is performed taking into account jointly both real observations and RTM-simulated data. The proposed Joint Gaussian Process (JGP) provides a solid framework for exploiting the regularities between the two types of data. The JGP automatically detects the relative quality of the simulated and real data, and combines them accordingly. This occurs by learning an additional hyper-parameter w.r.t. a standard GP model, and fitting parameters through maximizing the pseudo-likelihood of the real observations.</p>
</h5>
</div>
<div class="col-md-4">
<h6><b>References</b><br/>
<li>Svendsen, D. H., Martino, L., Campos-Taberner, M., García-Haro, F. J., &amp; Camps-Valls, G. (2017). Joint Gaussian processes for biophysical parameter retrieval. IEEE Transactions on Geoscience and Remote Sensing, 56(3), 1718-1727. </li>
<li>Svendsen, Daniel Heestermans, et al. "Joint Gaussian processes for inverse modeling." 2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS). IEEE, 2017. </li>
<li>Bonilla, Edwin V., Kian M. Chai, and Christopher Williams. "Multi-task Gaussian process prediction." Advances in neural information processing systems. 2008. </li>
</h6>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<h1 class="panel-title"></h1><h4><b>Deep Gaussian processes for biophysical parameter estimation</b></h4>
</div>
<div class="panel-body">
<div class="col-md-2">
<a href="./subpages/ISP - Software.html"> <img height="180" src="/images/adicionales/dgp.webp" width="160"/> </a>
</div>
<div class="col-md-6">
<h5><p class="text-justify">The interaction of electromagnetic radiation with a planetary atmosphere and vegetation is a complex physical process, to the extent that often a single (shallow) GP models cannot capture feature relations for inversion. This motivates the use of deeper hierarchical architectures, while still preserving the desirable properties of GPs. We therefore employ the use of deep Gaussian Processes (DGPs) for bio-geo-physical model inversion. Unlike shallow GP models, DGPs account for complicated (modular, hierarchical) processes, provide an efficient solution that scales well to big datasets, and improve prediction accuracy over their single layer counterpart.</p>
</h5>
</div>
<div class="col-md-4">
<h6><b>References</b><br/>
<li>Deep Gaussian Processes for Parameter Retrieval and Model Inversion
Daniel H. Svendsen, Pablo M. Alvarez, Ana Belen Ruescas, Rafael Molina and Gustau Camps-Valls, Submitted, 2020. </li>
<li>Svendsen, Daniel Heestermans, et al. "Deep Gaussian Processes for Geophysical Parameter Retrieval." IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium. IEEE, 2018.</li>
<li>Salimbeni, Hugh, and Marc Deisenroth. "Doubly stochastic variational inference for deep Gaussian processes." Advances in Neural Information Processing Systems. 2017.</li>
</h6>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<h1 class="panel-title"></h1><h4><b>Fair Kernel Learning</b></h4>
</div>
<div class="panel-body">
<div class="col-md-2">
<a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/blob/main/code/soft_regression/FairKernelLearning2017.zip"> <img height="150" src="/images/adicionales/FairKernelLearning2017.webp" width="170"/> </a>
</div>
<div class="col-md-6">
<h5>
<p class="text-justify">New social and economic activities massively exploit big data and machine learning algorithms to do inference on people's lives. Applications include automatic curricula evaluation, wage determination, and risk assessment for credits and loans. Recently, many governments and institutions have raised concerns about the lack of fairness, equity and ethics in machine learning to treat these problems. It has been shown that not including sensitive features that bias fairness, such as gender or race, is not enough to mitigate the discrimination when other related features are included. Instead, including fairness in the objective function has been shown to be more efficient.

We present novel fair regression and dimensionality reduction methods built on a previously proposed fair classification framework. Both methods rely on using the Hilbert Schmidt independence criterion as the fairness term. Unlike previous approaches, this allows us to simplify the problem and to use multiple sensitive variables simultaneously. Replacing the linear formulation by kernel functions allows the methods to deal with nonlinear problems. For both linear and nonlinear formulations the solution reduces to solving simple matrix inversions or generalized eigenvalue problems. This simplifies the evaluation of the solutions for different trade-off values between the predictive error and fairness terms. We illustrate the usefulness of the proposed methods in toy examples, and evaluate their performance on real world experiments of predicting income using gender and/or race discrimination as sensitive variables.
</p></h5>
</div>
<div class="col-md-4">
<h6><b>References</b><br/>
<li> Fair Kernel Learning. Adrián Pérez-Suay, Valero Laparra, Gonzalo Mateo-García, Jordi Muñoz-Marí, Luis Gómez-Chova and Gustau Camps-Valls. ECML PKDD 2017 (Accepted)</li>
</h6>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<h1 class="panel-title"></h1><h4><b>MSVR: Multioutput Support Vector Regression</b></h4>
</div>
<div class="panel-body">
<div class="col-md-2">
<a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/blob/main/code/soft_regression/msvr-2-1.zip"> <img height="121" src="/images/adicionales/svm_map.webp" width="160"/> </a>
</div>
<div class="col-md-6">
<h5>
<p class="text-justify">Standard SVR formulation only considers the single-output problem. In the case of several output variables, other methods (neural networks, kernel ridge regression) must be deployed, but the good properties of SVR are lost: hinge-loss function and sparsity. The proposed model M-SVR extends the single-output SVR by taking into account the nonlinear relations between features but also among the output variables, which are typically inter-dependent.</p>
</h5>
</div>
<div class="col-md-4">
<h6><b>References</b><br/>
<li>Multioutput support vector regression for remote sensing biophysical parameter estimation
Tuia, D. and Verrelst, J. and Alonso, L. and Perez-Cruz, F. and Camps-Valls, G.
IEEE Geoscience and Remote Sensing Letters 8 (4):804-808, 2011</li>
</h6>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<h1 class="panel-title"></h1><h4><b>Epsilon-Huber Support Vector Regression</b></h4>
</div>
<div class="panel-body">
<div class="col-md-2">
<a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/blob/main/code/soft_regression/svr_e_huber.zip"> <img height="121" src="/images/adicionales/losses_web.webp" width="140"/> </a>
</div>
<div class="col-md-6">
<h5>
<p class="text-justify">The combination of the classical Vapnik's e-insensitive loss function and the Huber cost function leads to enhanced performance when different noise sources are present in the data. This cost function has been applied to system identification, gamma-filtering, and to SVR. </p>
</h5>
</div>
<div class="col-md-4">
<h6><b>References</b><br/>
<li>A unified SVM framework for signal estimation
Rojo-Álvarez, J.L. and Martínez-Ramón, M. and Muñoz-Marí, J. and Camps-Valls, G.
Digital Signal Processing: A Review Journal 26 (1):1-20, 2014 </li>
</h6>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<h1 class="panel-title"></h1><h4><b>SS-SVR: Semi-supervised Support Vector Regression</b></h4>
</div>
<div class="panel-body">
<div class="col-md-2">
<a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/blob/main/code/soft_regression/demoSemiSVR.zip"> <img height="111" src="/images/adicionales/ssvr.webp" width="160"/> </a>
</div>
<div class="col-md-6">
<h5>
<p class="text-justify">It contains two kernel-based methods for semi-supervised regression. The methods rely on building a graph or hypergraph Laplacian with both the available labeled and unlabeled data, which is further used to deform the training kernel matrix. The deformed kernel is then used for support vector regression (SVR). Given the high computational burden involved, we present two alternative formulations based on the Nyström method and the Incomplete Cholesky Factorization to achieve operational processing times. The semi-supervised SVR algorithms are successfully tested in multiplatform LAI estimation and oceanic chlorophyll concentration prediction. Experiments are carried out with both multispectral and hyperspectral data, demonstrating good generalization capabilities when low number of labeled samples are available, which is usually the case in biophysical parameter estimation.</p>
</h5>
</div>
<div class="col-md-4">
<h6><b>References</b><br/>
<li>Biophysical parameter estimation with a semisupervised support vector machine
Camps-Valls, G. and Munoz-Marí, J. and Gómez-Chova, L. and Richter, K. and Calpe-Maravilla, J.
IEEE Geoscience and Remote Sensing Letters 6 (2):248-252, 2009 </li>
</h6>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<h1 class="panel-title"></h1><h4><b>ARTMO: Automated Radiative Transfer Models Operator</b></h4>
</div>
<div class="panel-body">
<div class="col-md-2">
<a href="http://ipl.uv.es/artmo/"> <img height="151" src="/images/adicionales/artmo.webp" width="140"/> </a>
</div>
<div class="col-md-6">
<h5>
<p class="text-justify">The in-house developed Automated Radiative Transfer Models Operator (ARTMO) Graphic User Interface (GUI) is a software package that provides essential tools for running and inverting a suite of plant RTMs, both at the leaf and at the canopy level. ARTMO facilitates consistent and intuitive user interaction, thereby streamlining model setup, running, storing and spectra output plotting for any kind of optical sensor operating in the visible, near-infrared and shortwave infrared range (400-2500 nm). the ARTMO package includes physical, statistical and hybrid inversion and model emulation. Some modules are pure machine learning techniques for regression, active learning, dimensionality reduction and feature ranking!</p>
</h5>
</div>
<div class="col-md-4">
<h6><b>References</b><br/>
<li>Toward a semiautomatic machine learning retrieval of biophysical parameters
Caicedo, J.P.R. and Verrelst, J. and Munoz-Mari, J. and Moreno, J. and Camps-Valls, G.
IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 7 (4):1249-1259, 2014 </li>
</h6>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<h1 class="panel-title"></h1><h4><b>KARMA: Kernel AutoRegressive Moving Average with the Support Vector Machine</b></h4>
</div>
<div class="panel-body">
<div class="col-md-2">
<a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/blob/main/code/soft_regression/karma.zip"> <img height="121" src="/images/code/lorentz.webp" width="140"/> </a>
</div>
<div class="col-md-6">
<h5>
<p class="text-justify">Nonlinear system identification based on Support Vector Machines (SVM) has been usually addressed by means of the standard SVM regression (SVR), which can be seen as an implicit nonlinear Auto Regressive and Moving Average (ARMA) model in some Reproducing Kernel Hilbert Spaces (RKHS). The proposal here is twofold: First, the explicit consideration of an ARMA model in RKHS (SVM-ARMA2K) is originally proposed. Second, a general class of SVM-based system identification nonlinear models is presented, based on the use of composite Mercer's kernels.</p>
</h5>
</div>
<div class="col-md-4">
<h6><b>References</b><br/>
<li>Support vector machines for nonlinear Kernel ARMA system identification
Martínez-Ramón, M. and Rojo-Álvarez, J.L. and Camps-Valls, G. and Muñoz-Marí, J. and Navia-Vázquez, A. and Soria-Olivas, E. and Figueiras-Vidal, A.R.
IEEE Transactions on Neural Networks 17 (6):1617-1622, 2006 </li>
<li>A unified SVM framework for signal estimation
Rojo-Álvarez, J.L. and Martínez-Ramón, M. and Muñoz-Marí, J. and Camps-Valls, G.
Digital Signal Processing: A Review Journal 26 (1):1-20, 2014 </li>
</h6>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<h1 class="panel-title"></h1><h4><b>ARX-RVM: Autorregressive eXogenous Relevance Vector Machine</b></h4>
</div>
<div class="panel-body">
<div class="col-md-2">
<a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/blob/main/code/soft_regression/karma_rvm.zip"> <img height="121" src="/images/adicionales/sbprior.webp" width="140"/> </a>
</div>
<div class="col-md-6">
<h5>
<p class="text-justify">Nonlinear system identification based on relevance vector machines (RVMs) has been traditionally addressed by
stacking the input and/or output regressors and then performing
standard RVM regression. Here we introduce a full family of
composite kernels to integrate the input and output information
in the mapping function. An improved trade-off between accuracy
and sparsity is obtained in several benchmark problems. Also, the
ARX-RVM yields confidence intervals for the predictions, and it is less
sensitive to free parameter selection.</p>
</h5>
</div>
<div class="col-md-4">
<h6><b>References</b><br/>
<li>Nonlinear system identification with composite relevance vector machines
Camps-Valls, G. and Martínez-Ramón, M. and Rojo-Álvarez, J.L. and Muñoz-Marí, J.
IEEE Signal Processing Letters 14 (4):279-282, 2007  </li>
<li>A unified SVM framework for signal estimation
Rojo-Álvarez, J.L. and Martínez-Ramón, M. and Muñoz-Marí, J. and Camps-Valls, G.
Digital Signal Processing: A Review Journal 26 (1):1-20, 2014 </li>
</h6>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<h1 class="panel-title"></h1><h4><b>KSNR: Kernel Signal to Noise Ratio</b></h4>
</div>
<div class="panel-body">
<div class="col-md-2">
<a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/blob/main/code/soft_regression/ksnr.zip"> <img height="121" src="/images/adicionales/signal-noise.webp" width="140"/> </a>
</div>
<div class="col-md-6">
<h5>
<p class="text-justify">The kernel signal to noise ratio (KSNR) considers a least squares regression model that
maximizes the signal variance while minimizes the estimated noise variance in a reproducing kernel
Hilbert space (RKHS). The KSNR can be used in any kernel method to deal with correlated (possibly non-Gaussian) noise.
KSNR yields more fitted solutions and extracts more noise-free features when confronted with standard approaches.</p>
</h5>
</div>
<div class="col-md-4">
<h6><b>References</b><br/>
<li>Learning with the kernel signal to noise ratio
Gomez-Chova, L. and Camps-Valls, G.
IEEE International Workshop on Machine Learning for Signal Processing, MLSP 2012 </li>
</h6>
</div>
</div>
</div>
<!-- NOT YET READY BORDERLINE


<div class="panel panel-default">
<div class="panel-heading">
<h1 class="panel-title"><h4><b>&gamma;-SVM filtering</h4></b></h1>
</div>
<div class="panel-body">
<div class="col-md-2">
<a href="code/regression/gamma_family.zip"> <img src="images/gammafilter.jpg" height="121" width="140" /> </a>
</div>
<div class="col-md-6">
<h5>
<p class="text-justify">A family of kernel methods, based on the gamma-filter structure, is introduced for non-linear system
identification and time series prediction. The kernel trick allows us to develop the natural non-linear
extension of the (linear) support vector machine (SVM) gamma-filter,
but this approach yields a rigid system model without non-linear cross relation
between time-scales. Several functional analysis properties allow us to develop a full, principled family
of kernel gamma-filters. The improved performance in several application examples suggests that a more
appropriate representation of signal states is achieved. A second step is the development of an explicitly
recursive gamma-filter in Hilbert spaces.</p>
</h5>
</div>
<div class="col-md-4">
<h6><b>References</b><br>
<li>Robust γ-filter using support vector machines
Camps-Valls, G. and Martínez-Ramón, M. and Rojo-Álvarez, J.L. and Soria-Olivas, E.
Neurocomputing 62 (1-4):493-499, 2004  </li>
<li>Learning non-linear time-scales with kernel gamma-filters
Camps-Valls, G. and Muñoz-Marí, J. and Martínez-Ramón, M. and Requena-Carrión, J. and Rojo-Álvarez, J.L.
Neurocomputing 72 (4-6):1324-1328, 2009  </li>
<li>Explicit recursive and adaptive filtering in reproducing kernel hilbert spaces
Tuia, D. and Munoz-Mari, J. and Rojo-Alvarez, J.L. and Martinez-Ramon, M. and Camps-Valls, G.
IEEE Transactions on Neural Networks and Learning Systems 25 (7):1413-1419, 2014 </li>
</h6>
</div>
</div>
</div>


-->
</div><!-- /.container -->
<!-- Bootstrap core JavaScript
    ================================================== -->
<!-- Placed at the end of the document so the pages load faster -->
<script src="./ISP - Regression software_files/jquery-1.12.4.min.js"></script>
<script src="./ISP - Regression software_files/bootstrap.min.js"></script>
</body></html>
