<!DOCTYPE html>

<!-- saved from url=(0049)./vista_toolbox.html -->
<html lang="en"><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
<meta content="" name="description"/>
<meta content="" name="author"/>
<link href="./images/favicon.ico" rel="icon"/>
<title>ISP - Software: VistaQuality</title>
<!-- Bootstrap core CSS -->
<link href="./vista_toolbox_files/bootstrap.min.css" rel="stylesheet"/>
<!-- Bootstrap theme -->
<link href="./vista_toolbox_files/bootstrap-theme.min.css" rel="stylesheet"/>
<!-- Custom styles for this template -->
<!-- <link href="theme.css" rel="stylesheet"> -->
<!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
<!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
<!-- Local styles -->
<link href="./vista_toolbox_files/styles.css" rel="stylesheet"/>
</head>
<body role="document" style="padding-top: 50px;">
<!-- Fixed navbar -->
<div class="row" id="Top">
<hr/>
</div>
<div class="container">
<div class="row"> <!------------------------------ TITLE --------------------------------------->
<div class="col-md-3">
</div>
<div class="col-md-6" style="width: 52%;text-align: left; margin-top: 0%; margin-left: 0%; margin-right: auto;">
<p><left></left></p><h1><span class="label label-info">VistaQualityTools: </span></h1><p></p>
<p><left></left></p><h2><span class="label label-info"><span style="font-style: italic;">The Image and Video Quality Toolbox based on Vision Models</span></span></h2><p></p><br/>
<p align="center" style="color:rgb(0,150,200)"><big><strong><span style="font-style: italic;">Contributors:</span></strong><br/>
<strong><span style="color:rgb(200,0,0)">Image:</span></strong> J. Malo, V. Laparra, J. Muñoz, I. Epifanio,<br/>
                                                                                     A.M. Pons, M. Martinez and E. Simoncelli<br/>
<strong><span style="color:rgb(200,0,0)">Video:</span></strong> J. Malo, J. Gutiérrez and A.B. Watson<br/>
                        jesus.malo@uv.es<br/>
<!---------------  <a href="paper/Plos_Orient_maps_iteration2.pdf"> </a> ------------------->
                        (c) Universitat de València 1996 - 2018</big><br/><br/>
</p></div>
<div class="col-md-3">
</div>
</div>
<div class="row"> <!--------------------------------------- PICTURE - ABSTRACT + INDEX - PICTURE --------------------------------------------->
<!--------------------------------------- IZQUIERDA --------------------------------------------->
<!--------------------------------------- IZQUIERDA --------------------------------------------->
<!--------------------------------------- IZQUIERDA --------------------------------------------->
<div class="col-md-3">
<div class="container" style="width: 120%;text-align: left; margin-top: -5%; margin-left: -30%; margin-top: -75%; margin-right: auto;">
<img alt="Jesus" class="img-responsive" src="/images/adicionales/BasesDist.webp"/>
<p align="justify"><strong>The problem:</strong><small>. Given an original image (top) distortions of different nature appear to have different perceptual effect (bottom). The challenge is computing a descriptor of distortion which is correlated
              with the opinion of observers collected in <a href="http://www.ponomarenko.info/tid2013.htm">subjectively rated databases</a>.
              The complexity of human vision implies that the Euclidean distance (or Mean Squared Error) is not a good proxy for subjective distortion.
              Nevertheless, the image quality problem goes beyond fitting any flexible model to maximize the correlation with subjective opinion (see [<a href="https://arxiv.org/abs/1801.09632">Front. Neurosci. 2018</a>]).</small></p><br/><br/><br/><br/><br/>
</div>
</div>
<!--------------------------------------- CENTRO --------------------------------------------->
<!--------------------------------------- CENTRO --------------------------------------------->
<div class="col-md-6" style="width: 50%;text-align: left; margin-top: 0%; margin-left: -1%; margin-right: auto;">
<p align="justify"> The image distortion measures in <strong>VistaQualityTools</strong> are based on distances between the original and the distorted scenes in the visual response domain. Therefore, they rely on the cortical descriptions in <a href="../../soft_visioncolor/subpages/vistamodels.html"><strong>VistaModels</strong></a>, including metrics based on (a) normalized DCTs, (b) normalized orthonormal wavelets, and (c) multi-layer models with normalized overcomplete wavelets.
            All these measures substantially overperform the widely acclaimed SSIM.<br/>
            Our video quality measure developed at the NASA Ames Research Center is based on the same visual response principle. It achieved the 2nd best performance in the VQEG evaluation phase II.</p><br/>
<div class="list-group">
<a class="list-group-item" href="./vista_toolbox.html#download">
<h4 class="list-group-item-heading" style="color:rgb(255,50,50)"><strong>Download Toolboxes!</strong></h4>
</a>
<a class="list-group-item" href="./vista_toolbox.html#Empirical">
<h4 class="list-group-item-heading" style="color:rgb(0,150,200)"><strong>(A) Image Quality Metrics:</strong><br/><br/>
                       * Performance in subjectively rated databases</h4>
<ul>
<li>V1_model_DCT_DN_color</li>
<li>V1_model_wavelet_DN_color</li>
<li>BioMultiLayer_L_NL_color (deep-DN)</li>
<li>Comparison with SSIM and RMSE</li>
</ul>
</a>
<a class="list-group-item" href="./vista_toolbox.html#test1">
<h4 class="list-group-item-heading" style="color:rgb(0,150,200)">* Saturation with linearly increasing energy</h4>
</a>
<a class="list-group-item" href="./vista_toolbox.html#test1">
<h4 class="list-group-item-heading" style="color:rgb(0,150,200)">* Constant MSE but perceptually increasing distortion</h4>
</a>
<a class="list-group-item" href="./vista_toolbox.html#models">
<h4 class="list-group-item-heading" style="color:rgb(0,150,200)">* Models and Distortion Measures</h4>
<ul>
<li>V1_model_DCT_DN_color: opponent colors, DCT + Divisive Normalization</li>
<li>V1_model_wavelet_DN_color: Opponent colors, Orthonormal Wavelets + Divisive Normalization</li>
<li>BioMultiLayer_L_NL_color (or deep-DN): psychophysically sensible cascade of L+NL layers: Spectral integration + Von Kries adaptation, ATD channels and Weber-like saturation, Overcomplete Wavelets + Divisive Normalization</li>
</ul>
</a>
<a class="list-group-item" href="./vista_toolbox.html#video">
<h4 class="list-group-item-heading" style="color:rgb(0,150,200)"><strong>(B) Video Quality:</strong><br/><br/>
                       * Spatio-temporal CSF and energy masking</h4>
<ul>
<li>Contrast definition</li>
<li>Sensitivity of the Standard Spatial Observer</li>
<li>Temporal filtering</li>
<li>Non quadratic spatial and temporal summation</li>
<li>Local energy masking</li>
<li>Field replication model and global shift compensation</li>
</ul>
</a>
<a class="list-group-item" href="./vista_toolbox.html#download">
<h4 class="list-group-item-heading" style="color:rgb(255,50,50)"><strong>Download Toolboxes!</strong></h4>
</a>
<a class="list-group-item" href="./vista_toolbox.html#references">
<h4 class="list-group-item-heading" style="color:rgb(0,150,200)"><strong>Citation and References</strong></h4>
</a>
</div>
</div>
<!--------------------------------------- DERECHA --------------------------------------------->
<!--------------------------------------- DERECHA --------------------------------------------->
<div class="col-md-3">
<div class="container" style="width: 140%;text-align: right; margin-left: -5%; margin-right: auto;">
<div class="row">
<div class="col-md-12" style="width: 100%;text-align: right; margin-left: 0%; margin-top: -45%; margin-right: auto;">
<img alt="" class="img-responsive" src="/images/adicionales/metricas2.webp"/><br/>
<p ;style="margin-top: -0%;text-align: left; margin-left: 15%" align="justify"><strong>Our solution:</strong><small>
                                             The scatter plots show the performance of two perceptual metrics in reproducing subjective opinion. On the one hand (in red)
                                             the widely acclaimed Structural SIMilarity index (SSIM) that received the <a href="https://youtu.be/e5-LCFGdgMA">EMMY Award of the American TV Industry in 2015</a>,
                                             and, on the other hand (in blue), our metric based on a cascade of L+NL layers [<a href="https://arxiv.org/abs/1711.00526">PLoS 2018</a>].</small></p>
</div>
</div>
</div>
</div>
</div>
<!-- %%%%%%%%%%%%%%%%%%%%% CUANDO PASE EL CHAPARRON VUELVE A INTRODUCIR EL NIVEL <h2> EN VERDE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
<!-- %%%%%%%%%%%%%%%%%%%%% CUANDO PASE EL CHAPARRON VUELVE A INTRODUCIR EL NIVEL <h2> EN VERDE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
<!-- %%%%%%%%%%%%%%%%%%%%% CUANDO PASE EL CHAPARRON VUELVE A INTRODUCIR EL NIVEL <h2> EN VERDE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
<!-- %%%%%%%%%%%%%%%%%%%%% CUANDO PASE EL CHAPARRON VUELVE A INTRODUCIR EL NIVEL <h2> EN VERDE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
<div class="row">
<div class="col-md-3">
<!-- %%%%%%%%%%%%%%%%%%%%% Nadie a la izquierda %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
</div>
<div class="col-md-6" style="width: 60%;text-align: left; margin-left: -10%; margin-right: auto;">
<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% EMPIRICAL MODELS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
<hr id="Empirical"/><br/><br/>
<a href="./vista_toolbox.html#Top"><small>Back to top</small></a>
<p align="justify"></p><h2><span class="label label-success">(A)    Image Quality Metrics</span></h2><p></p>
<p align="justify">The distortion metrics in VistaQualityTools rely on the three cortical models we have developed over the years (a) DCT transform and Divisive Normalization [IVC 1997, Displays 00, Patt.Rec.03, IEEE Trans.Im.Proc. 06], (b) Orthonormal Wavelets and Divisive Normalization [JOSA A 10, Neur.Comp. 10], and (c) Cascades of linear transforms and nonlinear saturations [PLoS 18, Front. Neurosci. 18].</p>
<p align="justify"></p><h3><span class="label label-warning">Performance in subjectively rated-databases</span></h3><p></p>
<table border="0" cellpadding="2" cellspacing="2" style="width: 170%; text-align: left; margin-left: -35%; margin-right: auto;">
<tbody>
<tr align="center"><td colspan="2" rowspan="1">
<img alt="a" class="img-responsive" src="/images/adicionales/metricas.webp"/>
</td>
</tr>
</tbody>
</table>
<br/>
<hr id="test1"/><br/><br/>
<a href="./vista_toolbox.html#Top"><small>Back to top</small></a>
<p align="justify"></p><h3><span class="label label-warning">Saturation of the distortion and perceptual differences in constant-MSE series</span></h3><p></p>
<p align="justify">Subjective distortion has two distinct properties: (a) visual degradation with linearly increasing Euclidean norm is highly visible for low energy and saturates for high energies,
               and (b) constant energy (or MSE) does not mean constant perceptual distortion. The image series below illustrate these properties and show how the metrics based on vision models
               (in blue) qualitatively follow the subjective behavior while MSE and SSIM fail to reproduce the behavior.</p>
<table border="0" cellpadding="2" cellspacing="2" style="width: 200%; text-align: left; margin-left: -45%; margin-right: auto;">
<tbody>
<tr align="center"><td colspan="2" rowspan="1">
<img alt="a" class="img-responsive" src="/images/adicionales/metricas3.webp"/>
</td>
</tr>
</tbody>
</table>
<br/>
<hr id="models"/><br/><br/>
<a href="./vista_toolbox.html#Top"><small>Back to top</small></a>
<p align="justify"></p><h3><span class="label label-success">Models and Distortion measures</span></h3><p></p>
<p align="justify"></p><h3><span class="label label-warning">1995 - 2008: Metric based on linear opponent channels, local-DCT and Div. Norm.</span></h3><p></p>
<p align="justify">This metric is based on an invertible representation originally tuned to reproduce contrast response curves [Pons PhD Thesis, 1997].
               It was applied to reproduce subjective distortion opinion [Im.Vis.Comp.97, Displays99] and to improve the perceptual quality of JPEG and MPEG  through 
               (a) transform coding of the achromatic channel [Eletr.Lett95, Eletr.Lett99, Im.Vis.Comp.00 Patt.Recog.03, IEEE TNN 05, IEEE TIP 06a, IEEE TIP 06b, JMLR08], 
               (b) the color channels [RPSP12], and (c) by improving the motion estimation [LNCS97, Eletr.Lett98, Eletr.Lett00a, Eletr.Lett00b, IEEE TIP 01].</p>
<p align="justify"><strong>Download the Toolbox!:</strong><strong><a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_imvideo/vista_toolbox/V1_model_DCT_DN_color.zip"> V1_model_DCT_DN_color.zip (74MB) </a></strong><a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_imvideo/vista_toolbox/V1_model_DCT_DN_color.zip"> </a> <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_imvideo/vista_toolbox/V1_model_DCT_DN_color.zip"><img alt="mat" src="/images/adicionales/matlab_ico.gif" style="border: 0px solid ; width: 21px; height: 20px;"/></a></p>
<p align="justify"></p><h3><span class="label label-warning">2009 - 2010: Metric based on linear opponent channels, Orthogonal Wavelets and Div. Norm.</span></h3><p></p>
<p align="justify">In this metric the parameters of the divisive normalization (linear weights, interaction kernel, semisaturation, excitation and summation exponents) were fitted to reproduce subjective image distortion opinion [JOSA A 10] following exhaustive grid search as in [IEEE ICIP 02]. This model (which relies on the orthogonal wavelets of the MatlabPyrTools) was found to have excellent redundancy reduction properties [LNCS10, Neur.Comp.10].</p>
<p align="justify"><strong>Download the Toolbox!:</strong><strong><a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_imvideo/vista_toolbox/V1_model_wavelet_DN_color.zip"> V1_model_wavelet_DN_color.zip (14MB) </a></strong><a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_imvideo/vista_toolbox/V1_model_wavelet_DN_color.zip"> </a> <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_imvideo/vista_toolbox/V1_model_wavelet_DN_color.zip"><img alt="mat" src="/images/adicionales/matlab_ico.gif" style="border: 0px solid ; width: 21px; height: 20px;"/></a></p>
<p align="justify"></p><h3><span class="label label-warning">2013 - 2018: Metric based on a Multi-Layer network with nonlinear opponent channels, Overcomplete Wavelets and Div. Norm.</span></h3><p></p>
<p align="justify">This metric is based on a multi-layer model (or biologically-plausible deep network) that performs the a chain of perceptually meaningful operations: nonlinear opponent chromatic channels, contrast computation, frequency selectivity and energy masking, and wavelet analysis + cross-subband masking [PLoS 18].</p>
<p align="justify">The parameters of the different layers were fitted in different ways: while the 2nd and 3rd layers (contrast and CSF+masking) were determined using MAximum Differentiation [Malo and Simoncelli SPIE.13], layers 1st and 4th (chromatic front-end and wavelet layer) were fitted to reproduce subjective image distortion data [PLoS 18, Front. Neurosci. 18a, Front. Neurosci. 18b].</p>
<p align="justify"><strong>Download the Toolbox!:</strong><strong><a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_imvideo/vista_toolbox/BioMultiLayer_L_NL_color.zip"> BioMultiLayer_L_NL_color.zip (49MB) </a></strong><a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_imvideo/vista_toolbox/BioMultiLayer_L_NL_color.zip"> </a> <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_imvideo/vista_toolbox/BioMultiLayer_L_NL_color.zip"><img alt="mat" src="/images/adicionales/matlab_ico.gif" style="border: 0px solid ; width: 21px; height: 20px;"/></a></p>
<hr id="video"/><br/><br/>
<a href="./vista_toolbox.html#Top"><small>Back to top</small></a>
<p align="justify"></p><h2><span class="label label-success">(B)   Video Quality Measure</span></h2><p></p>
<p align="justify">Our distortion metric is based on weighting the contrast difference between the original and the distorted sequences. 
               This adaptive weighting boosts perceptually visible features and attenuates negligible features. Once the background has been taken into account to consider masking, we compute the energy of the weighted difference using non-quadratic exponents.
               The parameters of these elements (widths of the filters and the masking kernels, summation exponents) were fitted to maximize the correlation with the subjective opinion. 
               Then, we played with different versions of the model by considering subsets of the elements. We found that masking is as important as the CSF in reproducing the opinion of the observers [IEEE ICIP 02].</p>
<p align="justify"><strong>Download the Toolbox!:</strong><strong><a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_imvideo/vista_toolbox/video_metric_sso.zip"> video_metric_sso.zip (34kB) </a></strong><a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_imvideo/vista_toolbox/video_metric_sso.zip"> </a> <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_imvideo/vista_toolbox/video_metric_sso.zip"><img alt="mat" src="/images/adicionales/matlab_ico.gif" style="border: 0px solid ; width: 21px; height: 20px;"/></a></p>
<p align="justify"><small>Performance of the different versions of the vision model as a function of its elements in terms of regression error. <strong>SSO</strong> stands for the CSF of the Standard Spatial Observer, <strong>m</strong> stands for masking, <strong>t</strong> stands for temporal filtering, <strong>p</strong> stands for post-summaton temporal filtering, and <strong>h</strong> stands for field doubling compensation. </small></p>
<table border="0" cellpadding="2" cellspacing="2" style="width: 115%; text-align: left; margin-left: -10%; margin-right: auto;">
<tbody>
<tr align="center"><td colspan="2" rowspan="1">
<img alt="a" class="img-responsive" src="/images/adicionales/ieee02.webp"/>
</td>
</tr>
</tbody>
</table>
<br/>
<hr id="download"/><br/><br/>
<a href="./vista_toolbox.html#Top"><small>Back to top</small></a>
<p align="justify"></p><h2><span class="label label-danger">Download VistaQualityTools!</span></h2><p></p>
<ul>
<li> <strong><span style="color:rgb(255,0,0)">Updated Matlab Toolbox (VISTALAB 4.0):</span> <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_imvideo/vista_toolbox/Vistalab.zip"> Vistalab.zip (30MB) </a></strong><a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_imvideo/vista_toolbox/Vistalab.zip"> </a> <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_imvideo/vista_toolbox/Vistalab.zip"><img alt="mat" src="/images/adicionales/matlab_ico(1).gif" style="border: 0px solid ; width: 21px; height: 20px;"/></a>
</li><li> <strong>Outdated toolbox  (VISTALAB 1.0): <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_imvideo/vista_toolbox/BasicVideoTools_code.zip"> BasicVideoTools_code.zip (15MB) </a></strong><a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_imvideo/vista_toolbox/BasicVideoTools_code.zip"> </a> <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_imvideo/vista_toolbox/BasicVideoTools_code.zip"><img alt="mat" src="/images/adicionales/matlab_ico(1).gif" style="border: 0px solid ; width: 21px; height: 20px;"/></a> <br/> The first stand alone version of VISTALAB was known as BasicVideoTools. This outdated version is included here only for compatibility with the code in the experiments of the motion-aftereffect <a href="https://www.frontiersin.org/articles/10.3389/fnhum.2015.00557/full">Front. Human Neurosci. 15 paper</a>.
             </li><li> <strong><span style="color:rgb(255,0,0)">Extensions of VISTALAB I: VistaVideoCoding</span> <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_imvideo/vista_toolbox/VistaVideoCoding.zip">VistaVideoCoding.zip (60MB) </a></strong><a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_imvideo/vista_toolbox/VistaVideoCoding.zip"> </a> <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_imvideo/vista_toolbox/VistaVideoCoding.zip"><img alt="mat" src="/images/adicionales/matlab_ico(1).gif" style="border: 0px solid ; width: 21px; height: 20px;"/></a>
</li><li> <strong><span style="color:rgb(255,0,0)">Extensions of VISTALAB II: VistaModels</span> <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_imvideo/vista_toolbox/BioMultiLayer_L_NL_color.zip">BioMultiLayer_L_NL_color.zip (40MB) </a></strong><a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_imvideo/vista_toolbox/BioMultiLayer_L_NL_color.zip"> </a> <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_imvideo/vista_toolbox/BioMultiLayer_L_NL_color.zip"><img alt="mat" src="/images/adicionales/matlab_ico(1).gif" style="border: 0px solid ; width: 21px; height: 20px;"/></a>
</li><li> <strong><span style="color:rgb(255,0,0)">Extensions of VISTALAB III: COLORLAB</span> <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_imvideo/vista_toolbox/Colorlab.zip">Colorlab.zip (15MB) </a></strong><a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_imvideo/vista_toolbox/Colorlab.zip"> </a> <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_imvideo/vista_toolbox/Colorlab.zip"><img alt="mat" src="/images/adicionales/matlab_ico(1).gif" style="border: 0px solid ; width: 21px; height: 20px;"/></a>
</li></ul>
<hr id="references"/><br/>
<a href="./vista_toolbox.html#Top"><small>Back to top</small></a>
<p align="justify"></p><h2><span class="label label-success">Citation and References</span></h2><p></p>
<p align="justify">VISTALAB is released free of charge for the scientific community: please cite us when using the software (both the web site and first journal paper that used VISTALAB)<br/><br/>
</p><p align="justify"><strong><span style="color:rgb(0,160,0)">WEB: </span></strong></p>
<strong>J. Malo &amp; J. Gutierrez. <br/> VISTALAB: the Matlab toolbox for Spatio-Temporal Vision. Univ. Valencia 1997<br/>
<a href="../../soft_visioncolor/subpages/virtual_labs.html">https://ipl-uv.github.io/code/visioncolor/vistalab.html</a></strong><p></p>
<br/>
<p align="justify"><strong><span style="color:rgb(0,160,0)">FIRST PAPER: </span></strong></p>
<strong> Malo, Gutiérrez, Epifanio, Ferri,<br/> <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_imvideo/ELECT98.PS.gz">Perceptually weighted optical flow for motion-based  segmentation in MPEG-4 paradigm.</a> Electr. Lett. 36 (20):1693-1694 (2000)</strong> <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_imvideo/ELECT98.PS.gz"><img alt="mat" src="/images/adicionales/pdf16x16.gif" style="border: 0px solid ; width: 16px; height: 16px;"/></a><br/><br/>
<p align="justify"><strong><span style="color:rgb(0,160,0)">Other papers: </span></strong></p>
       V. Laparra &amp; J. Malo.<br/> <a href="https://www.frontiersin.org/articles/10.3389/fnhum.2015.00557/full">Visual aftereffects and sensory nonlinearities from a single statistical framework</a> Frontiers in Human Neuroscience 9:557 (2015) <a href="https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_imvideo/vista_toolbox/LaparraMalo15.pdf"><img alt="mat" src="/images/adicionales/pdf16x16.gif" style="border: 0px solid ; width: 16px; height: 16px;"/></a><br/><br/>
<br/><br/><br/><br/><br/>
<div class="col-md-3">
<!-- %%%%%%%%%%%%%%%%%%%%% Nadie a la derecha %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
</div>
</div>
</div><!-- /.container -->
<!-- Bootstrap core JavaScript
    ================================================== -->
<!-- Placed at the end of the document so the pages load faster -->
<script src="./vista_toolbox_files/jquery-1.12.4.min.js"></script>
<script src="./vista_toolbox_files/bootstrap.min.js"></script>
</div></body></html>