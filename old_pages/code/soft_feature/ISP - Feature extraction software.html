<!DOCTYPE html>
<!-- saved from url=(0035)https://isp.uv.es/soft_feature.html -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="icon" href="https://isp.uv.es/images/favicon.ico">
    <title>ISP - Feature extraction software</title>
    <!-- Bootstrap core CSS -->
    <link href="./ISP - Feature extraction software_files/bootstrap.min.css" rel="stylesheet">
    <!-- Bootstrap theme -->
    <link href="./ISP - Feature extraction software_files/bootstrap-theme.min.css" rel="stylesheet">
    <!-- Custom styles for this template -->
    <!-- <link href="theme.css" rel="stylesheet"> -->
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <!-- Local styles -->
    <link href="./ISP - Feature extraction software_files/styles.css" rel="stylesheet">
  </head>

<body role="document" style="padding-top: 50px;">

<nav class="navbar navbar-inverse navbar-fixed-top">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="https://isp.uv.es/index.html">ISP</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li><a href="https://isp.uv.es/people.html">people</a></li>
        <li class="dropdown">
          <a href="https://isp.uv.es/research.html" class="dropdown-toggle disabled" data-toggle="dropdown">research<span class="caret"></span></a>
          <ul class="dropdown-menu">
            <li><a href="https://isp.uv.es/machine.html">Machine learning</a></li>
            <li><a href="https://isp.uv.es/neuro.html">Visual neuroscience</a></li>
            <li role="separator" class="divider"></li>
            <li><a href="https://isp.uv.es/visual.html">Visual brain</a></li>
            <li><a href="https://isp.uv.es/earth.html">Earth science</a></li>
            <li><a href="https://isp.uv.es/society.html">Social science</a></li>
          </ul>
        </li>
        <li><a href="https://isp.uv.es/projects.html">projects</a></li>
        <li><a href="https://isp.uv.es/facilities.html">facilities</a></li>
        <li class="dropdown">
          <a href="https://isp.uv.es/papers.html" class="dropdown-toggle disabled" data-toggle="dropdown">publications<span class="caret"></span></a>
          <ul class="dropdown-menu">
            <li><a href="https://isp.uv.es/papers.html">journals</a></li>
            <li><a href="https://isp.uv.es/conferences.html">conferences</a></li>
            <li><a href="https://isp.uv.es/books.html">books</a></li>
            <li><a href="https://isp.uv.es/talks.html">talks</a></li>
            <li role="separator" class="divider"></li>
            <li><a href="https://isp.uv.es/techreports.html">technical reports</a></li>
            <li><a href="https://isp.uv.es/theses.html">theses</a></li>
          </ul>
        </li>
 <li class="active"><a href="https://isp.uv.es/software.html">code</a></li>
 <li><a href="https://isp.uv.es/data.html">data</a></li>
        <li><a href="https://isp.uv.es/seminars.html">seminars</a></li>
        <li><a href="https://isp.uv.es/courses.html">courses</a></li>
        <li><a href="https://isp.uv.es/collaborators.html">collaborators</a></li>
        <li><a href="https://isp.uv.es/ispnews.html">news</a></li>
        <li><a href="https://isp.uv.es/contact.html">contact</a></li>

      </ul>
    </div><!--/.nav-collapse -->
  </div>
</nav>

<div class="container">

<h3>Manifold Learning: feature extraction, dimensionality reduction, density estimation and adaptation</h3>




<div class="panel panel-default">
<div class="panel-heading">
<h1 class="panel-title"></h1><h4><b>ROCK-PCA: Rotated Complex Kernel PCA for nonlinear spatio-temporal data analysis</b></h4>
</div>
<div class="panel-body">
<div class="col-md-2">
<a href="https://github.com/DiegoBueso/ROCK-PCA"> <img src="./ISP - Feature extraction software_files/rock.png" height="150" width="150"> </a>
</div>
<div class="col-md-6">
<h5><p class="text-justify"> The rotated complex kernel PCA (ROCK-PCA for short) works in reproducing kernel Hilbert spaces
to account for nonlinear processes, operates in the complex domain to account for both space and time features and time-lagged correlations, and adds an extra rotation for improved flexibility and physical consistency. The result is an explicitly resolved spatio-temporal decomposition of Earth and climate data cubes. The method is unsupervised and computationally very efficient. We illustrate its ability to uncover spatio-temporal patterns using synthetic experiments and real data. Results of the decomposition of three essential climate variables are shown: satellite-based global gross primary productivity (GPP), soil moisture (SM), and reanalysis sea surface temperature (SST) data. The ROCK-PCA method allows identifying their annual and seasonal oscillations, as well as their nonseasonal trends and spatial variability patterns.</p>
</h5>
</div>
<div class="col-md-4">
<h6><b>References</b><br>
<li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8989964">
Nonlinear PCA for Spatio-Temporal Analysis of Earth Observation Data</a>
Bueso, D. and Piles, M. and Camps-Valls, G.
IEEE Transactions on Geoscience and Remote Sensing 58 (8), 2020 
</li>
</h6>
</div>
</div>
</div>





<div class="panel panel-default">
<div class="panel-heading">
<h1 class="panel-title"></h1><h4><b>DRR: Dimensionality Reduction via Regression</b></h4>
</div>
<div class="panel-body">
<div class="col-md-2">
<a href="https://isp.uv.es/drr.html"> <img src="./ISP - Feature extraction software_files/drr_image3.JPG" height="121" width="160"> </a>
</div>
<div class="col-md-6">
<h5><p class="text-justify">Dimensionality Reduction via Regression (DRR) is a manifold learning technique intended to remove the residual statistical dependence between PCA components due to the curvature in the dataset. DRR is based on the prediction of PCA coefficients from neighbor coefficients using multivariate regression (hence generalizing PPA). DRR is a computationallly convenient step forward to SPCA in the family of manifold learning techniques generalizing PCA through the use of curves instead of stright lines (that includes NL-PCA, PPA, and SPCA).</p>
</h5>
</div>
<div class="col-md-4">
<h6><b>References</b><br>
<li>Dimensionality reduction via regression in hyperspectral imagery
Laparra, V. and Malo, J. and Camps-Valls, G.
IEEE Journal on Selected Topics in Signal Processing 9 (6):1026-1036, 2015 </li>
</h6>
</div>
</div>
</div>




<div class="panel panel-default">
<div class="panel-heading">
<h1 class="panel-title"></h1><h4><b>PPA: Principal Polynomial Analysis</b></h4>
</div>
<div class="panel-body">
<div class="col-md-2">
<a href="https://isp.uv.es/ppa.html"> <img src="./ISP - Feature extraction software_files/ppa_code.JPG" height="111" width="140"> </a>
</div>
<div class="col-md-6">
<h5><p class="text-justify">Principal Polynomial Analysis (PPA) is a manifold learning technique based on the use of a sequence of principal polynomials that capture the eventually nonlinear nature of the data, hence generalizing PCA by admitting curves instead of straight lines. PPA follows the Sequential Principal Curves Analysis philosophy (invertible projection onto a set of curvilinear festures, see below) but using a computationally convenient (univariate) analytical form for the curves and a dimensionality reduction goal function. PPA improves the PCA energy compaction ability thus reducing its dimensionality reduction error. PPA defines a manifold-dependent metric that generalizes Mahalanobis distance for curved manifolds, and allows to compute generalized curvatures at any point.</p>
</h5>
</div>
<div class="col-md-4">
<h6><b>References</b><br>
<li>Principal polynomial analysis
Laparra, V. and Jim√©nez, S. and Tuia, D. and Camps-Valls, G. and Malo, J.
International Journal of Neural Systems 24 (7) 2014 </li>
</h6>
</div>
</div>
</div>




<div class="panel panel-default">
<div class="panel-heading">
<h1 class="panel-title"></h1><h4><b>SIMFEAT: A simple MATLAB(tm) toolbox of linear and kernel feature extraction</b></h4>
</div>
<div class="panel-body">
<div class="col-md-2">
<a href="https://isp.uv.es/code/featureextraction/simfeat.zip"> <img src="./ISP - Feature extraction software_files/feat_extr2.jpg" height="121" width="140"> </a>
</div>
<div class="col-md-6">
<h5><p class="text-justify">Toolbox of linear and kernel feature extraction: (1) Linear methods: PCA, MNF, CCA, PLS, OPLS, and (2) Kernel feature extractors: KPCA, KMNF, KCCA, KPLS, KOPLS and KECA.</p>
<p>Last version of the toolbox is in GitHub: <a href="https://github.com/IPL-UV/simFeat">https://github.com/IPL-UV/simFeat</a>.
</p>
</h5>
</div>
<div class="col-md-4">
<h6><b>References</b><br>
<li>Kernel multivariate analysis framework for supervised subspace learning: A tutorial on linear and kernel multivariate methods
Arenas-Garcia, J. and Petersen, K.B. and Camps-Valls, G. and Hansen, L.K.
IEEE Signal Processing Magazine 30 (4):16-29, 2013 </li>
</h6>
</div>
</div>
</div>




<div class="panel panel-default">
<div class="panel-heading">
<h1 class="panel-title"></h1><h4><b>RBIG: Rotation-Based Iterative Gaussianization</b></h4>
</div>
<div class="panel-body">
<div class="col-md-2">
<a href="https://isp.uv.es/rbig.html"> <img src="./ISP - Feature extraction software_files/softwa1.gif" height="121" width="140"> </a>
</div>
<div class="col-md-6">
<h5><p class="text-justify">RBIG is an invertible multivariate Gaussianization transform based on the repeated application of univariate histogram Gaussianization and arbitrary multivariate rotation. By reversing the order of the linear+nonlinear stages in Deep Networks and Projection Pursuit, we show that even inexpensive random rotations lead to the Gaussian goal. RBIG can be used for multivariate PDF estimation and all the associated applications.</p>
</h5>
</div>
<div class="col-md-4">
<h6><b>References</b><br>
<li>Iterative gaussianization: From ICA to random rotations
Laparra, V. and Camps-Valls, G. and Malo, J.
IEEE Transactions on Neural Networks 22 (4):537-549, 2011 </li>
<li> PCA Gaussianization for one-class remote sensing image classification
Laparra, V. and Mu√±oz-Mar√≠, J. and Camps-Valls, G. and Malo, J.
Proceedings of SPIE - The International Society for Optical Engineering 7477 2009
</li>
<li>
PCA Gaussianization for image processing
Laparra, V. and Camps-Valls, G. and Malo, J.
Proceedings - International Conference on Image Processing, ICIP 2009
</li>
</h6>
</div>
</div>
</div>





<div class="panel panel-default">
<div class="panel-heading">
<h1 class="panel-title"></h1><h4><b>SPCA: Sequential Principal Curves Analysis</b></h4>
</div>
<div class="panel-body">
<div class="col-md-2">
<a href="https://isp.uv.es/spca.html"> <img src="./ISP - Feature extraction software_files/Cuerno_crit_1_ori_dom.PNG" height="121" width="140"> </a>
</div>
<div class="col-md-6">
<h5><p class="text-justify">SPCA is an invertible manifold learning technique&nbsp;based on the use of a sequence of nonparametric principal curves that capture the eventually nonlinear nature of the data, hence generalizing PCA by admitting curves instead of straight lines. On top of the unfolding associated to the projection onto the Principal Curves, the metric along the curves is set according to the local PDF thus leading to multivariate histogram equalization fulfilling either NonLinear ICA (information maximization) or optimal Vector Quantization (error minimization).</p>
</h5>
</div>
<div class="col-md-4">
<h6><b>References</b><br>
<li>Nonlinearities and adaptation of color vision from sequential principal curves analysis
Laparra, V. and Jim√©nez, S. and Camps-Valls, G. and Malo, J.
Neural Computation 24 (10):2751-2788, 2012
</li>
</h6>
</div>
</div>
</div>



<div class="panel panel-default">
<div class="panel-heading">
<h1 class="panel-title"></h1><h4><b>SSKPLS: Semisupervised Kernel Partial Least Squares</b></h4>
</div>
<div class="panel-body">
<div class="col-md-2">
<a href="https://isp.uv.es/code/featureextraction/sskpls_toolbox.zip"> <img src="./ISP - Feature extraction software_files/sskpls.jpg" height="121" width="140"> </a>
</div>
<div class="col-md-6">
<h5><p class="text-justify">Exploitation of probabilistic cluster kernels for nonlinear feature extraction with kernels. The kernel function is built from data, and outperforms standard kernel functions, as well as information theoretic kernels such as the Fisher kernels and mutual information kernels.</p>
</h5>
</div>
<div class="col-md-4">
<h6><b>References</b><br>
<li>Spectral clustering with the probabilistic cluster kernel
Emma Izquierdo-Verdiguier and Robert Jenssen and Luis G√≥mez-Chova and Gustavo Camps-Valls
Neurocomputing 149, Part C :1299-1304, 2015
</li>
<li>Semisupervised kernel feature extraction for remote sensing image analysis
Izquierdo-Verdiguier, E. and Gomez-Chova, L. and Bruzzone, L. and Camps-Valls, G.
IEEE Transactions on Geoscience and Remote Sensing 52 (9):5567-5578, 2014
</li>
</h6>
</div>
</div>
</div>


<div class="panel panel-default">
<div class="panel-heading">
<h1 class="panel-title"></h1><h4><b>HOCCA: Higher Order Canonical Correlation Analysis</b></h4>
</div>
<div class="panel-body">
<div class="col-md-2">
<a href="https://isp.uv.es/HOCCA.html"> <img src="./ISP - Feature extraction software_files/hocca.JPG" height="121" width="140"> </a>
</div>
<div class="col-md-6">
<h5><p class="text-justify">Higher Order Canonical Analysis (HOCCA) is a linear invertible manifold learning technique to be applied on several data sets coming from the same underlying source (the domain adaptation problem): It finds in each data set independent components which are related across the data sets, thus conciliating the goals of ICA and CCA at the same time. The canonical domain found by HOCCA maximizes the independence in the internal domain (similarly to separate ICAs) and maximizes the correspondence between the features in each aquisition condition (similarly to CCA).</p>
</h5>
</div>
<div class="col-md-4">
<h6><b>References</b><br>
<li>Spatio-Chromatic Adaptation via Higher-Order Canonical Correlation Analysis of Natural Images
Gutmann, Michael U. and Laparra, Valero and Hyv√§rinen, Aapo and Malo, Jes√∫s
PLoS ONE Public Library of Science 9 (2):e86481, 2014
</li>
</h6>
</div>
</div>
</div>


<div class="panel panel-default">
<div class="panel-heading">
<h1 class="panel-title"></h1><h4><b>SSMA: SemiSupervised Manifold Alignment</b></h4>
</div>
<div class="panel-body">
<div class="col-md-2">
<a href="https://isp.uv.es/code/featureextraction/ssma.zip"> <img src="./ISP - Feature extraction software_files/ssma.png" height="121" width="140"> </a>
</div>
<div class="col-md-6">
<h5><p class="text-justify">The SSMA Toolbox is a Matlab Toolbox for the semisupervised manifold alignment of generic data without the need of having corresponding pairs, just a set of (few) labeled samples in each domain. </p>
</h5>
</div>
<div class="col-md-4">
<h6><b>References</b><br>
<li>Semisupervised manifold alignment of multimodal remote sensing images
Tuia, D. and Volpi, M. and Trolliet, M. and Camps-Valls, G.
IEEE Transactions on Geoscience and Remote Sensing 52 (12):7708-7720, 2014
</li>
</h6>
</div>
</div>
</div>


<div class="panel panel-default">
<div class="panel-heading">
<h1 class="panel-title"></h1><h4><b>KEMA: Kernel Manifold Alignment</b></h4>
</div>
<div class="panel-body">
<div class="col-md-2">
<a href="https://github.com/dtuia/KEMA"> <img src="./ISP - Feature extraction software_files/simpsonize.jpg" height="100" width="140"> </a>
</div>
<div class="col-md-6">
<h5><p class="text-justify">Kernelization of SSMA, which allows for much better semantic alignments of multisource data. </p>
</h5>
</div>
<div class="col-md-4">
<h6><b>References</b><br>
<li>Kernel Manifold Alignment for Domain Adaptation
Tuia, D. and G. Camps-Valls
PLoS ONE, 2016
</li>
</h6>
</div>
</div>
</div>


<div class="panel panel-default">
<div class="panel-heading">
<h1 class="panel-title"></h1><h4><b>OKECA: Optimized Kernel Entropy Component Analysis</b></h4>
</div>
<div class="panel-body">
<div class="col-md-2">
<a href="https://isp.uv.es/code/featureextraction/okeca_v1.zip"> <img src="./ISP - Feature extraction software_files/okeca.png" height="100" width="140"> </a>
</div>
<div class="col-md-6">
<h5><p class="text-justify">Optimized kernel feature extraction based on entropy estimation in Hilbert feature spaces. Outstanding results are obtained with much less components: OKECA is intrinsically sparse and compact, very useful for data visualization and dimensionality reduction. </p>
</h5>
</div>
<div class="col-md-4">
<h6><b>References</b><br>
<li>Optimized Kernel Entropy Components.
E. Izquierdo-Verdiguier, V. Lapara, R. Jenssen, L. G√≥mez-Chova, G. Camps-Valls
IEEE Transactions on Neural Networks and Learning Systems, 2016
</li>
</h6>
</div>
</div>
</div>



<div class="panel panel-default">
<div class="panel-heading">
<h1 class="panel-title"></h1><h4><b>KSNR: Kernel Signal to Noise Ratio</b></h4>
</div>
<div class="panel-body">
<div class="col-md-2">
<a href="https://isp.uv.es/code/featureextraction/ksnr.zip"> <img src="./ISP - Feature extraction software_files/signal-noise.jpg" height="121" width="140"> </a>
</div>
<div class="col-md-6">
<h5>
<p class="text-justify">The kernel signal to noise ratio (KSNR) considers a feature extraction model that
maximizes the signal variance while minimizes the estimated noise variance in a reproducing kernel
Hilbert space (RKHS). The KSNR can be used for dimensionality reduction as an excellent alternative to kPCA when dealing
with correlated (possibly non-Gaussian) noise. KSNR yields more fitted solutions and extracts more
noise-free features when confronted with standard approaches.</p>
</h5>
</div>
<div class="col-md-4">
<h6><b>References</b><br>
<li>Learning with the kernel signal to noise ratio
Gomez-Chova, L. and Camps-Valls, G.
IEEE International Workshop on Machine Learning for Signal Processing, MLSP 2012 </li>
</h6>
</div>
</div>
</div>


<div class="panel panel-default">
<div class="panel-heading">
<h1 class="panel-title"></h1><h4><b>EPLS: Unsupervised sparse convolutional neural networks for feature extraction</b></h4>
</div>
<div class="panel-body">
<div class="col-md-2">
<a href="https://sites.google.com/site/adriromsor/epls"> <img src="./ISP - Feature extraction software_files/epls.jpg" height="121" width="140"> </a>
</div>
<div class="col-md-6">
<h5>
<p class="text-justify">EPLS stands for Enhancing Population and Lifetime Sparsity, a very good alternative to achieve sparse representations
when training machines such as convolutional neural nets. EPLS is a meta-parameter free, off-the-shelf, simple and fast unsupervised feature learning algorithm, which exploits a new way of optimizing for sparsity. The algorithm sets an output target with one "hot code" while ensuring a strong form of lifetime sparsity to avoid dead outputs and optimizes for that specific target to learn the dictionary bases. </p>
</h5>
</div>
<div class="col-md-4">
<h6><b>References</b><br>
<li>Unrolling loopy top-down semantic feedback in convolutional deep networks
Carlo Gatta, Adriana Romero, Joost van de Weijer.
Deep-vision workshop CVPR, 2014.</li>
<li>Unsupervised Deep Feature Extraction Of Hyperspectral Images
Adriana Romero, Carlo Gatta, Gustavo Camps-Valls
IEEE Workshop on Hyperspectral Image and Signal Processing, Whispers, 2014</li>
<li>Unsupervised Deep Feature Extraction for Remote Sensing Image Classification
Romero, A. and Gatta, C. and Camps-Valls, G.
Geoscience and Remote Sensing, IEEE Transactions on 2015</li>
<li>Shared feature representations of LiDAR and optical images: Trading sparsity for semantic discrimination
Campos-Taberner, M. and Romero, A. and Gatta, C. and Camps-Valls, G.
Geoscience and Remote Sensing Symposium (IGARSS), 2015 IEEE International, 2015</li>
</h6>
</div>
</div>
</div>

</div><!-- /.container -->
    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="./ISP - Feature extraction software_files/jquery-1.12.4.min.js"></script>
    <script src="./ISP - Feature extraction software_files/bootstrap.min.js"></script>



</body></html>