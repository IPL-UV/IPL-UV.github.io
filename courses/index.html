<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>ISP - Image and Signal Processing group</title>
<link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css integrity="sha512-z3gLpd7yknf1YoNbCzqRKc4qyor8gaKU1qmn+CShxbuBusANI9QpRohGBreCFkKxLhei6S9CQXFEbbKuqLg0DA==" crossorigin=anonymous referrerpolicy=no-referrer><link rel="shortcut icon" href=/images/favicon.ico type=image/x-icon><link rel=stylesheet href=/style/style.css><script src=/js/mode.js></script><script src=https://cdn.jsdelivr.net/npm/marked/marked.min.js></script></head><nav class=custom-navbars><div class=custom-container><a href=/ class="custom-logo custom-hide-on-large"><img src=/images/isp_logo_sinfondo.webp alt="ISP Icon" class=custom-logo_nav>
<span class=custom-text-isp>ISP</span>
</a><button class=navbar-toggler aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class=custom-navbar-collapse><ul class=custom-navbar-nav><li class="custom-nav-item custom-desktop-only"><a class=custom-nav-link href=/>ISP</a></li><li class=custom-nav-item><a class=custom-nav-link href=/people/>People</a></li><li class="custom-nav-item custom-dropdown"><a class="custom-nav-link custom-dropdown-toggle" href=/research/philosophy/>Research</a><ul class=custom-dropdown-menu><li><a class=custom-dropdown-item href=/research/philosophy/>Philosophy</a></li><li><a class=custom-dropdown-item href=/research/machine_learning/>Machine learning</a></li><li><a class=custom-dropdown-item href=/research/visual_neuroscience/>Visual neuroscience</a></li><li><a class=custom-dropdown-item href=/research/visual_brain/>Visual brain</a></li><li><a class=custom-dropdown-item href=/research/earth_science/>Earth science</a></li><li><a class=custom-dropdown-item href=/research/social_science>Social science</a></li></ul></li><li class=custom-nav-item><a class=custom-nav-link href=/projects/>Projects</a></li><li class=custom-nav-item><a class=custom-nav-link href=/facilities/>Facilities</a></li><li class="custom-nav-item custom-dropdown"><a class="custom-nav-link custom-dropdown-toggle" href=/publications/journals/>Publications</a><ul class=custom-dropdown-menu><li><a class=custom-dropdown-item href=/publications/journals/>Journals</a></li><li><a class=custom-dropdown-item href=/publications/conferences/>Conferences</a></li><li><a class=custom-dropdown-item href=/publications/books/>Books</a></li><li><a class=custom-dropdown-item href=/publications/talks/>Talks</a></li><li><a class=custom-dropdown-item href=/publications/technical_reports/>Technical Reports</a></li><li><a class=custom-dropdown-item href=/publications/theses/>Theses</a></li></ul></li><li class=custom-nav-item><a class=custom-nav-link href=/code/>Code</a></li><li class=custom-nav-item><a class=custom-nav-link href=/data/>Data</a></li><li class=custom-nav-item><a class=custom-nav-link href=/seminars/>Seminars</a></li><li class=custom-nav-item><a class=custom-nav-link href=/courses/>Courses</a></li><li class=custom-nav-item><a class=custom-nav-link href=/collaborators/>Collaborators</a></li><li class=custom-nav-item><a class=custom-nav-link href=/news/>News</a></li><li class=custom-nav-item><a class=custom-nav-link href=/contact/>Contact</a></li></ul></div></div></nav><style>.custom-navbars{background-color:#222;position:fixed;top:0;width:100%;display:flex;align-items:center;justify-content:space-between;padding:0 1rem;z-index:1000;height:3rem}.custom-container{display:flex;width:100%;height:100%;justify-content:center;align-items:center}.custom-logo{display:flex;align-items:center}.custom-logo_nav{height:30px}.custom-hide-on-large{display:none}.navbar-toggler{display:none}.navbar-toggler-icon{background-image:url("data:image/svg+xml,%3csvg viewBox='0 0 30 30' xmlns='http://www.w3.org/2000/svg'%3e%3cpath stroke='white' stroke-width='2' stroke-linecap='round' stroke-miterlimit='10' d='M4 7h22M4 15h22M4 23h22'/%3e%3c/svg%3e")}.custom-navbar-collapse{flex-grow:.95;height:100%;align-content:center}.custom-navbar-nav{list-style:none;display:flex;flex-direction:row;padding:0;margin:0;height:100%}.custom-nav-item{flex:auto;text-align:center;border:2px solid transparent;border-radius:8px;height:100%;display:flex;align-items:center;justify-content:center}.custom-nav-item:hover{background-color:#a0a0a0;border-color:#fff}.custom-nav-link{text-decoration:none;color:#f8f8f8;font-size:clamp(1rem,1.1vw,1.1rem);position:relative;display:block}.custom-dropdown-menu{position:absolute;background-color:rgba(46,46,46,.9);text-decoration:none;display:none;list-style:none;padding:.5rem 0;margin:0;border-radius:5px;font-size:clamp(1rem,1.1vw,1.1rem);top:3rem}.custom-dropdown-toggle::after{content:"▼";font-size:.5rem;margin-left:.3rem;color:#f8f8f8;display:inline-block;vertical-align:middle}.custom-dropdown-item{padding:0 1rem;color:#f8f8f8;text-decoration:none;margin:0;display:block;border:2px solid transparent;border-radius:8px}.custom-dropdown-item:hover{background-color:#a0a0a0;border-color:#fff}.custom-dropdown:hover .custom-dropdown-menu{display:block}.custom-nav-item.active{background-color:#464646}.custom-dropdown-item.active{background-color:#535353}@media(max-width:1070px){.custom-container{display:flex;width:100%;justify-content:space-between}.navbar-toggler{display:block;position:absolute;right:1rem;top:.5rem}.custom-hide-on-large{display:flex;align-items:center;text-decoration:none}.custom-navbar-collapse{display:none;flex-direction:column;width:100%;background-color:rgba(46,46,46,.9);position:absolute;top:3rem;left:0;z-index:999;height:auto;overflow:hidden;max-height:0;transition:max-height .3s ease-out;padding:.5rem 0;border-bottom-left-radius:.5rem;border-bottom-right-radius:.5rem}.custom-navbar-collapse.show{display:flex;max-height:100vh;overflow-y:auto}.custom-navbar-nav{flex-direction:column;align-items:center;width:100%}.custom-nav-item{width:100%;margin:0;display:block}.custom-nav-item a{text-align:left;margin-left:10%}.custom-text-isp{margin-left:.5rem;font-size:1rem;text-decoration:none;color:#f8f8f8}.custom-nav-link{width:100%;text-align:center}.custom-dropdown-menu{position:static;display:none;width:100%;background-color:#333}.custom-dropdown:hover .custom-dropdown-menu,.custom-dropdown .custom-dropdown-menu.show{display:block}}</style><script>document.addEventListener("DOMContentLoaded",function(){const n=document.querySelector(".navbar-toggler"),t=document.querySelector(".custom-navbar-collapse"),e=window.location.pathname;n.addEventListener("click",function(){t.classList.toggle("show")}),document.addEventListener("click",function(e){e.target.closest(".custom-navbars")||t.classList.remove("show")});const s=document.querySelectorAll(".custom-nav-link");s.forEach(t=>{const n=new URL(t.href).pathname;n===e&&e!=="/"&&t.closest(".custom-nav-item").classList.add("active")});const o=document.querySelectorAll(".custom-dropdown-item");o.forEach(t=>{const n=new URL(t.href).pathname;if(n===e){t.classList.add("active");const e=t.closest(".custom-dropdown");e&&e.classList.add("active")}});const i=document.querySelectorAll(".custom-dropdown");i.forEach(t=>{const n=t.querySelectorAll(".custom-dropdown-item");n.forEach(n=>{const s=new URL(n.href).pathname;s===e&&t.classList.add("active")})})})</script><main><div class=container><div class=content-container><div class=panel-item><div class=panel-header><h4 class=panel-title><b>Color Vision and Colorimetry</b></h4></div><div class=panel-body><div class="panel-content row"><div class="panel-image col-md-2"><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/courses/Color_Vision.zip><img src=/images/courses/color.webp alt=color></a></div><div class="panel-description col-md-10"><h5><p class=text-justify><p><strong>Course Duration:</strong> 30 hours<br><strong>Instructor:</strong> J. Malo</p><p>Color is a 5-dimensional perception that is not only related to the spectrum coming from an object, but also strongly related to its spatio-temporal context. It is a powerful feature that allows humans to make reliable inferences about objects that would be nice to understand and mimic in artificial vision. In this course, we derive the linear CIE tristimulus theory from its experimental color matching foundations. We derive the relations between spectrum and tristimulus vectors through the color matching functions, the chromatic coordinates, chromatic purity and luminance. Phenomenology of color discrimination and adaptation reveals the limitations of the linear description and sets the foundations of color appearance models. In addition, we link the above perceptual representations of color with the conventional representation of color in computers.</p></p></h5></div></div></div></div><div class=panel-item><div class=panel-header><h4 class=panel-title><b>Google Earth Engine Introduction</b></h4></div><div class=panel-body><div class="panel-content row"><div class="panel-image col-md-2"><a href=google-earth-engine-introduction/material><img src=/images/courses/gee_screenshot.webp alt=gee_screenshot></a></div><div class="panel-description col-md-10"><h5><p class=text-justify><p><strong>Course Duration:</strong> N/A<br><strong>Instructors:</strong> Emma Izquierdo & Jordi Muñoz-Marí</p><p>A short introduction to Google Earth Engine.</p></p></h5></div></div></div></div><div class=panel-item><div class=panel-header><h4 class=panel-title><b>Human Vision: Facts, Mechanistic Models, and Principled Theories</b></h4></div><div class=panel-body><div class="panel-content row"><div class="panel-image col-md-2"><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/courses/visual_percept.pptx><img src=/images/courses/human.webp alt=human></a></div><div class="panel-description col-md-10"><h5><p class=text-justify><p><strong>Course Duration:</strong> 30 hours<br><strong>Instructor:</strong> J. Malo</p><p>In this course, I introduce the facts on color vision from radiometry and photometry, the vision of spatio-temporal textures, the models of mechanisms and circuits that (kind of) reproduce these facts, and the statistical elements of theories that explain the facts.</p></p></h5></div></div></div></div><div class=panel-item><div class=panel-header><h4 class=panel-title><b>Hyperspectral Image Processing</b></h4></div><div class=panel-body><div class="panel-content row"><div class="panel-image col-md-2"><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/courses/esa_course.zip><img src=/images/courses/hyper.webp alt=hyper></a></div><div class="panel-description col-md-10"><h5><p class=text-justify><p><strong>Course Duration:</strong> 60 hours<br><strong>Instructor:</strong> G. Camps-Valls</p><p>We introduce the main concepts of hyperspectral image processing. We start by a soft introduction to hyperspectral image processing, the standard processing chain, and the current challenges in the field. Then we analyze the current state of the art in several topics: feature extraction, supervised classification, unmixing and abundance estimation, and retrieval of biophysical parameters. All the methods and techniques studied are reviewed both theoretically and through MATLAB exercises.</p></p></h5></div></div></div></div><div class=panel-item><div class=panel-header><h4 class=panel-title><b>Information Theory for Visual Communication</b></h4></div><div class=panel-body><div class="panel-content row"><div class="panel-image col-md-2"><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/courses/Info_theory_for_Neurosci_RBIG_infomax_DN_SPCA_PPA_DDR.zip><img src=/images/courses/Info_Theory2.webp alt=Info_Theory2></a></div><div class="panel-description col-md-10"><h5><p class=text-justify><p><strong>Course Duration:</strong> 30 hours<br><strong>Instructor:</strong> J. Malo</p><p>In this course, I introduce the elements of information theory required to understand why Uniformization or Gaussianization of density functions and noise in the system are key for the transmission of visual information. This knowledge is the basis of our long-standing agenda on developing invertible transforms for uniformization (SPCA, PPA, DRR) and Gaussianization (RBIG), and our research to calibrate neural noise in the visual system and Divisive Normalization models.</p></p></h5></div></div></div></div><div class=panel-item><div class=panel-header><h4 class=panel-title><b>Kernel Methods in Machine Learning</b></h4></div><div class=panel-body><div class="panel-content row"><div class="panel-image col-md-2"><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/courses/kernel_course.zip><img src=/images/courses/sskpls.webp alt=sskpls></a></div><div class="panel-description col-md-10"><h5><p class=text-justify><p><strong>Course Duration:</strong> 30 hours<br><strong>Instructor:</strong> G. Camps-Valls</p><p>Two fundamental operations in Machine Learning such as regression and classification involve drawing nonlinear boundaries or functions through a set of (labeled or unlabeled) training samples. These boundaries or functions at certain (test) samples can be deduced from the similarities between the test sample and the training samples. These similarities can be encoded in Kernels, and the representer theorem can be used to obtain expressions for the functions at any test sample. In this course, we will also review the application of the kernelization of scalar products (e.g., as in the covariance matrix) to obtain nonlinear generalizations of classical feature extraction methods.</p></p></h5></div></div></div></div><div class=panel-item><div class=panel-header><h4 class=panel-title><b>Machine Learning and Signal Processing for Remote Sensing Data Analysis (IGARSS'14 tutorial)</b></h4></div><div class=panel-body><div class="panel-content row"><div class="panel-image col-md-2"><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/courses/tutorial_igarss15.tar.gz><img src=/images/courses/remote_sensing.webp alt=remote_sensing></a></div><div class="panel-description col-md-10"><h5><p class=text-justify><p><strong>Course Duration:</strong> N/A<br><strong>Instructors:</strong> G. Camps-Valls and D. Tuia</p><p>In this tutorial, we will present the remote sensing image processing chain, and take the attendants on a tour of different strategies for feature extraction, classification, unmixing, retrieval, and pattern analysis for data understanding. On the one hand, we will present powerful methodologies for remote sensing data classification: extracting knowledge from data, including interactive approaches via active learning, classifiers that encode prior knowledge and invariances, semi-supervised learning that exploits the information of unlabeled data, and domain adaptation to compensate for shifts in the ever-changing data distributions. On the other hand, we will pay attention to recent advances in bio-geophysical parameter estimation that incorporate heteroscedasticity, online adaptation, and problem understanding. From there, we will take a leap towards the more challenging step of understanding the geoscience problems from data by reviewing the latest advances in (directed) graphical models, structure learning, and empirical causal inference. Beyond theory, we will also present results of recent studies illustrating all the covered issues. Finally, we will provide code to the attendees to try the different methodologies and provide a solid ground for their future experimentations.</p></p></h5></div></div></div></div><div class=panel-item><div class=panel-header><h4 class=panel-title><b>Remote Sensing for Water Quality</b></h4></div><div class=panel-body><div class="panel-content row"><div class="panel-image col-md-2"><a href="https://www.youtube.com/watch?v=d67aO2z06dI"><img src=/images/courses/EO_AnnaB_2023.webp alt=EO_AnnaB_2023></a></div><div class="panel-description col-md-10"><h5><p class=text-justify><p><strong>Course Duration:</strong> N/A<br><strong>Instructor:</strong> Anna B. Ruescas</p><p>Anna B. Ruescas leads this session on remote sensing for water quality, as part of the 2023 ESA Earth Observation Advanced Training Course at the Wroclaw University of Environmental and Life Sciences.</p></p></h5></div></div></div></div><div class=panel-item><div class=panel-header><h4 class=panel-title><b>Representation of Spatial Information</b></h4></div><div class=panel-body><div class="panel-content row"><div class="panel-image col-md-2"><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/courses/Represent_Spatial_Information.zip><img src=/images/courses/base.webp alt=base></a></div><div class="panel-description col-md-10"><h5><p class=text-justify><p><strong>Course Duration:</strong> 30 hours<br><strong>Instructor:</strong> J. Malo</p><p>Statistical regularities in photographic images imply that certain representations of spatial information are better than others in terms of coding efficiency. In this course, we present the information theory concepts (entropy, multi-information, correlation and negentropy) for unsupervised feature extraction or dictionary learning required in image coding. Redundancy in images and sequences is reviewed, and basic techniques for compact information representation are introduced such as vector quantization, predictive coding, and transform coding. Application of these concepts in images is the basis of DCT and Wavelet representations, which are the core of JPEG and JPEG2000.</p></p></h5></div></div></div></div><div class=panel-item><div class=panel-header><h4 class=panel-title><b>Satellite-based Tools for Investigating Aquatic Ecosystems</b></h4></div><div class=panel-body><div class="panel-content row"><div class="panel-image col-md-2"><a href="https://www.youtube.com/watch?v=pknYqAAtxRE"><img src=/images/courses/annaB_SNAP23.webp alt=annaB_SNAP23></a></div><div class="panel-description col-md-10"><h5><p class=text-justify><p><strong>Course Duration:</strong> N/A<br><strong>Instructor:</strong> Anna B. Ruescas</p><p>Anna B. Ruescas participates in this session on the use of Sentinel Constellation SNAP Tools, as part of the 2023 Satellite-based Tools for Investigating Aquatic Ecosystems Training at the Trevor Platt Science Foundation.</p></p></h5></div></div></div></div><div class=panel-item><div class=panel-header><h4 class=panel-title><b>Statistical Signal Processing</b></h4></div><div class=panel-body><div class="panel-content row"><div class="panel-image col-md-2"><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/courses/ps_2014.pdf><img src=/images/courses/signal_theme.webp alt=signal_theme></a></div><div class="panel-description col-md-10"><h5><p class=text-justify><p><strong>Course Duration:</strong> 60 hours<br><strong>Instructor:</strong> G. Camps-Valls</p><p>Material for a master course on (statistical) signal processing. I cover the essential background for engineers and physicists interested in signal processing: Probability and random variables, discrete time random processes, spectral estimation, signal decomposition and transforms, and an introduction to information theory.</p></p></h5></div></div></div></div><div class=panel-item><div class=panel-header><h4 class=panel-title><b>Texture and Motion in the Visual Cortex</b></h4></div><div class=panel-body><div class="panel-content row"><div class="panel-image col-md-2"><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/courses/Color_Vision.zip><img src=/images/courses/mt_theoret_sensit.webp alt=mt_theoret_sensit></a></div><div class="panel-description col-md-10"><h5><p class=text-justify><p><strong>Course Duration:</strong> 40 hours<br><strong>Instructor:</strong> J. Malo</p><p>Neurons in V1 and MT cortex play a determinant role in the analysis of the shape of objects, their spatial texture, and the estimation of retinal motion. In this course, we describe the basic psychophysical and physiological phenomena related to low-level spatio-temporal vision: the contrast sensitivity functions, masking, adaptation, and aftereffects. These facts are mediated by the context-dependent nonlinearities of the response of neurons with specific receptive fields. We analyze the geometric properties of the standard model of V1 and their consequences in image discrimination. We introduce the concept of optical flow, its properties, and how this description of motion can be estimated from the 3D wavelet sensors in V1 and the aggregated sensors in MT.</p></p></h5></div></div></div></div><div class=panel-item><div class=panel-header><h4 class=panel-title><b>The GLaSS Training Material Builds on the Global Lakes Use Cases</b></h4></div><div class=panel-body><div class="panel-content row"><div class="panel-image col-md-2"><a href=https://www.learn-eo.org/lessons_glass.php><img src=/images/courses/CarwH7tWIAAn2ry.jpg-large.webp alt=CarwH7tWIAAn2ry.jpg-large></a></div><div class="panel-description col-md-10"><h5><p class=text-justify><p><strong>Course Duration:</strong> N/A<br><strong>Instructor:</strong> Ana B. Ruescas & GLaSS team</p><p>The GLaSS training material builds on the global lakes use cases of GLaSS. It allows students and professionals in fields such as aquatic ecology, environmental technology, remote sensing, and GIS to learn about the possibilities of optical remote sensing of water quality, by using the Sentinel-2 and Sentinel-3 satellites and Landsat 8.</p></p></h5></div></div></div></div><p></p></div></div></main></body><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js integrity=sha384-C6RzsynM9kWDrMNeT87bh95OGNyZPhcTNXj1NW7RuBCsyN/o0jlpcV8Qyq46cDfL crossorigin=anonymous defer></script><script type=text/javascript src=https://cdn.jsdelivr.net/gh/pcooksey/bibtex-js@1.0.0/src/bibtex_js.min.js defer></script></html>