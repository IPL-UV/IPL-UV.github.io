<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>ISP - Image and Signal Processing group</title>
<link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css integrity="sha512-z3gLpd7yknf1YoNbCzqRKc4qyor8gaKU1qmn+CShxbuBusANI9QpRohGBreCFkKxLhei6S9CQXFEbbKuqLg0DA==" crossorigin=anonymous referrerpolicy=no-referrer><link rel="shortcut icon" href=/images/favicon.ico type=image/x-icon><link rel=stylesheet href=/style/style.css><script src=/js/mode.js></script><script src=https://cdn.jsdelivr.net/npm/marked/marked.min.js></script></head><nav class=custom-navbars><div class=custom-container><a href=/ class="custom-logo custom-hide-on-large"><img src=/images/isp_logo_sinfondo.webp alt="ISP Icon" class=custom-logo_nav>
<span class=custom-text-isp>ISP</span>
</a><button class=navbar-toggler aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class=custom-navbar-collapse><ul class=custom-navbar-nav><li class="custom-nav-item custom-desktop-only"><a class=custom-nav-link href=/>ISP</a></li><li class=custom-nav-item><a class=custom-nav-link href=/people/>People</a></li><li class="custom-nav-item custom-dropdown"><a class="custom-nav-link custom-dropdown-toggle" href=/research/philosophy/>Research</a><ul class=custom-dropdown-menu><li><a class=custom-dropdown-item href=/research/philosophy/>Philosophy</a></li><li><a class=custom-dropdown-item href=/research/machine_learning/>Machine learning</a></li><li><a class=custom-dropdown-item href=/research/visual_neuroscience/>Visual neuroscience</a></li><li><a class=custom-dropdown-item href=/research/visual_brain/>Visual brain</a></li><li><a class=custom-dropdown-item href=/research/earth_science/>Earth science</a></li><li><a class=custom-dropdown-item href=/research/social_science>Social science</a></li></ul></li><li class=custom-nav-item><a class=custom-nav-link href=/projects/>Projects</a></li><li class=custom-nav-item><a class=custom-nav-link href=/facilities/>Facilities</a></li><li class="custom-nav-item custom-dropdown"><a class="custom-nav-link custom-dropdown-toggle" href=/publications/journals/>Publications</a><ul class=custom-dropdown-menu><li><a class=custom-dropdown-item href=/publications/journals/>Journals</a></li><li><a class=custom-dropdown-item href=/publications/conferences/>Conferences</a></li><li><a class=custom-dropdown-item href=/publications/books/>Books</a></li><li><a class=custom-dropdown-item href=/publications/talks/>Talks</a></li><li><a class=custom-dropdown-item href=/publications/technical_reports/>Technical Reports</a></li><li><a class=custom-dropdown-item href=/publications/theses/>Theses</a></li></ul></li><li class=custom-nav-item><a class=custom-nav-link href=/code/>Code</a></li><li class=custom-nav-item><a class=custom-nav-link href=/data/>Data</a></li><li class=custom-nav-item><a class=custom-nav-link href=/seminars/>Seminars</a></li><li class=custom-nav-item><a class=custom-nav-link href=/courses/>Courses</a></li><li class=custom-nav-item><a class=custom-nav-link href=/collaborators/>Collaborators</a></li><li class=custom-nav-item><a class=custom-nav-link href=/news/>News</a></li><li class=custom-nav-item><a class=custom-nav-link href=/contact/>Contact</a></li></ul></div></div></nav><style>.custom-navbars{background-color:#222;position:fixed;top:0;width:100%;display:flex;align-items:center;justify-content:space-between;padding:0 1rem;z-index:1000;height:3rem}.custom-container{display:flex;width:100%;height:100%;justify-content:center;align-items:center}.custom-logo{display:flex;align-items:center}.custom-logo_nav{height:30px}.custom-hide-on-large{display:none}.navbar-toggler{display:none}.navbar-toggler-icon{background-image:url("data:image/svg+xml,%3csvg viewBox='0 0 30 30' xmlns='http://www.w3.org/2000/svg'%3e%3cpath stroke='white' stroke-width='2' stroke-linecap='round' stroke-miterlimit='10' d='M4 7h22M4 15h22M4 23h22'/%3e%3c/svg%3e")}.custom-navbar-collapse{flex-grow:.95;height:100%;align-content:center}.custom-navbar-nav{list-style:none;display:flex;flex-direction:row;padding:0;margin:0;height:100%}.custom-nav-item{flex:auto;text-align:center;border:2px solid transparent;border-radius:8px;height:100%;display:flex;align-items:center;justify-content:center}.custom-nav-item:hover{background-color:#a0a0a0;border-color:#fff}.custom-nav-link{text-decoration:none;color:#f8f8f8;font-size:clamp(1rem,1.1vw,1.1rem);position:relative;display:block}.custom-dropdown-menu{position:absolute;background-color:rgba(46,46,46,.9);text-decoration:none;display:none;list-style:none;padding:.5rem 0;margin:0;border-radius:5px;font-size:clamp(1rem,1.1vw,1.1rem);top:3rem}.custom-dropdown-toggle::after{content:"â–¼";font-size:.5rem;margin-left:.3rem;color:#f8f8f8;display:inline-block;vertical-align:middle}.custom-dropdown-item{padding:0 1rem;color:#f8f8f8;text-decoration:none;margin:0;display:block;border:2px solid transparent;border-radius:8px}.custom-dropdown-item:hover{background-color:#a0a0a0;border-color:#fff}.custom-dropdown:hover .custom-dropdown-menu{display:block}.custom-nav-item.active{background-color:#464646}.custom-dropdown-item.active{background-color:#535353}@media(max-width:1070px){.custom-container{display:flex;width:100%;justify-content:space-between}.navbar-toggler{display:block;position:absolute;right:1rem;top:.5rem}.custom-hide-on-large{display:flex;align-items:center;text-decoration:none}.custom-navbar-collapse{display:none;flex-direction:column;width:100%;background-color:rgba(46,46,46,.9);position:absolute;top:3rem;left:0;z-index:999;height:auto;overflow:hidden;max-height:0;transition:max-height .3s ease-out;padding:.5rem 0;border-bottom-left-radius:.5rem;border-bottom-right-radius:.5rem}.custom-navbar-collapse.show{display:flex;max-height:100vh;overflow-y:auto}.custom-navbar-nav{flex-direction:column;align-items:center;width:100%}.custom-nav-item{width:100%;margin:0;display:block}.custom-nav-item a{text-align:left;margin-left:10%}.custom-text-isp{margin-left:.5rem;font-size:1rem;text-decoration:none;color:#f8f8f8}.custom-nav-link{width:100%;text-align:center}.custom-dropdown-menu{position:static;display:none;width:100%;background-color:#333}.custom-dropdown:hover .custom-dropdown-menu,.custom-dropdown .custom-dropdown-menu.show{display:block}}</style><script>document.addEventListener("DOMContentLoaded",function(){const n=document.querySelector(".navbar-toggler"),t=document.querySelector(".custom-navbar-collapse"),e=window.location.pathname;n.addEventListener("click",function(){t.classList.toggle("show")}),document.addEventListener("click",function(e){e.target.closest(".custom-navbars")||t.classList.remove("show")});const s=document.querySelectorAll(".custom-nav-link");s.forEach(t=>{const n=new URL(t.href).pathname;n===e&&e!=="/"&&t.closest(".custom-nav-item").classList.add("active")});const o=document.querySelectorAll(".custom-dropdown-item");o.forEach(t=>{const n=new URL(t.href).pathname;if(n===e){t.classList.add("active");const e=t.closest(".custom-dropdown");e&&e.classList.add("active")}});const i=document.querySelectorAll(".custom-dropdown");i.forEach(t=>{const n=t.querySelectorAll(".custom-dropdown-item");n.forEach(n=>{const s=new URL(n.href).pathname;s===e&&t.classList.add("active")})})})</script><main><div class=container><div class="content-container grid-layout"><div class=box-single><h1>Methods and Models</h1></div><div class=box-content><p><h1 id=super-resolution-methodology>Super-Resolution Methodology</h1><p>Diffusion models have recently overtaken GAN models in the state-of-the-art of generative image methodologies. While GANs have been extensively used in image super-resolution, they are unstable due to their very delicate training process. Diffusion models, which are also probabilistic models drawing results from a likelihood distribution, have proven to be easier to train and deliver better results, surpassing other methodologies in many reconstruction metrics. This development has also been noticed in the remote sensing community, with recent super-resolution works switching to diffusion-based SR methodologies.</p><br><p align=center><img src=/images/projects/lat_px_dif.webp width=100%><p align=center><em>Figure: Pixel-space vs latent-space diffusion models</em></p></p><br><h1 id=explainable-ai>Explainable AI</h1><p>The use of deep learning in SR does not usually provide any insight besides the final outcome, which makes the EO community reluctant to adopt machine learning due to the lack of transparency and trust. Explainable artificial intelligence (xAI) aims to increase our understanding of DL models while preserving their predictive power. In OpenSR, we will provide quality indices and xAI tools for trusted and accountable SR. In particular, we will combine traditional model inspection and interpretation methods with more specific post-hoc methods. These quality metrics will be crystallized in a quality assurance band (BQA) accompanying the SR product.
Regarding the quality metrics and uncertainty estimation for remote sensing super-resolution, most of the proposed super-resolution algorithms utilize three commonly used metrics: PSNR, SSIM, and LPIPS. While these metrics are frequently reported in remote sensing super-resolution works, other types of degradation, such as chrominance differences or spatial shifts, might compromise the super-resolution evaluation scope. OpenSR will evaluate the entire super-resolution process with diferent metrics:</p><br><p align=center><img src=/images/projects/qualitymetrics.webp width=50%><p align=center><em>Figure: Quality metrics proposed to benchmark super-resolution algorithms</em></p></p><br><h1 id=benchmarking-toolbox>Benchmarking Toolbox</h1><p>Defining &lsquo;high-quality&rsquo; results in SR methodologies remains a controversial issue. Present-day literature of the field often presents SR techniques through the lens of computer vision, leaning heavily on synthetic datasets and metrics that are hyper-sensitive to factors unrelated to spatial resolution enhancement. To address this challenge, in OpenSR project we have created a comprehensive benchmark, OpenSR-test toolbox, designed exclusively for evaluating remote sensing image SR. Our framework incorporates quality metrics and three curated datasets, each spanning various scale factors with consistent metadata. The benchmarking framework will be publicly available at OpenSR-test.</p><br><p align=center><img src=/images/projects/opnsr-test.webp width=60%><p align=center><em>Figure: OpenSR-test benchmark for real-world Sentinel-2 imagery super-resolution.</em></p></p><br><p align=center><img src=/images/projects/groups_2.webp width=50%></p></p></div></div></main></body><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js integrity=sha384-C6RzsynM9kWDrMNeT87bh95OGNyZPhcTNXj1NW7RuBCsyN/o0jlpcV8Qyq46cDfL crossorigin=anonymous defer></script><script type=text/javascript src=https://cdn.jsdelivr.net/gh/pcooksey/bibtex-js@1.0.0/src/bibtex_js.min.js defer></script></html>