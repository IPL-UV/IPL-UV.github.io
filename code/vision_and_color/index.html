<!doctype html><html lang=en-us dir=ltr><head><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>ISP - Image and Signal Processing group</title>
<link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH crossorigin=anonymous><link rel="shortcut icon" href=/images/favicon.ico type=image/x-icon><link rel=stylesheet href=/github/style/style.min.f7dd1c60a6cdba402b3b03b09262e8921a1b461c6157b54a640584ac10ed246e.css integrity="sha256-990cYKbNukArOwOwkmLokhobRhxhV7VKZAWErBDtJG4=" crossorigin=anonymous><script src=/github/js/mode.f2979a93a325fecf9605263bd141398a311c8e23388ed7dcff74f92f7e632866.js integrity="sha256-8peak6Ml/s+WBSY70UE5ijEcjiM4jtfc/3T5L35jKGY=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/marked/marked.min.js></script></head></head><nav class="navbar navbar-expand-lg bg-body-tertiary fixed-top"><div class=container-fluid><a href=/ class="d-lg-none d-flex align-items-center a_logonav"><img src=/images/isp_logo_sinfondo.webp alt="ISP Icon" height=30 class=logo_nav>
<span class="ms-2 text-isp">ISP</span>
</a><button class="navbar-toggler ms-auto" type=button data-bs-toggle=collapse data-bs-target=#navbarTogglerDemo01 aria-controls=navbarTogglerDemo01 aria-expanded=false aria-label="Toggle navigation" style=height:40px>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarTogglerDemo01><ul class="navbar-nav mx-auto mb-lg-0"><li class="nav-item px-2 nav-item-highlight d-none d-lg-block"><a class="nav-link a" aria-current=page href=/github/>ISP</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/github/people/>People</a></li><li class="nav-item dropdown px-2 nav-item-highlight"><a class="nav-link dropdown-toggle a" href=/github/research/philosophy id=navbarDropdownResearch role=button aria-expanded=false>Research</a><ul class=dropdown-menu aria-labelledby=navbarDropdownResearch><li><a class="dropdown-item a" href=/github/research/machine_learning/>Machine learning</a></li><li><a class="dropdown-item a" href=/github/research/visual_neuroscience/>Visual neuroscience</a></li><li><a class="dropdown-item a" href=/github/research/visual_brain/>Visual brain</a></li><li><a class="dropdown-item a" href=/github/research/earth_science/>Earth science</a></li><li><a class="dropdown-item a" href=/github/research/social_science>Social science</a></li></ul></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/github/projects/>Projects</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/github/facilities/>Facilities</a></li><li class="nav-item dropdown px-2 nav-item-highlight"><a class="nav-link dropdown-toggle a" href=/github/publications/journals/ id=navbarDropdownPublications role=button aria-expanded=false>Publications</a><ul class=dropdown-menu aria-labelledby=navbarDropdownPublications><li><a class="dropdown-item a" href=/github/publications/journals/>Journals</a></li><li><a class="dropdown-item a" href=/github/publications/conferences/>Conferences</a></li><li><a class="dropdown-item a" href=/github/publications/books/>Books</a></li><li><a class="dropdown-item a" href=/github/publications/talks/>Talks</a></li><li><a class="dropdown-item a" href=/github/publications/technical_reports/>Technical Reports</a></li><li><a class="dropdown-item a" href=/github/publications/theses/>Theses</a></li></ul></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/github/code/>Code</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/github/data/>Data</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/github/seminars/>Seminars</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/github/courses/>Courses</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/github/collaborators/>Collaborators</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/github/news/>News</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/github/contact/>Contact</a></li></ul></div></div></nav><style>.navbar{--bs-navbar-padding-y:0rem !important;background-color:#222!important}.a{color:#949494!important}.a:hover{color:#fff!important}.nav-item-highlight{padding:.4rem}.nav-item-highlight:hover{background-color:#2d70aa!important}.dropdown-menu{background-color:#333!important;color:#fff!important;display:block}.dropdown-item{color:#949494!important}.dropdown-item:hover{background-color:#2d70aa!important;color:#fff!important}.navbar-nav li.nav-item .nav-link,.dropdown-menu .dropdown-item{transition:color .3s,background-color .3s}.nav-link.active,.dropdown-item.active{color:#fff!important;background-color:#007bff!important}.navbar-toggler-icon{background-image:url("data:image/svg+xml,%3csvg viewBox='0 0 30 30' xmlns='http://www.w3.org/2000/svg'%3e%3cpath stroke='white' stroke-width='2' stroke-linecap='round' stroke-miterlimit='10' d='M4 7h22M4 15h22M4 23h22'/%3e%3c/svg%3e")}.navbar-toggler{border:none}.navbar-toggler:focus{outline:none}@media(min-width:992px){.dropdown-menu{display:none}.dropdown:hover .dropdown-menu{display:block}}@media(max-width:991px){.navbar-nav{max-height:calc(100vh - 56px);overflow-y:auto}.navbar-nav::-webkit-scrollbar{width:9px}.navbar-nav::-webkit-scrollbar-thumb{background-color:#888;border-radius:10px}.navbar-nav::-webkit-scrollbar-thumb:hover{background-color:#555}.navbar-nav .nav-link{font-size:1.2rem}.dropdown-menu .dropdown-item{font-size:1rem}.dropdown-toggle::after{display:inline-block;margin-left:.255em;vertical-align:.255em;content:"";border-top:.3em solid;border-right:.3em solid transparent;border-bottom:0;border-left:.3em solid transparent}.navbar .dropdown-toggle::after{content:none!important}.navbar-toggler-icon{width:1.2em;height:1.2em}.navbar-toggler{border:none!important}.navbar-toggler:focus{box-shadow:none!important}.d-flex{height:45px}.a_logonav{text-decoration:none!important}.text-isp{font-size:1.3rem;color:#9d9d9d;display:inline-block;vertical-align:middle;text-decoration:none!important}.navbar-nav{max-height:calc(50vh - 56px);overflow-y:auto}}</style><script>document.addEventListener("DOMContentLoaded",function(){let e=document.querySelectorAll(".navbar .dropdown");e.forEach(function(e){let t=e.querySelector(".dropdown-toggle");t.addEventListener("click",function(e){window.innerWidth<992&&e.target===t&&(window.location.href=t.href)});let n=e.querySelectorAll(".dropdown-item");n.forEach(function(e){e.addEventListener("click",function(){window.location.href=e.href})})}),document.addEventListener("click",function(t){window.innerWidth<992&&!t.target.closest(".navbar .dropdown")&&e.forEach(function(e){e.querySelector(".dropdown-menu").classList.remove("show")})})})</script><main><div class=container><div class=content-container><div class=title-container><h1>Vision and color science</h1></div><div class=panel-item><div class=panel-header><h4 class=panel-title><b>AfterEffects: Motion, Color and Texture After-Effects</b></h4></div><div class=panel-body><div class="panel-content row"><div class="panel-image col-md-2"><a href=./aftereffects/content><img src=/images/code/motion_after.webp alt="AfterEffects Image"></a></div><div class="panel-description col-md-6"><h5><p class=text-justify>AfterEffects is a web-based platform containing code and data for generating motion, color, and texture after-effects. It reproduces these effects according to infomax or error minimization organization goals using Sequential Principal Curves Analysis (SPCA).</p></h5></div><div class="panel-references col-md-4"><h6><b>References</b></h6><ul class=references-list><li>Sequential Principal Curves Analysis: Infomax and Error Minimization Applications. Available online with datasets.</li></ul></div></div></div></div><div class=panel-item><div class=panel-header><h4 class=panel-title><b>COLORLAB: Visual Statistics Coding and Restoration Toolbox</b></h4></div><div class=panel-body><div class="panel-content row"><div class="panel-image col-md-2"><a href=./colorlab/content><img src=/images/code/color.webp alt="COLORLAB Image"></a></div><div class="panel-description col-md-6"><h5><p class=text-justify>COLORLAB is a color computation and visualization toolbox to be used in the MATLAB environment. It is intended for colorimetric applications, such as color image processing and psychophysical experimentation. The toolbox includes classical Tristimulus representations (e.g. CIE XYZ 1931), current Color Appearance Models (CIE Lab 76 or CIECAM2000), and various tools for visualization and color processing.</p></h5></div><div class="panel-references col-md-4"><h6><b>References</b></h6><ul class=references-list><li>Classical Tristimulus Representations and Current Color Appearance Models for Psychophysical Experimentation. CIE XYZ 1931 and CIECAM2000.</li></ul></div></div></div></div><div class=panel-item><div class=panel-header><h4 class=panel-title><b>CSFTools</b></h4></div><div class=panel-body><div class="panel-content row"><div class="panel-image col-md-2"><a href=./csftools/content><img src=/images/code/csf.webp alt="CSFTools Image"></a></div><div class="panel-description col-md-6"><h5><p class=text-justify>CSFTools is a MATLAB toolbox that provides spatial Contrast Sensitivity Functions (CSF) of the ModelFest Standard Spatial Observer (Watson&amp;Malo 2002). It also includes chromatic CSFs from Mullen (1985). The toolbox covers spatio-temporal CSF in terms of frequency (Kelly 1979) and speed (Daly 2000).</p></h5></div><div class="panel-references col-md-4"><h6><b>References</b></h6><ul class=references-list><li>ModelFest Standard Spatial Observer. Watson, A. & Malo, J., 2002.</li><li>Spatio-Temporal CSFs and Contrast Sensitivity. Mullen, R., 1985.</li></ul></div></div></div></div><div class=panel-item><div class=panel-header><h4 class=panel-title><b>DNTools</b></h4></div><div class=panel-body><div class="panel-content row"><div class="panel-image col-md-2"><a href=./dntools/content><img src=/images/code/neural.webp alt="DNTools Image"></a></div><div class="panel-description col-md-6"><h5><p class=text-justify>DNTools is a MATLAB toolbox that models texture perception using Orthonormal Wavelets and Divisive Normalization. It simulates a V1 cortex model tuned to image quality opinions and is found to perform non-linear ICA on natural images.</p></h5></div><div class="panel-references col-md-4"><h6><b>References</b></h6><ul class=references-list><li>Wavelet-based Texture Perception Model with Divisive Normalization. Available as part of the DNTools Toolbox.</li></ul></div></div></div></div><div class=panel-item><div class=panel-header><h4 class=panel-title><b>LinearNoisyTools</b></h4></div><div class=panel-body><div class="panel-content row"><div class="panel-image col-md-2"><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/TFM_imprimir_ya.pdf><img src=/images/code/opt_archit.webp alt="LinearNoisyTools Image"></a></div><div class="panel-description col-md-6"><h5><p class=text-justify>LinearNoisyTools is a MATLAB toolbox for modeling texture perception, focusing on Poisson noise in V1 sensors. The toolbox includes achromatic CSF filtering, steerable wavelets with optimized architectures, and psychophysically measured noise models.</p></h5></div><div class="panel-references col-md-4"><h6><b>References</b></h6><ul class=references-list><li>Achromatic CSF Filtering and Poisson Noise in V1. Optimized Wavelet Architectures for Texture Perception.</li></ul></div></div></div></div><div class=panel-item><div class=panel-header><h4 class=panel-title><b>TheHumanCamera</b></h4></div><div class=panel-body><div class="panel-content row"><div class="panel-image col-md-2"><a href=./thehumancamera/content><img src=/images/code/human.webp alt="The Human Camera Image"></a></div><div class="panel-description col-md-6"><h5><p class=text-justify>The Human Camera is a Brain-Machine Interface designed for image transmission. It uses the human brain for feature extraction and dimensionality reduction, instead of conventional algorithms. The toolbox showcases decoding algorithms based on a V1 cortex model.</p></h5></div><div class="panel-references col-md-4"><h6><b>References</b></h6><ul class=references-list><li>Brain-Machine Interface for Image Transmission: The Human Camera Toolbox. Decoding Algorithms Based on V1 Cortex Models.</li></ul></div></div></div></div><div class=panel-item><div class=panel-header><h4 class=panel-title><b>Topographic ICA</b></h4></div><div class=panel-body><div class="panel-content row"><div class="panel-image col-md-2"><a href=./tica/content><img src=/images/code/fig_web_3.webp alt="Topographic ICA Image"></a></div><div class="panel-description col-md-6"><h5><p class=text-justify>Topographic Independent Component Analysis (TICA) is a method for learning topographically organized features from visual data. It extends the Independent Component Analysis (ICA) algorithm by incorporating spatial proximity, aiming to model how neurons in the visual cortex represent information with localized and continuous receptive fields. TICA has been applied to understand orientation domains in the brain&rsquo;s visual cortex, but studies have shown that the model may produce discontinuous, scrambled orientation maps, contradicting experimental observations.</p></h5></div><div class="panel-references col-md-4"><h6><b>References</b></h6><ul class=references-list><li>Hyvärinen, A. and Hoyer, P. (2001). Topographic Independent Component Analysis.</li><li>Ma, L. et al. (2008). Neurocomputing: An application of TICA in visual representation.</li></ul></div></div></div></div><div class=panel-item><div class=panel-header><h4 class=panel-title><b>VirtualNeuroLabs</b></h4></div><div class=panel-body><div class="panel-content row"><div class="panel-image col-md-2"><a href=./virtualneurolabs/content><img src=/images/code/mt_theoret_sensit.webp alt="VirtualNeuroLabs Image"></a></div><div class="panel-description col-md-6"><h5><p class=text-justify>VirtualNeuroLabs provides a collection of MATLAB tools designed to simulate physiological and psychophysical experiments. It also illustrates the behavior of neural models when processing complex stimuli. These tools are ideal for students studying visual neuroscience at a graduate level.</p></h5></div><div class="panel-references col-md-4"><h6><b>References</b></h6><ul class=references-list><li>Simulating Physiological and Psychophysical Experiments Using VirtualNeuroLabs. Visual Neuroscience Study Tool.</li></ul></div></div></div></div><p></p></div></div></main></body><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js integrity=sha384-C6RzsynM9kWDrMNeT87bh95OGNyZPhcTNXj1NW7RuBCsyN/o0jlpcV8Qyq46cDfL crossorigin=anonymous defer></script><script type=text/javascript src=https://cdn.jsdelivr.net/gh/pcooksey/bibtex-js@1.0.0/src/bibtex_js.min.js defer></script></html>