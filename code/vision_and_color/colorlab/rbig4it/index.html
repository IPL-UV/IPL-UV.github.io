<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>ISP - Image and Signal Processing group</title>
<link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css integrity="sha512-z3gLpd7yknf1YoNbCzqRKc4qyor8gaKU1qmn+CShxbuBusANI9QpRohGBreCFkKxLhei6S9CQXFEbbKuqLg0DA==" crossorigin=anonymous referrerpolicy=no-referrer><link rel="shortcut icon" href=/github/images/isp_ico.webp type=image/x-icon><link rel=stylesheet href=/github/style/style.css><script src=/github/js/mode.js></script><script src=https://cdn.jsdelivr.net/npm/marked/marked.min.js></script></head><nav class=custom-navbars><div class=custom-container><a href=/ class="custom-logo custom-hide-on-large"><img src=/github/images/isp_logo_sinfondo.webp alt="ISP Icon" class=custom-logo_nav>
<span class=custom-text-isp>ISP</span>
</a><button class=navbar-toggler aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class=custom-navbar-collapse><ul class=custom-navbar-nav><li class="custom-nav-item custom-desktop-only"><a class=custom-nav-link href=/github/>ISP</a></li><li class=custom-nav-item><a class=custom-nav-link href=/github/people/>People</a></li><li class="custom-nav-item custom-dropdown"><a class="custom-nav-link custom-dropdown-toggle" href=/github/research/philosophy>Research</a><ul class=custom-dropdown-menu><li><a class=custom-dropdown-item href=/github/research/machine_learning/>Machine learning</a></li><li><a class=custom-dropdown-item href=/github/research/visual_neuroscience/>Visual neuroscience</a></li><li><a class=custom-dropdown-item href=/github/research/visual_brain/>Visual brain</a></li><li><a class=custom-dropdown-item href=/github/research/earth_science/>Earth science</a></li><li><a class=custom-dropdown-item href=/github/research/social_science>Social science</a></li></ul></li><li class=custom-nav-item><a class=custom-nav-link href=/github/projects/>Projects</a></li><li class=custom-nav-item><a class=custom-nav-link href=/github/facilities/>Facilities</a></li><li class="custom-nav-item custom-dropdown"><a class="custom-nav-link custom-dropdown-toggle" href=/github/publications/journals/>Publications</a><ul class=custom-dropdown-menu><li><a class=custom-dropdown-item href=/github/publications/journals/>Journals</a></li><li><a class=custom-dropdown-item href=/github/publications/conferences/>Conferences</a></li><li><a class=custom-dropdown-item href=/github/publications/books/>Books</a></li><li><a class=custom-dropdown-item href=/github/publications/talks/>Talks</a></li><li><a class=custom-dropdown-item href=/github/publications/technical_reports/>Technical Reports</a></li><li><a class=custom-dropdown-item href=/github/publications/theses/>Theses</a></li></ul></li><li class=custom-nav-item><a class=custom-nav-link href=/github/code/>Code</a></li><li class=custom-nav-item><a class=custom-nav-link href=/github/data/>Data</a></li><li class=custom-nav-item><a class=custom-nav-link href=/github/seminars/>Seminars</a></li><li class=custom-nav-item><a class=custom-nav-link href=/github/courses/>Courses</a></li><li class=custom-nav-item><a class=custom-nav-link href=/github/collaborators/>Collaborators</a></li><li class=custom-nav-item><a class=custom-nav-link href=/github/news/>News</a></li><li class=custom-nav-item><a class=custom-nav-link href=/github/contact/>Contact</a></li></ul></div></div></nav><main><div class=container><div class="content-container grid-layout"><div class=box-header><h1>RBIG4IT: Information Theory Measures via Multidimensional Gaussianization</h1></div><div class=box-abstract><p><p>Information theory is an outstanding framework to measure uncertainty, dependence and relevance in data and systems. It has several desirable properties for real world applications: it naturally deals with multivariate data, it can handle heterogeneous data types, and the measures can be interpreted in physical units. However, it has not been adopted by a wider audience because obtaining information from multidimensional data is a challenging problem due to the curse of dimensionality. Here we propose an indirect way of computing information based on a multivariate Gaussianization transform. Our proposal mitigates the difficulty of multivariate density estimation by reducing it to a composition of tractable (marginal) operations and simple linear transformations, which can be interpreted as a particular deep neural network. We introduce specific Gaussianization-based methodologies to estimate total correlation, entropy, mutual information and Kullback-Leibler divergence. We compare them to recent estimators showing the accuracy on synthetic data generated from different multivariate distributions. We made the tools and datasets publicly available to provide a test-bed to analyze future methodologies. Results show that our proposal is superior to previous estimators particularly in high-dimensional scenarios; and that it leads to interesting insights in neuroscience, geoscience, computer vision, and machine learning.</p><h1 id=software>Software</h1><h2 id=rbig-python-toolboxhttpsgithubcomipl-uvrbig><a href=https://github.com/IPL-UV/rbig>RBIG Python toolbox</a></h2><p>Includes tools to compute Information Theory measures used in the current paper [RBIG4IT2020]</p><h2 id=demo-in-google-colabhttpscolabresearchgooglecomgithubipl-uvrbigblobmasternotebooksinformation_theory_colabipynb><a href=https://colab.research.google.com/github/IPL-UV/rbig/blob/master/notebooks/information_theory_colab.ipynb>Demo in Google Colab</a></h2><p>This demo provides an interactive example of how to use the RBIG Python toolbox to compute information measures like entropy and mutual information. It&rsquo;s an easy-to-follow resource for those interested in testing the methods on their data.</p><h2 id=rbig-matlab-toolboxhttpsgithubcomipl-uvrbig_matlab><a href=https://github.com/IPL-UV/rbig_matlab>RBIG Matlab toolbox</a></h2><p>Includes tools to compute Information Theory measures and scripts to generate the synthetic data for the experiments in the current paper [RBIG4IT2020]</p><h2 id=paper-summary>Paper Summary</h2><p>The measures that can be computed using RBIG defined in this paper are the ones in the following figure + the Kulback-Leibler divergence. The main point is that RBIG allows to get acurated estimations of these measures even in multidimensional datasets.</p><h2 id=extended-results>Extended results</h2><p>Here extra results for the paper are shown. Mainly figures for results on synthetic data that would taken too much space in the original paper.</p><h2 id=total-correlation>Total Correlation</h2><p>The total correlation is a measure of multivariate dependence among several variables. In this section, we show how the RBIG methodology allows for precise estimation of total correlation, even for datasets with non-Gaussian distributions or high dimensionality.</p><h2 id=entropy>Entropy</h2><p>Entropy is a fundamental concept in information theory, representing the amount of uncertainty in a dataset. Using RBIG, we show that entropy can be computed more efficiently compared to classical estimators, particularly in datasets with complex dependencies.</p><h2 id=kld>KLD</h2><p>Kullback-Leibler divergence measures how one probability distribution diverges from a second, reference distribution. RBIG offers a more accurate way to compute KLD in high-dimensional settings, improving performance in tasks like anomaly detection and model comparison.</p><h2 id=mutual-information>Mutual Information</h2><p>Mutual information quantifies the amount of information obtained about one random variable through another. The RBIG methodology demonstrates superior performance in estimating mutual information, especially in datasets with nonlinear dependencies, making it a valuable tool for feature selection and data analysis.</p></p></div><aside class=box-gallery><div class=gallery-grid><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/github/images/code/fig_1.webp","[Information Theory Measures](#paper-summary)","This figure illustrates the key measures of Information Theory that can be estimated using the RBIG methodology, including total correlation, entropy, mutual information, and Kullback-Leibler divergence.")'><img src=/github/images/code/fig_1.webp alt="[Information Theory Measures](#paper-summary)"></a><p class=gallery-title><a href=#paper-summary>Information Theory Measures</a></p><div class=gallery-description><p>This figure illustrates the key measures of Information Theory that can be estimated using the RBIG methodology, including total correlation, entropy, mutual information, and Kullback-Leibler divergence.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/github/images/code/Table_T.webp","[Total Correlation Estimation](#extended-results)","This table shows the total correlation estimation results in relative mean absolute error for different distributions: Gaussian, uniform, and Student PDFs (μ = 3, 5, 20 for each row). Each column corresponds to experiments with varying dimensions D.")'><img src=/github/images/code/Table_T.webp alt="[Total Correlation Estimation](#extended-results)"></a><p class=gallery-title><a href=#extended-results>Total Correlation Estimation</a></p><div class=gallery-description><p>This table shows the total correlation estimation results in relative mean absolute error for different distributions: Gaussian, uniform, and Student PDFs (μ = 3, 5, 20 for each row). Each column corresponds to experiments with varying dimensions D.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/github/images/code/Figure_T.webp","[Total Correlation Results](#total-correlation)","Graphical representation of total correlation estimation results. This figure compares the performance of different estimators for various multivariate distributions and dimensions.")'><img src=/github/images/code/Figure_T.webp alt="[Total Correlation Results](#total-correlation)"></a><p class=gallery-title><a href=#total-correlation>Total Correlation Results</a></p><div class=gallery-description><p>Graphical representation of total correlation estimation results. This figure compares the performance of different estimators for various multivariate distributions and dimensions.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/github/images/code/Table_H.webp","[Entropy Estimation](#entropy)","This table presents the entropy estimation results for different data distributions and dimensional settings, showcasing the performance of the RBIG estimator.")'><img src=/github/images/code/Table_H.webp alt="[Entropy Estimation](#entropy)"></a><p class=gallery-title><a href=#entropy>Entropy Estimation</a></p><div class=gallery-description><p>This table presents the entropy estimation results for different data distributions and dimensional settings, showcasing the performance of the RBIG estimator.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/github/images/code/Figure_H.webp","[Entropy Results](#entropy)","A figure showing the entropy estimation results, comparing the RBIG approach with other methods across various distributions and dimensionalities.")'><img src=/github/images/code/Figure_H.webp alt="[Entropy Results](#entropy)"></a><p class=gallery-title><a href=#entropy>Entropy Results</a></p><div class=gallery-description><p>A figure showing the entropy estimation results, comparing the RBIG approach with other methods across various distributions and dimensionalities.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/github/images/code/Table_KL.webp","[Kullback-Leibler Divergence Estimation](#entropy)","This table summarizes the results of Kullback-Leibler divergence estimations for synthetic datasets, illustrating the performance of RBIG in different high-dimensional settings.")'><img src=/github/images/code/Table_KL.webp alt="[Kullback-Leibler Divergence Estimation](#entropy)"></a><p class=gallery-title><a href=#entropy>Kullback-Leibler Divergence Estimation</a></p><div class=gallery-description><p>This table summarizes the results of Kullback-Leibler divergence estimations for synthetic datasets, illustrating the performance of RBIG in different high-dimensional settings.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/github/images/code/Figure_KL1.webp","[KLD Estimation Results](#kld)","A visual comparison of Kullback-Leibler divergence estimators. The RBIG estimator is shown to perform well in a range of complex, high-dimensional scenarios.")'><img src=/github/images/code/Figure_KL1.webp alt="[KLD Estimation Results](#kld)"></a><p class=gallery-title><a href=#kld>KLD Estimation Results</a></p><div class=gallery-description><p>A visual comparison of Kullback-Leibler divergence estimators. The RBIG estimator is shown to perform well in a range of complex, high-dimensional scenarios.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/github/images/code/Table_I.webp","[Mutual Information Estimation](#kld)","This table presents the mutual information estimation results. The performance of RBIG is compared to other estimators for various distributions.")'><img src=/github/images/code/Table_I.webp alt="[Mutual Information Estimation](#kld)"></a><p class=gallery-title><a href=#kld>Mutual Information Estimation</a></p><div class=gallery-description><p>This table presents the mutual information estimation results. The performance of RBIG is compared to other estimators for various distributions.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/github/images/code/Figure_MI.webp","[Mutual Information Results](#mutual-information)","Graphical results of mutual information estimation comparing RBIG with recent methods across several distributions and dimensions.")'><img src=/github/images/code/Figure_MI.webp alt="[Mutual Information Results](#mutual-information)"></a><p class=gallery-title><a href=#mutual-information>Mutual Information Results</a></p><div class=gallery-description><p>Graphical results of mutual information estimation comparing RBIG with recent methods across several distributions and dimensions.</p></div></div><div id=imageModal class=modal><span class=modal-close onclick=closeModal()>&#215;</span>
<img class=modal-content id=modalImage><div id=modalCaption class=modal-caption></div></div></div></aside><section class="box-references title2"><h2>References</h2><ul class=references-list><li><strong>Information Theory Measures via Multidimensional Gaussianization [RBIG4IT2020]</strong><br><span>V. Laparra, E. Johnson, G. Camps-Valls, R. Santos-Rodriguez, J. Malo.</span><br><em></em></li><li><strong>Iterative Gaussianization: from ICA to Random Rotations [TNN2011]</strong><br><span>V. Laparra, G. Camps & J. Malo.</span><br><em>IEEE Transactions on Neural Networks.</em></li></ul></section><section class="box-enlaces title2"><h2>Download</h2><ul class=enlaces-list><li><a href=https://github.com/IPL-UV/rbig>RBIG Python toolbox</a></li><li><a href=https://colab.research.google.com/github/IPL-UV/rbig/blob/master/notebooks/information_theory_colab.ipynb>Demo in Google Colab</a></li><li><a href=https://github.com/IPL-UV/rbig_matlab>RBIG Matlab toolbox</a></li></ul></section></div></div></main></body><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js integrity=sha384-C6RzsynM9kWDrMNeT87bh95OGNyZPhcTNXj1NW7RuBCsyN/o0jlpcV8Qyq46cDfL crossorigin=anonymous defer></script><script type=text/javascript src=https://cdn.jsdelivr.net/gh/pcooksey/bibtex-js@1.0.0/src/bibtex_js.min.js defer></script></html>