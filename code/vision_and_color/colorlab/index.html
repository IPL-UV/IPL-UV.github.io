<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>ISP - Image and Signal Processing group</title>
<link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css integrity="sha512-z3gLpd7yknf1YoNbCzqRKc4qyor8gaKU1qmn+CShxbuBusANI9QpRohGBreCFkKxLhei6S9CQXFEbbKuqLg0DA==" crossorigin=anonymous referrerpolicy=no-referrer><link rel="shortcut icon" href=/try-isp-page/images/isp_ico.webp type=image/x-icon><link rel=stylesheet href=/try-isp-page/style/style.css><script src=/try-isp-page/js/mode.js></script><script src=https://cdn.jsdelivr.net/npm/marked/marked.min.js></script></head><nav class=custom-navbars><div class=custom-container><a href=/ class="custom-logo custom-hide-on-large"><img src=/try-isp-page/images/isp_logo_sinfondo.webp alt="ISP Icon" class=custom-logo_nav>
<span class=custom-text-isp>ISP</span>
</a><button class=navbar-toggler aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class=custom-navbar-collapse><ul class=custom-navbar-nav><li class="custom-nav-item custom-desktop-only"><a class=custom-nav-link href=/try-isp-page/>ISP</a></li><li class=custom-nav-item><a class=custom-nav-link href=/try-isp-page/people/>People</a></li><li class="custom-nav-item custom-dropdown"><a class="custom-nav-link custom-dropdown-toggle" href=/try-isp-page/research/philosophy/>Research</a><ul class=custom-dropdown-menu><li><a class=custom-dropdown-item href=/try-isp-page/research/philosophy/>Philosophy</a></li><li><a class=custom-dropdown-item href=/try-isp-page/research/machine_learning/>Machine learning</a></li><li><a class=custom-dropdown-item href=/try-isp-page/research/visual_neuroscience/>Visual science</a></li><li><a class=custom-dropdown-item href=/try-isp-page/research/visual_brain/>Image processing</a></li><li><a class=custom-dropdown-item href=/try-isp-page/research/earth_science/>Earth science</a></li><li><a class=custom-dropdown-item href=/try-isp-page/research/social_science>Social science</a></li></ul></li><li class=custom-nav-item><a class=custom-nav-link href=/try-isp-page/projects/>Projects</a></li><li class=custom-nav-item><a class=custom-nav-link href=/try-isp-page/facilities/>Facilities</a></li><li class="custom-nav-item custom-dropdown"><a class="custom-nav-link custom-dropdown-toggle" href=/try-isp-page/publications/journals/>Publications</a><ul class=custom-dropdown-menu><li><a class=custom-dropdown-item href=/try-isp-page/publications/journals/>Journals</a></li><li><a class=custom-dropdown-item href=/try-isp-page/publications/conferences/>Conferences</a></li><li><a class=custom-dropdown-item href=/try-isp-page/publications/books/>Books</a></li><li><a class=custom-dropdown-item href=/try-isp-page/publications/talks/>Talks</a></li><li><a class=custom-dropdown-item href=/try-isp-page/publications/technical_reports/>Technical Reports</a></li><li><a class=custom-dropdown-item href=/try-isp-page/publications/theses/>Theses</a></li></ul></li><li class=custom-nav-item><a class=custom-nav-link href=/try-isp-page/code/>Code</a></li><li class=custom-nav-item><a class=custom-nav-link href=/try-isp-page/data/>Data</a></li><li class=custom-nav-item><a class=custom-nav-link href=/try-isp-page/seminars/>Seminars</a></li><li class=custom-nav-item><a class=custom-nav-link href=/try-isp-page/courses/>Courses</a></li><li class=custom-nav-item><a class=custom-nav-link href=/try-isp-page/collaborators/>Collaborators</a></li><li class=custom-nav-item><a class=custom-nav-link href=/try-isp-page/news/>News</a></li><li class=custom-nav-item><a class=custom-nav-link href=/try-isp-page/contact/>Contact</a></li></ul></div></div></nav><main><div class=container><div class=content-container><div class=grid-container id=grid-container><div class=grid-item><a href><img src=/try-isp-page/images/code alt="ColorLab: The Matlab Toolbox for Colorimetry and Color Vision"></a><div class=text><a href class=nameLink_a>ColorLab: The Matlab Toolbox for Colorimetry and Color Vision</a><p><p><strong>ColorLab</strong> is a color computation and visualization toolbox to be used in the MATLAB environment. <strong>ColorLab</strong> is intended to deal with color in general-purpose quantitative colorimetric applications as color image processing and psychophysical experimentation.</p><p><strong>ColorLab</strong> uses colorimetrically meaningful representations of color and color images (tristimulus values, chromatic coordinates and luminance, or, dominant wavelength, purity and luminance), in any primaries system of the tristimulus colorimetry (including CIE standards as CIE XYZ or CIE RGB). <strong>ColorLab</strong> relates this variety of colorimetric representations to the usual device-dependent discrete-color representation, i.e. it solves the problem of displaying a colorimetrically specified scene in the monitor within the accuracy of the VGA.</p><p>A number of other interesting color representations are also provided, as CIE uniform color spaces (as CIE Lab and CIE Luv, opponent color representations based on advanced color vision models, and color appearance representations (RLab, LLab, SVF and CIECAMs). All these representations are invertible, so the result of image processing made in these colorimetrically meaningful representations can always be inverted back to the tristimulus representation at hand, and be displayed. <strong>ColorLab</strong> includes useful visualization routines to represent colors in the tristimulus space or in the chromatic diagram of any color basis, as well as an advanced vector quantization scheme for color palette design. An extensive color data base is also included, with the CIE 1931 color matching functions, reflectance data of 1250 chips from the Munsell Book of Color, McAdam ellipses, normalized spectra of a number of standard CIE illuminants, matrices to change to a number of tristimulus representations, and calibration data of an ordinary CRT monitor.</p><p>The standard tools in ColorLab (and in <a href=./../vistalab><strong>VistaLab</strong></a>) are the necessary building blocks to develop more sophisticated vision models included in the dedicated site <a href=./../vistamodels><strong>VistaModels</strong></a>.</p><h1 id=table-of-contents>Table of Contents</h1><ul><li><a href=#colorfulness-edition-using-the-purity>Colorfulness edition using the purity</a></li><li><a href=#hue-based-segmentation-and-edition-using-the-dominant-wavelength>Hue-based segmentation and edition using the dominant wavelength</a></li><li><a href=#luminance-edition-in-cd/m2>Luminance edition in cd/m2</a></li><li><a href=#changing-the-spectral-illumination-standard-and-user-defined-illuminants>Changing the spectral illumination (standard and user defined illuminants)</a></li><li><a href=#playing-with-mcadam-ellipses-and-munsell-chips>Playing with McAdam ellipses and Munsell chips</a></li><li><a href=#chromatic-induction-in-llab>Chromatic induction in LLab</a></li></ul><h1 id=colorfulness-edition-using-the-purity>Colorfulness edition using the purity</h1><p>Colorimetric Purity and Excitation Purity are the descriptors of colorfulness in Tristimulus Colorimetry. Both of them are available in ColorLab. In the example below we analyze the colors of an image in the CIE XYZ system and reduce the excitation purity by a constant factor leaving the luminace and the dominant wavelength unaltered in order to obtain an image with reduced colorfulness. Other posibilities to obtain this effect with ColorLab include using any other tristimulus representations or changing the colorfulness descriptors in a number of available non-linear color appearance models.</p><h1 id=hue-based-segmentation-and-edition-using-the-dominant-wavelength>Hue-based segmentation and edition using the dominant wavelength</h1><p>The Dominant Wavelength is the descriptor of hue in Tristimulus Colorimetry. In the example below we first segment the flowers by selecting a range of wavelenghts (in the CIE XYZ chromatic diagram) and then, we modify their hue by applying a rotation to the chromatic coordinates. Other posibilities to obtain this effect with ColorLab include using any other tristimulus representation or changing (rotating) the hue descriptor in a number of available non-linear color appearance models.</p><h1 id=luminance-edition-in-cdm24>Luminance edition in cd/m24</h1><p>The Luminance is the descriptor of brightness in Tristimulus Colorimetry. In the example below we reduce the luminance by reducing the lenght of the tristimulus vectors by a constant factor in an arbitrary (RBG) tristimulus space (note how the chromatic diagram is twisted). Of course the chromatic coordinates remain the same (as can be seen in the figures below). Other posibilities to obtain this effect with ColorLab include using any other tristimulus representation or changing the brightness descriptor in a number of available non-linear color appearance models.</p><h1 id=changing-the-spectral-illumination-standard-and-user-defined-illuminants>Changing the spectral illumination (standard and user defined illuminants)</h1><p>ColorLab is able to deal with the spectro-radiometric description of color images or estimate it from their (usual) colorimetric description by using the Munsell reflectances data set. In this way, the effect of changing the spectral radiance of the illuminant may be simulated by obtaining the new tristimulus values with the new illuminant. In the example below, each pixel of the original image is assumed to be a patch with a given (or estimated) reflectance under white light illumination. The user may define a different illuminant (in this case a purple radiation) and apply it to the reflectances, thus obtaining the new image and the new (tristimulus) colors. Of course, this can be done in any tristimulus representation. But, better than that, if non-linear color appearance models are used together with the corresponding pair procedure [JOSA A 04], color constancy may be predicted!.</p><h1 id=playing-with-mcadam-ellipses-and-munsell-chips>Playing with McAdam ellipses and Munsell chips</h1><p>Now you can easily check the non-uniformity of the tristimulus space in your computer screen! As ColorLab comes with the McAdam ellipses database and the Munsell chips database, its color reproduction ability allows you to generate the right colors to prove that your discrimination is not Euclidean.
In the first example below, we distort two given colors (green and blue) in by a constant factor in the chromatic diagram in the principal directions of the ellipsoids. Despite the eventual inaccuracies introduced by the use of a generic calibration, it is clear that blues are more different each other (the ellipse is smaller!) and the distortion in every case is more noticeable when it is done in the short direction of the ellipse.
The second example shows a set of Munsell chips of different chroma which are chosen to depart each other a constant number of JNDs.</p><h1 id=chromatic-induction-in-llab>Chromatic induction in LLab</h1><p>The perception of a test is modified by the stimuli in the surround. This is referred to as chromatic induction. In the example below, the (physically constant) gray test in the center changes its hue to blueish as the surround gets more yellow. Non-linear color appearance models are required to understand this effect.</p><h2 id=key-capabilities>Key Capabilities</h2><ul><li><strong>Visualization</strong>: Visualize color in tristimulus spaces or chromatic diagrams.</li><li><strong>Transformation</strong>: Move between tristimulus and non-linear color models like CIECAM.</li><li><strong>Quantitative Processing</strong>: Apply functions for color purity, luminance, and hue manipulation.</li><li><strong>Extensive Color Database</strong>: Includes CIE color matching functions, Munsell chips, McAdam ellipses, and more.</li></ul><h2 id=download-colorlab>Download ColorLab</h2><ul><li><strong>Toolbox</strong>: <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/Colorlab.zip>Colorlab.zip (15MB)</a></li><li><strong>User Guide</strong>: <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/COLORLAB_userguide.pdf>ColorLab_userguide.pdf (12MB)</a></li></ul></p></div></div><div class=grid-item><a href><img src=/try-isp-page/images/code alt="RBIG4IT: Information Theory Measures via Multidimensional Gaussianization"></a><div class=text><a href class=nameLink_a>RBIG4IT: Information Theory Measures via Multidimensional Gaussianization</a><p><p>Information theory is an outstanding framework to measure uncertainty, dependence and relevance in data and systems. It has several desirable properties for real world applications: it naturally deals with multivariate data, it can handle heterogeneous data types, and the measures can be interpreted in physical units. However, it has not been adopted by a wider audience because obtaining information from multidimensional data is a challenging problem due to the curse of dimensionality. Here we propose an indirect way of computing information based on a multivariate Gaussianization transform. Our proposal mitigates the difficulty of multivariate density estimation by reducing it to a composition of tractable (marginal) operations and simple linear transformations, which can be interpreted as a particular deep neural network. We introduce specific Gaussianization-based methodologies to estimate total correlation, entropy, mutual information and Kullback-Leibler divergence. We compare them to recent estimators showing the accuracy on synthetic data generated from different multivariate distributions. We made the tools and datasets publicly available to provide a test-bed to analyze future methodologies. Results show that our proposal is superior to previous estimators particularly in high-dimensional scenarios; and that it leads to interesting insights in neuroscience, geoscience, computer vision, and machine learning.</p><h1 id=software>Software</h1><h2 id=rbig-python-toolboxhttpsgithubcomipl-uvrbig><a href=https://github.com/IPL-UV/rbig>RBIG Python toolbox</a></h2><p>Includes tools to compute Information Theory measures used in the current paper [RBIG4IT2020]</p><h2 id=demo-in-google-colabhttpscolabresearchgooglecomgithubipl-uvrbigblobmasternotebooksinformation_theory_colabipynb><a href=https://colab.research.google.com/github/IPL-UV/rbig/blob/master/notebooks/information_theory_colab.ipynb>Demo in Google Colab</a></h2><p>This demo provides an interactive example of how to use the RBIG Python toolbox to compute information measures like entropy and mutual information. It&rsquo;s an easy-to-follow resource for those interested in testing the methods on their data.</p><h2 id=rbig-matlab-toolboxhttpsgithubcomipl-uvrbig_matlab><a href=https://github.com/IPL-UV/rbig_matlab>RBIG Matlab toolbox</a></h2><p>Includes tools to compute Information Theory measures and scripts to generate the synthetic data for the experiments in the current paper [RBIG4IT2020]</p><h2 id=paper-summary>Paper Summary</h2><p>The measures that can be computed using RBIG defined in this paper are the ones in the following figure + the Kulback-Leibler divergence. The main point is that RBIG allows to get acurated estimations of these measures even in multidimensional datasets.</p><h2 id=extended-results>Extended results</h2><p>Here extra results for the paper are shown. Mainly figures for results on synthetic data that would taken too much space in the original paper.</p><h2 id=total-correlation>Total Correlation</h2><p>The total correlation is a measure of multivariate dependence among several variables. In this section, we show how the RBIG methodology allows for precise estimation of total correlation, even for datasets with non-Gaussian distributions or high dimensionality.</p><h2 id=entropy>Entropy</h2><p>Entropy is a fundamental concept in information theory, representing the amount of uncertainty in a dataset. Using RBIG, we show that entropy can be computed more efficiently compared to classical estimators, particularly in datasets with complex dependencies.</p><h2 id=kld>KLD</h2><p>Kullback-Leibler divergence measures how one probability distribution diverges from a second, reference distribution. RBIG offers a more accurate way to compute KLD in high-dimensional settings, improving performance in tasks like anomaly detection and model comparison.</p><h2 id=mutual-information>Mutual Information</h2><p>Mutual information quantifies the amount of information obtained about one random variable through another. The RBIG methodology demonstrates superior performance in estimating mutual information, especially in datasets with nonlinear dependencies, making it a valuable tool for feature selection and data analysis.</p></p></div></div><div class=grid-item><a href><img src=/try-isp-page/images/code alt="Spatio-Chromatic Information available from different Neural Layers (J. Malo, Journal of Mathematical Neuroscience 2020)"></a><div class=text><a href class=nameLink_a>Spatio-Chromatic Information available from different Neural Layers (J. Malo, Journal of Mathematical Neuroscience 2020)</a><p><p>The image representations along the retina-cortex pathway are analyzed in terms of their ability to capture information about the visual scenes. The considered series of representations includes: (1) the LMS retinal images, (2) their von-Kries adapted version, (3) the opponent images at LGN, (4) their nonlinear version after Weber-like saturation, (5) the cortical local-frequency representation filtered by achromatic and chromatic CSFs, and (6) the cortical representation after divisive normalization.</p><p>Assuming a single-step transform from the retinal input to each of these representations, and sensors of the same Signal-to-Noise quality in each representation, our estimations of transmitted information show that: (a) progressively deeper representations are better in terms of the amount of captured information, (b) the transmitted information up to the cortical representation follows the probability of natural scenes over the chromatic and achromatic dimensions of the stimulus space, (c) the contribution of spatial transforms to capture visual information is substantially greater (67%) than the contribution of chromatic transforms (33%), and (d) nonlinearities of the responses contribute substantially to the transmitted information (about 28%) but less than the linear transforms (72%).</p><p>The parameters of the model have psychophysical origin and they were not statistically optimized in any way. The information estimates were computed using our <a href=./../rbig4it>Gaussianization transform</a> (Laparra et al. IEEE TNN 11) over our database of colorimetrically-calibrated images (Laparra et al. Neur.Comp. 12), but equivalent results are obtained using other estimates (e.g. offset corrected Kozachenko-Leonenko [Marin et al. IEEE PAMI 13]) or other databases (e.g. Foster et al [Vis.Res.15,16]).</p></p></div></div><div class=grid-item><a href><img src=/try-isp-page/images/code alt="VistaLab: The Matlab Toolbox for Linear Spatio-Temporal Vision Models"></a><div class=text><a href class=nameLink_a>VistaLab: The Matlab Toolbox for Linear Spatio-Temporal Vision Models</a><p><h1 id=the-matlab-toolbox-for-linear-spatio-temporal-vision-models>The Matlab toolbox for linear spatio-temporal Vision Models</h1><p><strong>VistaLab</strong> is a Matlab toolbox that provides the linear building-blocks to create spatio-temporal vision models and the tools to control the spatio-temporal properties of video sequences. These building blocks include the spatio-temporal receptive fields of LGN, V1, and MT cells, and the spatial and spatio-temporal Contrast Sensitivity Functions (CSFs). Additionally, <strong>VistaLab</strong> allows accurate spatio-temporal sampling, spatio-temporal Fourier domain visualization, and generation of video sequences with controlled texture and speed. Tools for video sequence generation include noise, random dots, and rigid-body animations with Lambertian reflectance.</p><p>The perception and video synthesis tools enable accurate illustrations of the visibility of achromatic spatio-temporal patterns. Linear filters in <strong>VistaLab</strong> provide rough approximations of pattern visibility, which can be enhanced with non-linear models available in related toolboxes.</p><p>The <strong>standard tools in VistaLab</strong> (and <a href=./../content><strong>ColorLab</strong></a>) are essential for building more sophisticated vision models, available on the <a href=./../vistamodels><strong>VistaModels</strong></a> dedicated site.</p><h1 id=table-of-contents>Table of Contents</h1><ul><li><a href=#retina-and-lateral-geniculate-nucleus-lgn>Retina and Lateral Geniculate Nucleus (LGN)</a></li><li><a href=#primary-visual-cortex-v1>Primary Visual Cortex (V1)</a></li><li><a href=#middle-temporal-mt-region>Middle Temporal (MT) region</a></li><li><a href=#spatio-temporal-contrast-sensitivities>Spatio-temporal Contrast Sensitivities</a></li><li><a href=#controlled-spatio-temporal-stimuli>Controlled spatio-temporal stimuli</a></li><li><a href=#extensions-of-vistalab>Extensions of VistaLab</a></li></ul><h1 id=retina-and-lateral-geniculate-nucleus-lgn>Retina and Lateral Geniculate Nucleus (LGN)</h1><p>Most of the Retinal Ganglion Cells and cells in the LGN can be modelled with center-surround receptive fields with monophasic or biphasic temporal response. VistaLab comes with a configurable implemenation of such receptive fields according to the general expressions in [Cai, Freeman, DeAngelis, J. Neurophysiol. 97]. Using these units it is easy to generate artificial retinas with arbitrary sampling [Martinez-Garcia et al. 16, Martinez-Garcia et al. 17].</p><p>The examples below show (a) the receptive field of a representative neuron in the spatiotemporal and in the 3D Fourier domain, and (b) the response of a population of such neurons to a natural movie assuming uniform retinal sampling and spatial invariance of the receptive field. VistaLab allows explicit implementation of each sensor response using the scalar product by the corresponding receptive field to get rid of the uniform sampling and the convolution assumptions.</p><h1 id=primary-visual-cortex-v1>Primary Visual Cortex (V1)</h1><p>Simple cells in the V1 cortex can be modelled with Gabor-like receptive fields tuned to certain spatial and temporal frequencies. VistaLab comes with a configurable implemenation of such receptive fields according to the general expressions in [Daugman JOSA A 89, Simoncelli & Heeger Vis. Res. 98]. Using these units it is easy to generate artificial cortex with arbitrary sampling [Martinez-Garcia et al. 17].</p><p>The examples below show six representative neurons tuned to the same spatial frequencies (7 cpd) but different temporal frequencies 2, 7, and 10 Hz, both positive and negative. Eventhough there is no conclusive tuning to two-dimensional speed due to the aperture problem [Heeger JOSA 87], in the direction perpendicular to the grating, these are tuned to 0.3, 1 and 1.5 degrees/sec respectively (both positive and negative). Figures show: (a) the receptive fields in the spatiotemporal and in the 3D Fourier domain, and (b) the response of a population of such neurons to a natural movie assuming uniform retinal sampling and spatial invariance of the receptive field. VistaLab allows explicit implementation of each sensor response using the scalar product by the corresponding receptive field to get rid of the uniform sampling and the convolution assumptions.</p><h1 id=middle-temporal-mt-region>Middle Temporal (MT) region</h1><p>Cells in the MT cortex receive projections from V1 cells aligned in a plane in the spatio-temporal Fourier domain. Therefore, they are narrow-band in speed tuning. VistaLab comes with a configurable implemenation of such receptive fields according to the general expressions in [Simoncelli & Heeger Vis. Res. 98]. Using these units and a spatio-temporal window it is easy to generate artificial MT cortex with arbitrary sampling [Martinez-Garcia et al. 17].</p><p>The examples below shows six representative sets of neurons tuned to tuned to speeds of 0.3, 1 and 1.5 degrees/sec respectively (both positive and negative). In this case while Figures show: (a) the receptive fields in 3D Fourier domain, the kind of features these cells are optimally tuned to, and (b) the response of a population of such neurons to a natural movie assuming uniform retinal sampling and spatial invariance of the receptive field. VistaLab allows explicit implementation of each sensor response using the scalar product by the corresponding receptive field to get rid of the uniform sampling and the convolution assumptions.</p><h1 id=spatio-temporal-contrast-sensitivities>Spatio-temporal Contrast Sensitivities</h1><p>VistaLab comes with different Contrast Sensitivity Functions (CSFs): (a) the spatial-achromatic CSF from the OSA Standard Spatial Observer <a href=http://www.uv.es/vista/vistavalencia/papers/icip02.pdf>Watson & Malo IEEE ICIP 02</a>, (b) the spatial-chromatic, Red-Green and Yellow-Blue CSFs of K. Mullen [Vis. Res. 85], with approrpiate scaling <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/2012b_Gutierrez_RPTSP_12c.PDF>Gutierrez et al. 12</a>, and (c) the achromatic spatio-temporal CSFs of D. Kelly [JOSA 79], and S. Daly (with object tracking speed compensation) [SPIE 98].</p><h1 id=controlled-spatio-temporal-stimuli>Controlled spatio-temporal stimuli</h1><p>The movies below illustrate the abilities of VistaLab for accurate motion control.</p><ul><li><p><strong>First row:</strong> includes sequences of the motion of a lambertian rigid body evolving in a gravitatory field with inelastic restrictions recorded from different points of view, this example allows arbitrary locations of the illumination and camera. In this case the actual motion in 3D world and the optical flow (motion in the retinal plane) are known.</p></li><li><p><strong>Second row:</strong> includes an example of random dots moving according to arbitrary optical flow fields.</p></li><li><p><strong>Third row:</strong> shows how static pictures can be animated using spatially uniform flows of arbitrary speed leading to interesting shape-from-motion effects in the case of noise patterns.</p></li><li><p><strong>Fourth row:</strong> shows different movies of the same periodic pattern moving at progressively increasing speeds. Aliasing introduces speed reversal at the expected place, as demonstrated by the Fourier diagrams below.</p></li></ul><h1 id=extensions-of-vistalab>Extensions of VistaLab</h1><p>VistaLab only addresses the linear part of the neural mechanisms that mediate the preattentive perception of spatio-temporal patterns. However, it doesnt combine these mechanisms to compute motion (optical flow), it doesnt include the nonlinear interactions between the linear mechanisms, and it doesnt include color.</p><p>These issues can be addressed with other toolboxes, namely <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/VistaVideoCoding.zip>VistaVideoCoding</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/BioMultiLayer_L_NL_color.zip>BioMultiLayer_L_NL_color</a> in VistaModels, and <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/Colorlab.zip>Colorlab</a>.</p><h1 id=key-capabilities>Key Capabilities</h1><ul><li><strong>Spatio-temporal Modeling</strong>: Build models for LGN, V1, and MT neural responses.</li><li><strong>Contrast Sensitivity</strong>: Apply achromatic and chromatic CSFs to video and images.</li><li><strong>Video Synthesis</strong>: Create controlled video sequences with specific spatio-temporal properties.</li><li><strong>Fourier Domain Tools</strong>: Visualize spatio-temporal frequency response of neural models.</li></ul></p></div></div><div class=grid-item><a href><img src=/try-isp-page/images/code alt="VistaModels: Computational models of Visual Neuroscience"></a><div class=text><a href class=nameLink_a>VistaModels: Computational models of Visual Neuroscience</a><p><p>The Toolboxes in the VistaModels site are organized in three categories of different nature: <a href=#a-empirical-mechanistic-models><strong>(a) Empirical-mechanistic Models</strong></a>, tuned to reproduce basic phenomena of color and texture perception, <a href=#b-efficient-coding-in-mechanistic-models><strong>(b) Principled Models</strong></a>, derived from information theoretic arguments, and <a href=#c-engineering-motivated-models><strong>(c) Engineering-motivated Models</strong></a>, developed to address applied problems in image and video processing.</p><p>The algorithms in <strong>VistaModels</strong> require the standard building blocks provided in the (more basic) toolboxes VistaLab and ColorLab. However, the necessary functions from these more basic toolboxes are included in the packages listed below for the user convenience.</p><h1 id=table-of-contents>Table of contents</h1><ul><li><a href=#a-empirical-mechanistic-models><strong>(A) Empirical-mechanistic Models</strong></a><ul><li><a href=#1995---2008-linear-opponent-color-channels-local-dct-and-divisive-normalization>1995 - 2008: Linear opponent color channels, local-DCT and Divisive Normalization</a></li><li><a href=#2009---2010-linear-opponent-color-channels-orthogonal-wavelet-and-divisive-normalization>2009 - 2010: Linear opponent color channels, Orthogonal Wavelet and Divisive Normalization</a></li><li><a href=#2013---2018-multi-layer-network-with-nonlinear-opponent-color-overcomplete-wavelet-and-divisive-normalization>2013 - 2018: Multi-Layer network with nonlinear opponent color, Overcomplete Wavelet and Divisive Normalization</a></li><li><a href=#2019---2021-convolutional-and-differentiable-implementations>2019 - 2021: Convolutional and differentiable implementations</a></li><li><a href=#psychophysical-test-bed-for-model-tuning-and-comparison>Psychophysical test-bed for model tuning and comparison</a></li><li><a href=#model-comparison>Model Comparison</a></li></ul></li><li><a href=#b-principled-models><strong>(B) Principled Models</strong></a><ul><li><a href=#efficient-coding-in-mechanistic-models>Efficient coding in mechanistic models</a></li><li><a href=#statistically-based-linear-receptive-fields>Statistically-based linear receptive fields</a></li><li><a href=#statistically-based-nonlinearities>Statistically-based nonlinearities</a></li></ul></li><li><a href=#c-engineering-motivated-models><strong>(C) Engineering-motivated Models</strong></a><ul><li><a href=#perceptually-weighted-motion-estimation-vistavideocoding>Perceptually-weighted motion estimation: VistaVideoCoding</a></li><li><a href=#image-coding-vistacore>Image Coding: VistaCoRe</a></li><li><a href=#image-and-video-quality-vistaqualitytools>Image and Video Quality: VistaQualityTools</a></li></ul></li></ul><h1 id=a-empirical-mechanistic-models>(A) Empirical-mechanistic Models</h1><p>Cascades of linear transforms and nonlinear saturations are ubiquitous in neuroscience and artificial intelligence ever since the [<a href=http://www.scholarpedia.org/article/Models_of_visual_cortex>McCulloch-Pitts model</a>]. More recently this has been exemplified in subtractive and divisive models of cortical interaction [Wilson & Cowan, Kybernetik 73; Carandini and Heeger, Nature Rev. Neurosci. 12].</p><p>Over the years, we have developed progressively better versions of such cascades to be applicable to color images and video sequences. These parametric models were empirically tuned to give a rough description of different color and texture perception phenomena (see the <a href=#psychophysical-test-bed-for-model-tuning-and-comparison>psychophysical test-bed</a> below for model tuning and comparison).</p><p>See a visual example of the effect of the local spatial-frequency transforms and the divisive normalization below (illustration of the 2018 model)</p><h2 id=1995---2008-linear-opponent-color-channels-local-dct-and-divisive-normalization>1995 - 2008: Linear opponent color channels, local-DCT and Divisive Normalization</h2><p>This model is invertible and was originally tuned to reproduce contrast response curves obtained from contrast incremental thresholds [Pons PhD Thesis, 1997]. It was applied to reproduce subjective distortion opinion [<a href=https://www.sciencedirect.com/science/article/abs/pii/S0262885696000042>Im.Vis.Comp.97</a>, <a href=https://www.sciencedirect.com/science/article/abs/pii/S0141938299000098>Displays 99</a>] and to improve the perceptual quality of JPEG and MPEG through (a) transform coding of the achromatic channel [<a href=https://www.uv.es/vista/vistavalencia/papers/ELECT95.PS.gz>Elect.Lett.95</a>, <a href=https://www.uv.es/vista/vistavalencia/papers/ELECT99.PS.gz>Elect.Lett.99</a>, <a href=https://www.uv.es/vista/vistavalencia/papers/ivc99.ps.gz>Im.Vis.Comp.00</a>, <a href=https://www.uv.es/vista/vistavalencia/papers/ieeeoct01.pdf>IEEE TIP 01</a>, <a href=https://www.uv.es/vista/vistavalencia/papers/patt_rec03.pdf>Patt.Recog.03</a>, <a href=https://www.uv.es/vista/vistavalencia/papers/SVM_JND8_ACCEPTED.pdf>IEEE TNN 05</a>, <a href=https://www.uv.es/vista/vistavalencia/papers/manuscript4.pdf>IEEE TIP 06a</a>, <a href=https://www.uv.es/vista/vistavalencia/papers/camps_JMLR_08.pdf>JMLR08</a>], (b) the color channels [<a href=https://www.eurekaselect.com/96168/article>RPSP12</a>], and (c) by improving the motion estimation [<a href=https://www.uv.es/vista/vistavalencia/papers/LNCS97.PS.gz>LNCS97</a>, <a href=https://www.uv.es/vista/vistavalencia/papers/ELECT98.PS.gz>Elect.Lett.98</a>, <a href=https://www.uv.es/vista/vistavalencia/papers/seg_ade2.ps>Elect.Lett.00a</a>, <a href=https://www.uv.es/vista/vistavalencia/papers/elect00.ps>Elect.Lett.00b</a>, <a href=https://www.uv.es/vista/vistavalencia/papers/ieeeoct01.pdf>IEEE TIP 01</a>].</p><ul><li><strong>Download the Toolbox!:</strong> <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/V1_model_DCT_DN_color.zip>V1_model_DCT_DN_color.zip (74MB)</a></li></ul><h2 id=2009---2010-linear-opponent-color-channels-orthogonal-wavelet-and-divisive-normalization>2009 - 2010: Linear opponent color channels, Orthogonal Wavelet and Divisive Normalization</h2><p>Even though we developed our own Matlab code for some specific overcomplete wavelets in the mid 90&rsquo;s [<a href=http://www.uv.es/vista/vistavalencia/papers/tesis/msc_jmalo.zip>MSc Thesis 95</a>, <a href=https://www.uv.es/vista/vistavalencia/papers/JMO97.PS.gz>J.Mod.Opt.97</a>], it took some time until we applied the Divisive Normalization interaction to Simoncelli&rsquo;s wavelets in MatlabPyrTools (which are substantially more efficient). The model was fitted to reproduce subjective image distortion opinion [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/Laparra_JOSA_10.pdf>JOSA A 10</a>] following exhaustive grid search as in [<a href=https://www.uv.es/vista/vistavalencia/papers/icip02.pdf>IEEE ICIP 02</a>]. This model (which relies on the orthogonal wavelets of the MatlabPyrTools) was found to have excellent redundancy reduction properties [<a href=https://link.springer.com/chapter/10.1007/978-3-642-11509-7_3>LNCS10</a>, <a href=https://www.uv.es/vista/vistavalencia/papers/Malo_Laparra_Neural_10b.pdf>Neur.Comp.10</a>].</p><ul><li><strong>Download the Toolbox!:</strong> <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/V1_model_wavelet_DN_color.zip>V1_model_wavelet_DN_color.zip (14MB)</a></li></ul><h2 id=2013---2018-multi-layer-network-with-nonlinear-opponent-color-overcomplete-wavelet-and-divisive-normalization>2013 - 2018: Multi-Layer network with nonlinear opponent color, Overcomplete Wavelet and Divisive Normalization</h2><p>Even though we developed a comprehensive color vision toolbox in the early 2000&rsquo;s (see <a href=./../content>ColorLab</a> ), it took some time until we included a fully adaptive chromatic front-end before the spatial processing models based on overcomplete wavelets. Note that the older toolboxes rely on (too rough) linear RGB to YUV transforms. This multi-layer model (or biologically-plausible deep network) performs the following chain of perceptually meaningful operations [<a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0201326">PLoS 18</a>].</p><p>The parameters of the different layers were fitted in different ways: while the 2nd and 3rd layers (contrast and CSF+masking) were determined using Maximum Differentiation [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/malo15a-reprint.pdf>Malo and Simoncelli SPIE 15</a>], layers 1st and 4th (chromatic front-end and wavelet layer) were fitted to reproduce subjective image distortion data [<a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0201326">PLoS 18</a>], and then fine-tuned to reproduce classical masking [<a href=https://www.frontiersin.org/articles/10.3389/fnins.2019.00008/full>Front. Neurosci. 19</a>].</p><ul><li><strong>Download the Toolbox!:</strong> <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/BioMultiLayer_L_NL_color.zip>BioMultiLayer_L_NL_color.zip (49MB)</a></li></ul><h2 id=2019---2021-convolutional-and-differentiable-implementations>2019 - 2021: Convolutional and differentiable implementations</h2><p>The matrix formulation developed in [<a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0201326">PLoS 18</a>, <a href=https://www.frontiersin.org/articles/10.3389/fnins.2019.00008/full>Front. Neurosci. 19</a>] and implemented in BioMultiLayer_L_NL_color is elegant but not applicable to large images nor appropriate to be included in python deep-learning schemes since it is implemented in Matlab. Recently we worked to solve these issues and confirm the choices of the chromatic part. This led to the deep Percepnet [<a href=https://ieeexplore.ieee.org/document/9190691>IEEE ICIP 20</a>], and to the convolutional version the above MultiLayer L+NL cascade [J.Vision, Proc. VSS 2021]. While Percepnet has the advantage of being implemented in python and hence ready for automatic differentiation (state-of-the-art in image quality), it has the disadvantage of being based on a restricted version of Divisive Normalization (no explicit interactions in space/scale) [<a href="https://openreview.net/forum?id=rJxdQ3jeg">ICLR 17</a>]. On the other hand, the BioMultiLayer_L_NL_color_convolutional has a more general and interpretable version of the Divisive Normalization (in includes full range of interactions in space/scale/orientation). Moreover, the color adaptation choices and the scaling of the achromatic and chromatic channels has been confirmed by positive psychophysical and statistical behaviors [<a href=https://journals.physiology.org/doi/abs/10.1152/jn.00487.2019>J. Neurophysiol.19</a>, <a href=https://mathematical-neuroscience.springeropen.com/articles/10.1186/s13408-020-00095-8>J. Math.Neurosci.20</a>]. However, derivatives are implemented in matlab, so it is not ready to be included in deep-learning schemes right away. There is a lot of room for improvement of its parameters!.</p><ul><li><p><strong>Download the Toolbox!:</strong> <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/BioMultiLayer_L_NL_color_convolutional.zip>BioMultiLayer_L_NL_color_convolutional.zip (76MB)</a></p></li><li><p><strong>Visit Github!:</strong> <a href=https://github.com/alexhepburn/perceptnet>Perceptnet</a></p></li><li><p><strong>Statistical and Psychophysical support for the chromatic choices (I/II):</strong> The small scale model in this paper uses the same chromatic choices as the BioMultiLayer_L_NL models. <a href=./../flow_wilson>Code and Data for small scale recurrent Wilson-Cowan network [J.Neurophysiol. 2020]</a></p></li><li><p><strong>Statistical and Psychophysical support for the chromatic choices (II/II):</strong> The small scale model in this paper uses the same chromatic choices as the BioMultiLayer_L_NL models. <a href=./../spatio_chromatic>Code and Data for small scale Div. Norm. [J.Math.Neurosci. 2020]</a></p></li></ul><h2 id=psychophysical-test-bed-for-model-tuning-and-comparison>Psychophysical test-bed for model tuning and comparison</h2><p>The figure below (computed using <a href=./../vistalab>VISTALAB</a> and <a href=./../content>ColorLab</a>) illustrates distinctive features of early vision: (a) the bandwidth of the achromatic and the chromatic channels is markedly different, (b) the response to contrast is a saturating nonlinearity, its slope (sensitivity) depends on the frequency and the response attenuates as a function of the properties of the background (note how the test is more salient -highlighted in green- on top of a very different background while it is masked -highlighted red- on top of similar backgrounds), and (c) the visibility of i.i.d. noise seen on top of a natural image is not uniform: e.g. visibility is smaller in high contrast regions.</p><p>These quite visible facts can be used to tune the parameters of the mechanistic models considered above. One could play with the parameters by hand until the response curves qualitatively reproduce what one actually sees. We suggested this idea to improve model fit in natural image databases [Front.Neurosci.18] and (for the first time!) here is data and code to perform such tune-it-yourself experiments: <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/experiments_VistaModels.zip>experiments_VistaModels.zip (400MB)</a>.</p><p>File is huge because it contains thousands of tests to compute detailed contrast response curves and distortion measures on the TD database. Moreover, it also has the corresponding responses of the three mechanistic models!.</p><p>Results below suggest that models are equivalent but the most recent displays better behavior (on top of having more plausible receptive fields [<a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0201326">PLoS 18</a>]). More importantly, while the results on Image Quality are way better than the popular Structural Similarity Index SSIM (see VistaQualityTools ) there is still a lot of room for improvement through these tune-it-yourself experiments!.</p><h2 id=model-comparison>Model Comparison</h2><h1 id=b-principled-models>(B) Principled Models</h1><h2 id=efficient-coding-in-mechanistic-models>Efficient coding in mechanistic models</h2><p>We have shown that models including point-wise Weber-like saturation for brightness lead to decreasing signal-to-noise ratio as a function of the luminance [J.Opt.95]. Moreover, taking into account more general cascades of linear+nonlinear layers (e.g. local-frequency transforms and divisive normalization after Weber-brightness) we have seen that the efficiency of such systems (in terms of redundancy reduction) decreases with luminance and contrast, which is consistent with the distribution of natural images in local frequency domains [PLoS 18]. We have seen that the discrimination ability of Local-DCT+Div.Norm. models is bigger in the more populated regions of the frequency-amplitude domain [Im.Vis.Comp.97]. Additionally, we have seen that the mutual information between the coefficients of the image representation progressively reduces from the retina to the normalized representation, both in the local-DCT + DN case [IEEE TIP 06] and in the Orthogonal wavelet+DN case <a href=https://www.uv.es/vista/vistavalencia/papers/Malo_Laparra_Neural_10b.pdf>Neur.Comp.10</a>.</p><p>The above body of results means that the Mechanistic Models considered above display remarkable adaptation to the natural image statistics.</p><p>In the same line, in collaboration with NYU (Balle and Simoncelli) we have optimized the described linear+nonlinear architectures for optimal autoencoding. By including both the linear and the nonlinear parts in the optimization we get unprecedented rate-distortion performances (see paper and code here [<a href=http://www.cns.nyu.edu/~lcv/iclr2017>ICLR 17</a>]), way better than our previous image coders based on V1 models with fixed linear stages (See the <a href=./../../../image_video_processing/vistacore/content>VistaCoRe</a> Toolbox).</p><h2 id=statistically-based-linear-receptive-fields>Statistically-based linear receptive fields</h2><p>Statistical goals such as decorrelation (Principal Component Analysis, PCA), and Independent Component Analysis (ICA) many time lead to sensible linear receptive fields when trained with natural scenes. For instance, spatio-spectral PCA leads to compact representations to disentangle reflectance and spectral illumination from retinal irradiance and lead to spatial-frequency sensors with smooth spectral response [IEEE TGRS 13] (see VistaSpatioSpectral). In collaboration with Helsinki University (Gutman and Hyvarinen) we explored ICA-related techniques. Complex ICA led to local and oriented receptive fields in phase quadrature [LNCS11] (download the <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/CICA_toolbox.zip>Complex ICA</a> Toolbox). Higher Order Canonical Correlation Analysis (HOCCA) combines the sparsity goal with optimal correspondence between identified features in domain adaptation problems leading to biologically plausible spatiochromatic receptive fields which adapt to changes in the illumination (PLoS 14, see the <a href=./../../../feature_extraction/hocca/content>HOCCA</a> Toolbox).</p><p>This analysis of ICA methods concluded with a refutation of a classical result in cortical organization based on Topographica ICA: in fact (as opposed to Hyvarinen & Hoyer Vis. Res. 2001) it does not lead to orientation domains [PLoS 17]. See code and results to analyze <a href=./../../tica/content>TICA</a> receptive fields.</p><h2 id=statistically-based-nonlinearities>Statistically-based nonlinearities</h2><p>Instead of optimizing the mechanistic models for efficient coding we tried a stronger approach to test the Efficient Coding Hypothesis: use pure data-driven techniques instead of assuming models which already have the right functional form. We developed a family of invertible techniques for manifold unfolding and for manifold Gaussianization.</p><p>The unfolding techniques identify nonlinear sensors that follow curved manifolds. These include Sequential Principal Curves Analysis <a href=./../../../feature_extraction/spca/content><strong>SPCA</strong></a> and sequels: Principal Polynomial Analysis <a href=./../../../feature_extraction/ppa/content><strong>PPA</strong></a> and Dimensionality Reduction based on Regression, <a href=./../../../feature_extraction/ddr/content><strong>DDR</strong></a>.</p><p>The Gaussianization technique (Rotation-Based Iterative Gaussianization, <a href=./../../../feature_extraction/rbig/content><strong>RBIG</strong></a>) does not identify sensors but it allows to compute the PDF. Therefore it is useful to define discrimination regions according to information maximization or error minimization. See the kind of predictions made by these unfolding techniques (SPCA [Network 06, NeCo12, Front. Human Neurosci.15, ArXiv 16, https://arxiv.org/pdf/1606.00856.pdf], and PPA-DRR [SPIE13, Int.J.Neur.Syst.14, IEEE Sel.Top.Sig.Proc.15]) and by the Gaussianization technique [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/2013_Courant_features_RBIG.pdf>Talk at LeCun Lab NYU 13</a>, IEEE TNN 11].</p><p>Closely related to optimal discrimination (or optimal metric) for error minimization is the concept of Fisher Information. Our lab has a tradition in the study of Riemannian metrics induced by nonlinear perception systems [J. Malo PhD 99, Displ.99]. Over the years, the ideas about the geometrical transforms induced by the system and their effect on information processing have evolved from distance computation to the consideration of the transformation of neural noise [Displ.99, Patt.Recog.03, IEEE TIP 06, JOSA A 10, SPIE 15, NIPS 17, PLoS 18].</p><h1 id=c-engineering-motivated>(C) Engineering-motivated</h1><h2 id=perceptually-weighted-motion-estimation-vistavideocoding>Perceptually-weighted motion estimation: VistaVideoCoding</h2><p>What can be predicted is not worth transmitting!. This simple idea is the core of predictive coding used in most successful video coders (e.g. MPEG). In predictive coding motion information is the key to predict future-from-past. MPEG-like coders first compute the optical flow (or displacement field) and encode the prediction error in a transformed domain which (not surprisingly!) is similar to the <a href=#a-empirical-mechanistic-models>V1 mechanistic models</a> described above.</p><p>In this video-coding context we improved motion estimation by connecting the optical flow computation with the perceptual relevance of the prediction error: we proposed to improve the resolution of the motion estimate only if the prediction error was hard to encode for our improved V1 models [LNCS97, Electr.Lett.98, J.Vis.01]. This gave rise to smoother motion flows more appropriate for motion-based segmentation [Electr.Lett.00a], and to better video coders [Electr.Lett.00b, IEEE TIP 01].</p><ul><li>Download the motion estimation and video coding toolbox! <a href=./../../../image_video_processing/videocodingtools/content>VistaVideoCoding</a>.</li></ul><h2 id=image-coding-vistacore>Image Coding: VistaCoRe</h2><p>Image compression requires vision models that rank visual features according to their perceptual relevance so that extra bits can be allocated to encode the subjectively important aspects of the image.</p><p>The vision model based on DCT and Divisive Normalization considered above leads to better decoded images at the same compression ratio than JPEG and variants based on simpler models of masking.</p><p>See the <a href=./../../../image_video_processing/vistacore/content>VistaCoRe</a> (Coding and Restoration Toolbox), and the references [Eletr.Lett95, Eletr.Lett99, Im.Vis.Comp.00 Patt.Recog.03, IEEE TNN 05, IEEE TIP 06a, IEEE TIP 06b, JMLR08].</p><h2 id=image-and-video-quality-vistaqualitytools>Image and Video Quality: VistaQualityTools</h2><p>Computing perceptual distances between images requires vision models that identify relevant and negligible visual features. Distortions in features that will be neglected by the observers should induce no effect in the distance. And the other way around for visually relevant features. The different models can be quantitatively compared by their accuracy in reproducing the opinion of viewers in subjectively rated databases.</p><p>The three vision models considered above (based on DCTs, orthonormal wavelets, and overcomplete wavelets) have been used to propose distortion metrics that overperform SSIM. See <a href=./../../../image_video_processing/vistaqualitytools/content>VistaQualityTools</a>, and the references [Im.Vis.Comp.97, Displays99, Patt.Recog.03, IEEE Trans.Im.Proc.06] for the DCT metric, [JOSA 10, Neur.Comp.10] for the orthogonal wavelet metric, and [PLoS 18, Frontiers Neurosci.18] for the metric based on overcomplete wavelets.</p></p></div></div><div class=grid-item><a href><img src=/try-isp-page/images/code alt="Visual Information Flow in Wilson Cowan Networks. Gómez-Villa et al. Journal of Neurophysiology 2019."></a><div class=text><a href class=nameLink_a>Visual Information Flow in Wilson Cowan Networks. Gómez-Villa et al. Journal of Neurophysiology 2019.</a><p>The Wilson-Cowan interaction of wavelet-like visual neurons is analyzed in total correlation terms for the first time. Theoretical and empirical results show that a psychophysically-tuned interaction achieves the biggest efficiency in the most frequent region of the image space. This an original confirmation of the Efficient Coding Hypothesis and suggests that neural field models can be an alternative to Divisive Normalization in image compression.</p></div></div></div><p>COLORLAB is a color computation and visualization toolbox to be used in the MATLAB environment. It is intended for colorimetric applications, such as color image processing and psychophysical experimentation. The toolbox includes classical Tristimulus representations (e.g. CIE XYZ 1931), current Color Appearance Models (CIE Lab 76 or CIECAM2000), and various tools for visualization and color processing.</p></div></div></main></body><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js integrity=sha384-C6RzsynM9kWDrMNeT87bh95OGNyZPhcTNXj1NW7RuBCsyN/o0jlpcV8Qyq46cDfL crossorigin=anonymous defer></script></html>