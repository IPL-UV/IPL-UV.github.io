<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>ISP - Image and Signal Processing group</title>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css integrity=sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH crossorigin=anonymous onerror='this.onerror=null,this.href="css/bootstrap.min.css"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer onerror='this.onerror=null,this.href="css/all.min.css"'><link rel="shortcut icon" href=/images/isp_ico.webp type=image/x-icon><link id=kenmore-font rel=stylesheet href=https://fonts.cdnfonts.com/css/twentieth-century-for-kenmore onerror='this.onerror=null,this.href="css/fonts/TwentiethCentury/fonts.css"'><link rel=stylesheet href=/css/style.css><script src=https://cdn.jsdelivr.net/npm/marked/marked.min.js onerror='this.onerror=null,this.src="js/marked.min.js"'></script><script src=/js/mode.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-D8CVQKS51G"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-D8CVQKS51G")</script></head><body><nav class=custom-navbars><div class=custom-container><a href=/ class="custom-logo custom-hide-on-large"><img src=/images/isp_logo_sinfondo.webp alt="ISP Icon" class=custom-logo_nav>
<span class=custom-text-isp>ISP</span>
</a><button class=navbar-toggler aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class=custom-navbar-collapse><ul class=custom-navbar-nav><li class="custom-nav-item custom-desktop-only"><a class=custom-nav-link href=/>ISP</a></li><li class=custom-nav-item><a class=custom-nav-link href=/people/>People</a></li><li class="custom-nav-item custom-dropdown"><a class="custom-nav-link custom-dropdown-toggle" href=/research/philosophy/>Research</a><ul class=custom-dropdown-menu><li><a class=custom-dropdown-item href=/research/philosophy/>Philosophy</a></li><li><a class=custom-dropdown-item href=/research/machine_learning/>Machine learning</a></li><li><a class=custom-dropdown-item href=/research/visual_neuroscience/>Visual science</a></li><li><a class=custom-dropdown-item href=/research/visual_brain/>Image processing</a></li><li><a class=custom-dropdown-item href=/research/earth_science/>Earth science</a></li><li><a class=custom-dropdown-item href=/research/social_science>Social science</a></li></ul></li><li class=custom-nav-item><a class=custom-nav-link href=/projects/>Projects</a></li><li class=custom-nav-item><a class=custom-nav-link href=/facilities/>Facilities</a></li><li class="custom-nav-item custom-dropdown"><a class="custom-nav-link custom-dropdown-toggle" href=/publications/journals/>Publications</a><ul class=custom-dropdown-menu><li><a class=custom-dropdown-item href=/publications/journals/>Journals</a></li><li><a class=custom-dropdown-item href=/publications/conferences/>Conferences</a></li><li><a class=custom-dropdown-item href=/publications/books/>Books</a></li><li><a class=custom-dropdown-item href=/publications/talks/>Talks</a></li><li><a class=custom-dropdown-item href=/publications/theses/>Theses</a></li></ul></li><li class=custom-nav-item><a class=custom-nav-link href=/code/>Code</a></li><li class=custom-nav-item><a class=custom-nav-link href=/data/>Data</a></li><li class=custom-nav-item><a class=custom-nav-link href=/seminars/>Seminars</a></li><li class=custom-nav-item><a class=custom-nav-link href=/courses/>Courses</a></li><li class=custom-nav-item><a class=custom-nav-link href=/collaborators/>Collaborators</a></li><li class=custom-nav-item><a class=custom-nav-link href=/news/>News</a></li><li class=custom-nav-item><a class=custom-nav-link href=/contact/>Contact</a></li></ul></div></div></nav><main><div class=container><div class="content-container grid-layout"><div class=box-header><h1>VistaLab: The Matlab Toolbox for Linear Spatio-Temporal Vision Models</h1></div><div class=box-abstract><p><h1 id=the-matlab-toolbox-for-linear-spatio-temporal-vision-models>The Matlab toolbox for linear spatio-temporal Vision Models</h1><p><strong>VistaLab</strong> is a Matlab toolbox that provides the linear building-blocks to create spatio-temporal vision models and the tools to control the spatio-temporal properties of video sequences. These building blocks include the spatio-temporal receptive fields of LGN, V1, and MT cells, and the spatial and spatio-temporal Contrast Sensitivity Functions (CSFs). Additionally, <strong>VistaLab</strong> allows accurate spatio-temporal sampling, spatio-temporal Fourier domain visualization, and generation of video sequences with controlled texture and speed. Tools for video sequence generation include noise, random dots, and rigid-body animations with Lambertian reflectance.</p><p>The perception and video synthesis tools enable accurate illustrations of the visibility of achromatic spatio-temporal patterns. Linear filters in <strong>VistaLab</strong> provide rough approximations of pattern visibility, which can be enhanced with non-linear models available in related toolboxes.</p><p>The <strong>standard tools in VistaLab</strong> (and <a href=./../content><strong>ColorLab</strong></a>) are essential for building more sophisticated vision models, available on the <a href=./../vistamodels><strong>VistaModels</strong></a> dedicated site.</p><h1 id=table-of-contents>Table of Contents</h1><ul><li><a href=#retina-and-lateral-geniculate-nucleus-lgn>Retina and Lateral Geniculate Nucleus (LGN)</a></li><li><a href=#primary-visual-cortex-v1>Primary Visual Cortex (V1)</a></li><li><a href=#middle-temporal-mt-region>Middle Temporal (MT) region</a></li><li><a href=#spatio-temporal-contrast-sensitivities>Spatio-temporal Contrast Sensitivities</a></li><li><a href=#controlled-spatio-temporal-stimuli>Controlled spatio-temporal stimuli</a></li><li><a href=#extensions-of-vistalab>Extensions of VistaLab</a></li></ul><h1 id=retina-and-lateral-geniculate-nucleus-lgn>Retina and Lateral Geniculate Nucleus (LGN)</h1><p>Most of the Retinal Ganglion Cells and cells in the LGN can be modelled with center-surround receptive fields with monophasic or biphasic temporal response. VistaLab comes with a configurable implemenation of such receptive fields according to the general expressions in [Cai, Freeman, DeAngelis, J. Neurophysiol. 97]. Using these units it is easy to generate artificial retinas with arbitrary sampling [Martinez-Garcia et al. 16, Martinez-Garcia et al. 17].</p><p>The examples below show (a) the receptive field of a representative neuron in the spatiotemporal and in the 3D Fourier domain, and (b) the response of a population of such neurons to a natural movie assuming uniform retinal sampling and spatial invariance of the receptive field. VistaLab allows explicit implementation of each sensor response using the scalar product by the corresponding receptive field to get rid of the uniform sampling and the convolution assumptions.</p><h1 id=primary-visual-cortex-v1>Primary Visual Cortex (V1)</h1><p>Simple cells in the V1 cortex can be modelled with Gabor-like receptive fields tuned to certain spatial and temporal frequencies. VistaLab comes with a configurable implemenation of such receptive fields according to the general expressions in [Daugman JOSA A 89, Simoncelli & Heeger Vis. Res. 98]. Using these units it is easy to generate artificial cortex with arbitrary sampling [Martinez-Garcia et al. 17].</p><p>The examples below show six representative neurons tuned to the same spatial frequencies (7 cpd) but different temporal frequencies 2, 7, and 10 Hz, both positive and negative. Eventhough there is no conclusive tuning to two-dimensional speed due to the aperture problem [Heeger JOSA 87], in the direction perpendicular to the grating, these are tuned to 0.3, 1 and 1.5 degrees/sec respectively (both positive and negative). Figures show: (a) the receptive fields in the spatiotemporal and in the 3D Fourier domain, and (b) the response of a population of such neurons to a natural movie assuming uniform retinal sampling and spatial invariance of the receptive field. VistaLab allows explicit implementation of each sensor response using the scalar product by the corresponding receptive field to get rid of the uniform sampling and the convolution assumptions.</p><h1 id=middle-temporal-mt-region>Middle Temporal (MT) region</h1><p>Cells in the MT cortex receive projections from V1 cells aligned in a plane in the spatio-temporal Fourier domain. Therefore, they are narrow-band in speed tuning. VistaLab comes with a configurable implemenation of such receptive fields according to the general expressions in [Simoncelli & Heeger Vis. Res. 98]. Using these units and a spatio-temporal window it is easy to generate artificial MT cortex with arbitrary sampling [Martinez-Garcia et al. 17].</p><p>The examples below shows six representative sets of neurons tuned to tuned to speeds of 0.3, 1 and 1.5 degrees/sec respectively (both positive and negative). In this case while Figures show: (a) the receptive fields in 3D Fourier domain, the kind of features these cells are optimally tuned to, and (b) the response of a population of such neurons to a natural movie assuming uniform retinal sampling and spatial invariance of the receptive field. VistaLab allows explicit implementation of each sensor response using the scalar product by the corresponding receptive field to get rid of the uniform sampling and the convolution assumptions.</p><h1 id=spatio-temporal-contrast-sensitivities>Spatio-temporal Contrast Sensitivities</h1><p>VistaLab comes with different Contrast Sensitivity Functions (CSFs): (a) the spatial-achromatic CSF from the OSA Standard Spatial Observer <a href=http://www.uv.es/vista/vistavalencia/papers/icip02.pdf>Watson & Malo IEEE ICIP 02</a>, (b) the spatial-chromatic, Red-Green and Yellow-Blue CSFs of K. Mullen [Vis. Res. 85], with approrpiate scaling <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/2012b_Gutierrez_RPTSP_12c.PDF>Gutierrez et al. 12</a>, and (c) the achromatic spatio-temporal CSFs of D. Kelly [JOSA 79], and S. Daly (with object tracking speed compensation) [SPIE 98].</p><h1 id=controlled-spatio-temporal-stimuli>Controlled spatio-temporal stimuli</h1><p>The movies below illustrate the abilities of VistaLab for accurate motion control.</p><ul><li><p><strong>First row:</strong> includes sequences of the motion of a lambertian rigid body evolving in a gravitatory field with inelastic restrictions recorded from different points of view, this example allows arbitrary locations of the illumination and camera. In this case the actual motion in 3D world and the optical flow (motion in the retinal plane) are known.</p></li><li><p><strong>Second row:</strong> includes an example of random dots moving according to arbitrary optical flow fields.</p></li><li><p><strong>Third row:</strong> shows how static pictures can be animated using spatially uniform flows of arbitrary speed leading to interesting shape-from-motion effects in the case of noise patterns.</p></li><li><p><strong>Fourth row:</strong> shows different movies of the same periodic pattern moving at progressively increasing speeds. Aliasing introduces speed reversal at the expected place, as demonstrated by the Fourier diagrams below.</p></li></ul><h1 id=extensions-of-vistalab>Extensions of VistaLab</h1><p>VistaLab only addresses the linear part of the neural mechanisms that mediate the preattentive perception of spatio-temporal patterns. However, it doesnt combine these mechanisms to compute motion (optical flow), it doesnt include the nonlinear interactions between the linear mechanisms, and it doesnt include color.</p><p>These issues can be addressed with other toolboxes, namely <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/VistaVideoCoding.zip>VistaVideoCoding</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/BioMultiLayer_L_NL_color.zip>BioMultiLayer_L_NL_color</a> in VistaModels, and <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/Colorlab.zip>Colorlab</a>.</p><h1 id=key-capabilities>Key Capabilities</h1><ul><li><strong>Spatio-temporal Modeling</strong>: Build models for LGN, V1, and MT neural responses.</li><li><strong>Contrast Sensitivity</strong>: Apply achromatic and chromatic CSFs to video and images.</li><li><strong>Video Synthesis</strong>: Create controlled video sequences with specific spatio-temporal properties.</li><li><strong>Fourier Domain Tools</strong>: Visualize spatio-temporal frequency response of neural models.</li></ul></p></div><aside class=box-gallery><div class=gallery-grid><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/code/noise.gif","[Noise texture for optical flow testing](#the-matlab-toolbox-for-linear-spatio-temporal-vision-models)","Accurate control of texture and speed was crucial both to (a) test our first perceptually weighted optical flow algorithms [Electr. Lett. 00, IEEE Trans.Im.Proc. 01](https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/seg_ade2.ps), and (b) generate stimuli for controlled motion aftereffects [Front. Human Neurosci. 15](https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/LaparraMalo15.pdf).")'><img src=/images/code/noise.gif alt="[Noise texture for optical flow testing](#the-matlab-toolbox-for-linear-spatio-temporal-vision-models)"></a><p class=gallery-title><a href=#the-matlab-toolbox-for-linear-spatio-temporal-vision-models>Noise texture for optical flow testing</a></p><div class=gallery-description><p>Accurate control of texture and speed was crucial both to (a) test our first perceptually weighted optical flow algorithms <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/seg_ade2.ps>Electr. Lett. 00, IEEE Trans.Im.Proc. 01</a>, and (b) generate stimuli for controlled motion aftereffects <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/LaparraMalo15.pdf>Front. Human Neurosci. 15</a>.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/code/TF_noise.gif","[3D Fourier transform of noise sequence](#the-matlab-toolbox-for-linear-spatio-temporal-vision-models)","Two views of the 3D Fourier transform of the sequence at the left (the ft=0 plane is highlighted in blue). Note how the modification of texture and speed in the sequence implies different energy distributions in the Fourier domain. Accurate control of texture and speed of movies allows to saturate certain sensors and induce stronger aftereffcts. This is because the description of motion aftereffects [Front. Human Neurosci. 15](https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/LaparraMalo15.pdf) requires models of V1 sensors in the spatio-temporal Fourier domain with frequency-dependent cross-inhibition.")'><img src=/images/code/TF_noise.gif alt="[3D Fourier transform of noise sequence](#the-matlab-toolbox-for-linear-spatio-temporal-vision-models)"></a><p class=gallery-title><a href=#the-matlab-toolbox-for-linear-spatio-temporal-vision-models>3D Fourier transform of noise sequence</a></p><div class=gallery-description><p>Two views of the 3D Fourier transform of the sequence at the left (the ft=0 plane is highlighted in blue). Note how the modification of texture and speed in the sequence implies different energy distributions in the Fourier domain. Accurate control of texture and speed of movies allows to saturate certain sensors and induce stronger aftereffcts. This is because the description of motion aftereffects <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/LaparraMalo15.pdf>Front. Human Neurosci. 15</a> requires models of V1 sensors in the spatio-temporal Fourier domain with frequency-dependent cross-inhibition.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/code/RF_LGN.gif","[Pulsating on-center LGN neuron](#retina-and-lateral-geniculate-nucleus-lgn)","Receptive field (or impulse response) in the spatiotemporal domain, where white stands for excitation and black stands for inhibition.")'><img src=/images/code/RF_LGN.gif alt="[Pulsating on-center LGN neuron](#retina-and-lateral-geniculate-nucleus-lgn)"></a><p class=gallery-title><a href=#retina-and-lateral-geniculate-nucleus-lgn>Pulsating on-center LGN neuron</a></p><div class=gallery-description><p>Receptive field (or impulse response) in the spatiotemporal domain, where white stands for excitation and black stands for inhibition.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/code/RF_LGN2.gif","[Temporal variation of spatial response](#retina-and-lateral-geniculate-nucleus-lgn)","Center-surround excitation at the stimulus onset is followed by reversed sign response when stimulation vanishes.")'><img src=/images/code/RF_LGN2.gif alt="[Temporal variation of spatial response](#retina-and-lateral-geniculate-nucleus-lgn)"></a><p class=gallery-title><a href=#retina-and-lateral-geniculate-nucleus-lgn>Temporal variation of spatial response</a></p><div class=gallery-description><p>Center-surround excitation at the stimulus onset is followed by reversed sign response when stimulation vanishes.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/code/sens_LGN.webp","[Frequency selectivity in LGN neuron](#retina-and-lateral-geniculate-nucleus-lgn)","Band pass of this kind of neurons in the spatio-temporal Fourier domain. This is the Fourier transform of the receptive field at the left.")'><img src=/images/code/sens_LGN.webp alt="[Frequency selectivity in LGN neuron](#retina-and-lateral-geniculate-nucleus-lgn)"></a><p class=gallery-title><a href=#retina-and-lateral-geniculate-nucleus-lgn>Frequency selectivity in LGN neuron</a></p><div class=gallery-description><p>Band pass of this kind of neurons in the spatio-temporal Fourier domain. This is the Fourier transform of the receptive field at the left.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/code/response_LGN.gif","[Response of some LGN population to a natural stimulus](#retina-and-lateral-geniculate-nucleus-lgn)","Assuming a spatially invariant population of LGN cells like the one depicted above uniformly covering the visual field, we can compute the response to a natural movie using plain convolution or product in the Fourier domain.")'><img src=/images/code/response_LGN.gif alt="[Response of some LGN population to a natural stimulus](#retina-and-lateral-geniculate-nucleus-lgn)"></a><p class=gallery-title><a href=#retina-and-lateral-geniculate-nucleus-lgn>Response of some LGN population to a natural stimulus</a></p><div class=gallery-description><p>Assuming a spatially invariant population of LGN cells like the one depicted above uniformly covering the visual field, we can compute the response to a natural movie using plain convolution or product in the Fourier domain.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/code/spect1.webp","[Spectrum of the original sequence](#retina-and-lateral-geniculate-nucleus-lgn)","Each branch of the X represents an object moving with constant speed (i.e. comes from one of the waving hands)")'><img src=/images/code/spect1.webp alt="[Spectrum of the original sequence](#retina-and-lateral-geniculate-nucleus-lgn)"></a><p class=gallery-title><a href=#retina-and-lateral-geniculate-nucleus-lgn>Spectrum of the original sequence</a></p><div class=gallery-description><p>Each branch of the X represents an object moving with constant speed (i.e. comes from one of the waving hands)</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/code/spect3.webp","[Spectrum of the response](#retina-and-lateral-geniculate-nucleus-lgn)","Response was computed in the Fourier domain as the aplication of the band-pass filter function of this specific LGN population to the original spectrum.")'><img src=/images/code/spect3.webp alt="[Spectrum of the response](#retina-and-lateral-geniculate-nucleus-lgn)"></a><p class=gallery-title><a href=#retina-and-lateral-geniculate-nucleus-lgn>Spectrum of the response</a></p><div class=gallery-description><p>Response was computed in the Fourier domain as the aplication of the band-pass filter function of this specific LGN population to the original spectrum.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/code/RF_V1.gif","[Pulsating V1 neurons](#primary-visual-cortex-v1)"," Receptive fields (or impulse response) in the spatiotemporal domain, where white stands for excitation and black stands for inhibition. In this figure there are 2*3 replications of a 1 degree visual field. Each replication shows the receptive field of a neuron tuned to the (same) central location but different spatio-temporal frequency.")'><img src=/images/code/RF_V1.gif alt="[Pulsating V1 neurons](#primary-visual-cortex-v1)"></a><p class=gallery-title><a href=#primary-visual-cortex-v1>Pulsating V1 neurons</a></p><div class=gallery-description><p>Receptive fields (or impulse response) in the spatiotemporal domain, where white stands for excitation and black stands for inhibition. In this figure there are 2*3 replications of a 1 degree visual field. Each replication shows the receptive field of a neuron tuned to the (same) central location but different spatio-temporal frequency.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/code/RF_V12.gif","[Temporal variation of spatial responses](#primary-visual-cortex-v1)"," In a surface of sensors of the corresponding kind, Gabor-like excitation/inhibition at the stimulus onset turns into travelling a wave that vanishes afterwards.")'><img src=/images/code/RF_V12.gif alt="[Temporal variation of spatial responses](#primary-visual-cortex-v1)"></a><p class=gallery-title><a href=#primary-visual-cortex-v1>Temporal variation of spatial responses</a></p><div class=gallery-description><p>In a surface of sensors of the corresponding kind, Gabor-like excitation/inhibition at the stimulus onset turns into travelling a wave that vanishes afterwards.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/code/sens_V1.webp","[Frequency selectivity in V1 neurons](#primary-visual-cortex-v1)","Band pass of the six considered neurons. This is the Fourier transform of the receptive fields shown above.")'><img src=/images/code/sens_V1.webp alt="[Frequency selectivity in V1 neurons](#primary-visual-cortex-v1)"></a><p class=gallery-title><a href=#primary-visual-cortex-v1>Frequency selectivity in V1 neurons</a></p><div class=gallery-description><p>Band pass of the six considered neurons. This is the Fourier transform of the receptive fields shown above.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/code/response_V1.gif","[Response of the six V1 populations to a natural stimulus](#primary-visual-cortex-v1)","Assuming a spatially invariant populations of V1 cells like the ones depicted above uniformly covering the visual field, we can compute their responses to a natural movie using plain convolution or product in the Fourier domain. In the figure we show the original stimulus and the six corresponding sets of responses. Note how the different populations respond to qualitatively different features of the stimulus (hand going up/down, static objects of the right frequency), and note how the cells tuned to too high speed do not respond.")'><img src=/images/code/response_V1.gif alt="[Response of the six V1 populations to a natural stimulus](#primary-visual-cortex-v1)"></a><p class=gallery-title><a href=#primary-visual-cortex-v1>Response of the six V1 populations to a natural stimulus</a></p><div class=gallery-description><p>Assuming a spatially invariant populations of V1 cells like the ones depicted above uniformly covering the visual field, we can compute their responses to a natural movie using plain convolution or product in the Fourier domain. In the figure we show the original stimulus and the six corresponding sets of responses. Note how the different populations respond to qualitatively different features of the stimulus (hand going up/down, static objects of the right frequency), and note how the cells tuned to too high speed do not respond.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/code/sens_MT.webp","[Frequency selectivity in MT neurons](#middle-temporal-mt-region)","Band pass of the six considered kinds of neurons. These are the sum of narrow-band V1-like filters shown above. Only filters aligned according to well-defined speed planes are combined in each MT cell.")'><img src=/images/code/sens_MT.webp alt="[Frequency selectivity in MT neurons](#middle-temporal-mt-region)"></a><p class=gallery-title><a href=#middle-temporal-mt-region>Frequency selectivity in MT neurons</a></p><div class=gallery-description><p>Band pass of the six considered kinds of neurons. These are the sum of narrow-band V1-like filters shown above. Only filters aligned according to well-defined speed planes are combined in each MT cell.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/code/RF_MT.gif","[Optimal patterns for MT neurons](#middle-temporal-mt-region)","These patterns were computed by injecting noise only in the band where the different cells are sensitive. Each replication shows the corresponding pattern (showing no specific spatial frequency content) but a markedly different speed.")'><img src=/images/code/RF_MT.gif alt="[Optimal patterns for MT neurons](#middle-temporal-mt-region)"></a><p class=gallery-title><a href=#middle-temporal-mt-region>Optimal patterns for MT neurons</a></p><div class=gallery-description><p>These patterns were computed by injecting noise only in the band where the different cells are sensitive. Each replication shows the corresponding pattern (showing no specific spatial frequency content) but a markedly different speed.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/code/RF_MT2.gif","[Temporal variation of MT neuron responses](#middle-temporal-mt-region)","In surfaces of sensors of the considered kinds, the patterns at the left lead to noisy travelling waves.")'><img src=/images/code/RF_MT2.gif alt="[Temporal variation of MT neuron responses](#middle-temporal-mt-region)"></a><p class=gallery-title><a href=#middle-temporal-mt-region>Temporal variation of MT neuron responses</a></p><div class=gallery-description><p>In surfaces of sensors of the considered kinds, the patterns at the left lead to noisy travelling waves.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/code/response_MT.gif","[Response of the six MT populations to a natural stimulus](#middle-temporal-mt-region)","Assuming a spatially invariant populations of MT cells like the ones depicted above uniformly covering the visual field, we can compute their responses to a natural movie using plain convolution or product in the Fourier domain. In the figure we show the original stimulus and the six corresponding sets of responses (where white means excitation, black means inhibition, and gray means spontaneous/basal response). Note how the different populations respond to qualitatively different features of the stimulus (hand going up/down, static objects of the right frequency), and note how the cells tuned to too high speed do not respond.")'><img src=/images/code/response_MT.gif alt="[Response of the six MT populations to a natural stimulus](#middle-temporal-mt-region)"></a><p class=gallery-title><a href=#middle-temporal-mt-region>Response of the six MT populations to a natural stimulus</a></p><div class=gallery-description><p>Assuming a spatially invariant populations of MT cells like the ones depicted above uniformly covering the visual field, we can compute their responses to a natural movie using plain convolution or product in the Fourier domain. In the figure we show the original stimulus and the six corresponding sets of responses (where white means excitation, black means inhibition, and gray means spontaneous/basal response). Note how the different populations respond to qualitatively different features of the stimulus (hand going up/down, static objects of the right frequency), and note how the cells tuned to too high speed do not respond.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/code/csfs.webp","[Spatial-only Achromatic and Chromatic CSFs](#spatio-temporal-contrast-sensitivities)","The achromatic CSF of the Standard Spatial Observer includes the oblique effect. The chromatic CSFs are assumed to be isotropic.")'><img src=/images/code/csfs.webp alt="[Spatial-only Achromatic and Chromatic CSFs](#spatio-temporal-contrast-sensitivities)"></a><p class=gallery-title><a href=#spatio-temporal-contrast-sensitivities>Spatial-only Achromatic and Chromatic CSFs</a></p><div class=gallery-description><p>The achromatic CSF of the Standard Spatial Observer includes the oblique effect. The chromatic CSFs are assumed to be isotropic.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/code/applyCSF.webp","[Spatial-only CSFs on a natural image](#spatio-temporal-contrast-sensitivities)","VistaLab applied together with COLORLAB allows accurate application of the spatial CSFs. In this example the natural image was first represented in CIE XYZ values. Then, it was transformed to a sensible linear Y, RG, YB representation [Ingling&Tsou, Vis. Res. 79], and then, each CSF was applied to the corresponding chromatic channel. The result was expressed in digital values to be shown in regular diaplays.")'><img src=/images/code/applyCSF.webp alt="[Spatial-only CSFs on a natural image](#spatio-temporal-contrast-sensitivities)"></a><p class=gallery-title><a href=#spatio-temporal-contrast-sensitivities>Spatial-only CSFs on a natural image</a></p><div class=gallery-description><p>VistaLab applied together with COLORLAB allows accurate application of the spatial CSFs. In this example the natural image was first represented in CIE XYZ values. Then, it was transformed to a sensible linear Y, RG, YB representation [Ingling&amp;Tsou, Vis. Res. 79], and then, each CSF was applied to the corresponding chromatic channel. The result was expressed in digital values to be shown in regular diaplays.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/code/csf_st.webp","[Spatio-temporal CSF (with saccade compensation)](#spatio-temporal-contrast-sensitivities)","In the motion-compensated case sensitivity to stationary patterns decreases a lot (note the reduction of the CSF at ft=0). Below you can see three different representations of this function.")'><img src=/images/code/csf_st.webp alt="[Spatio-temporal CSF (with saccade compensation)](#spatio-temporal-contrast-sensitivities)"></a><p class=gallery-title><a href=#spatio-temporal-contrast-sensitivities>Spatio-temporal CSF (with saccade compensation)</a></p><div class=gallery-description><p>In the motion-compensated case sensitivity to stationary patterns decreases a lot (note the reduction of the CSF at ft=0). Below you can see three different representations of this function.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/code/response_CSF.gif","[Natural movie filtered by spatio-temporal CSF](#spatio-temporal-contrast-sensitivities)","")'><img src=/images/code/response_CSF.gif alt="[Natural movie filtered by spatio-temporal CSF](#spatio-temporal-contrast-sensitivities)"></a><p class=gallery-title><a href=#spatio-temporal-contrast-sensitivities>Natural movie filtered by spatio-temporal CSF</a></p></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/code/gema1.gif","[Speed reversal in the Fourier domain](#controlled-spatio-temporal-stimuli)","Illustration of speed reversal due to aliasing, analyzed through the Fourier domain representation of periodic patterns at high speeds.")'><img src=/images/code/gema1.gif alt="[Speed reversal in the Fourier domain](#controlled-spatio-temporal-stimuli)"></a><p class=gallery-title><a href=#controlled-spatio-temporal-stimuli>Speed reversal in the Fourier domain</a></p><div class=gallery-description><p>Illustration of speed reversal due to aliasing, analyzed through the Fourier domain representation of periodic patterns at high speeds.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/code/aliasing.webp","[Speed reversal in the Fourier domain](#controlled-spatio-temporal-stimuli)","In the periodic pattern sequences all the sinusoidal components were computed to travel to the right with the same speed. However, in the last (high speed) case, the high frequency pattern seems to go backwards. Aliasing suffered by this component in this case is easy to understand in the Fourier domain. According to the optical flow equation in the Fourier domain [Watson & Ahumada, JOSA A 85], when the speed increases (in our case from left to right) so it does the inclination of all the sinusoidal components of the sequence in the fx, ft plane. For low speeds all the components are aligned. However, when the speed is too high, the high frequency components have temporal frequencies over the Nyquist limit. This implies an apparent inclination for those frequencies which is totally oposed to the rest of the components, and hence, speed reversal. NOTE: the spatial extent of the discrete domain is 2 deg, and the frame rate is 20 Hz, the spatial frequency of the high frequency component is 10 cpd, and the actual speed in the last case should be 1.5 deg/sec.")'><img src=/images/code/aliasing.webp alt="[Speed reversal in the Fourier domain](#controlled-spatio-temporal-stimuli)"></a><p class=gallery-title><a href=#controlled-spatio-temporal-stimuli>Speed reversal in the Fourier domain</a></p><div class=gallery-description><p>In the periodic pattern sequences all the sinusoidal components were computed to travel to the right with the same speed. However, in the last (high speed) case, the high frequency pattern seems to go backwards. Aliasing suffered by this component in this case is easy to understand in the Fourier domain. According to the optical flow equation in the Fourier domain [Watson & Ahumada, JOSA A 85], when the speed increases (in our case from left to right) so it does the inclination of all the sinusoidal components of the sequence in the fx, ft plane. For low speeds all the components are aligned. However, when the speed is too high, the high frequency components have temporal frequencies over the Nyquist limit. This implies an apparent inclination for those frequencies which is totally oposed to the rest of the components, and hence, speed reversal. NOTE: the spatial extent of the discrete domain is 2 deg, and the frame rate is 20 Hz, the spatial frequency of the high frequency component is 10 cpd, and the actual speed in the last case should be 1.5 deg/sec.</p></div></div><div id=imageModal class=modal><span class=modal-close onclick=closeModal()>&#215;</span>
<img class=modal-content id=modalImage><div id=modalCaption class=modal-caption></div></div></div></aside><section class="box-references title2"><h2>References</h2><ul class=references-list><li><strong><a href=# target=_blank class=references-name>VistaLab: The Matlab Toolbox for Spatio-Temporal Vision. Univ. Valencia 1997</a></strong><br><span>J. Malo & J. Gutierrez</span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/ELECT98.PS.gz target=_blank class=references-name>Perceptually Weighted Optical Flow for Motion-based Segmentation in MPEG-4</a></strong><br><span>J. Malo, et al.</span><br><em>Electronics Letters 36(20): 1693-1694 (2000)</em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/LaparraMalo15.pdf target=_blank class=references-name>Visual Aftereffects and Sensory Nonlinearities from a Single Statistical Framework</a></strong><br><span>V. Laparra & J. Malo</span><br><em>Frontiers in Human Neuroscience 9:557 (2015)</em></li></ul></section><section class="box-enlaces title2"><h2>Download</h2><ul class=enlaces-list><li><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/Vistalab.zip>Matlab Toolbox (version 4.0)</a></li><li><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/VistaVideoCoding.zip>Extensions of VistaLab I: VistaVideoCoding</a></li><li><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/BioMultiLayer_L_NL_color.zip>Extensions of VistaLab II: BioMultiLayer_L_NL_color</a></li><li><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/Colorlab.zip>Extensions of VistaLab III: ColorLab</a></li></ul></section></div></div></main><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js integrity=sha384-C6RzsynM9kWDrMNeT87bh95OGNyZPhcTNXj1NW7RuBCsyN/o0jlpcV8Qyq46cDfL crossorigin=anonymous defer></script></body></html>