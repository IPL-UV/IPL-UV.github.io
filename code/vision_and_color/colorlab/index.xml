<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>COLORLAB: Visual Statistics Coding and Restoration Toolbox on ISP - Image and Signal Processing group</title><link>https://juliocontrerash.github.io/try-isp-page/code/vision_and_color/colorlab/</link><description>Recent content in COLORLAB: Visual Statistics Coding and Restoration Toolbox on ISP - Image and Signal Processing group</description><generator>Hugo</generator><language>en-us</language><atom:link href="https://juliocontrerash.github.io/try-isp-page/code/vision_and_color/colorlab/index.xml" rel="self" type="application/rss+xml"/><item><title>ColorLab: The Matlab Toolbox for Colorimetry and Color Vision</title><link>https://juliocontrerash.github.io/try-isp-page/code/vision_and_color/colorlab/content/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://juliocontrerash.github.io/try-isp-page/code/vision_and_color/colorlab/content/</guid><description>ColorLab is a color computation and visualization toolbox to be used in the MATLAB environment. ColorLab is intended to deal with color in general-purpose quantitative colorimetric applications as color image processing and psychophysical experimentation.
ColorLab uses colorimetrically meaningful representations of color and color images (tristimulus values, chromatic coordinates and luminance, or, dominant wavelength, purity and luminance), in any primaries system of the tristimulus colorimetry (including CIE standards as CIE XYZ or CIE RGB).</description></item><item><title>RBIG4IT: Information Theory Measures via Multidimensional Gaussianization</title><link>https://juliocontrerash.github.io/try-isp-page/code/vision_and_color/colorlab/rbig4it/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://juliocontrerash.github.io/try-isp-page/code/vision_and_color/colorlab/rbig4it/</guid><description>Information theory is an outstanding framework to measure uncertainty, dependence and relevance in data and systems. It has several desirable properties for real world applications: it naturally deals with multivariate data, it can handle heterogeneous data types, and the measures can be interpreted in physical units. However, it has not been adopted by a wider audience because obtaining information from multidimensional data is a challenging problem due to the curse of dimensionality.</description></item><item><title>Spatio-Chromatic Information available from different Neural Layers (J. Malo, Journal of Mathematical Neuroscience 2020)</title><link>https://juliocontrerash.github.io/try-isp-page/code/vision_and_color/colorlab/spatio_chromatic/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://juliocontrerash.github.io/try-isp-page/code/vision_and_color/colorlab/spatio_chromatic/</guid><description>The image representations along the retina-cortex pathway are analyzed in terms of their ability to capture information about the visual scenes. The considered series of representations includes: (1) the LMS retinal images, (2) their von-Kries adapted version, (3) the opponent images at LGN, (4) their nonlinear version after Weber-like saturation, (5) the cortical local-frequency representation filtered by achromatic and chromatic CSFs, and (6) the cortical representation after divisive normalization.
Assuming a single-step transform from the retinal input to each of these representations, and sensors of the same Signal-to-Noise quality in each representation, our estimations of transmitted information show that: (a) progressively deeper representations are better in terms of the amount of captured information, (b) the transmitted information up to the cortical representation follows the probability of natural scenes over the chromatic and achromatic dimensions of the stimulus space, (c) the contribution of spatial transforms to capture visual information is substantially greater (67%) than the contribution of chromatic transforms (33%), and (d) nonlinearities of the responses contribute substantially to the transmitted information (about 28%) but less than the linear transforms (72%).</description></item><item><title>VistaLab: The Matlab Toolbox for Linear Spatio-Temporal Vision Models</title><link>https://juliocontrerash.github.io/try-isp-page/code/vision_and_color/colorlab/vistalab/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://juliocontrerash.github.io/try-isp-page/code/vision_and_color/colorlab/vistalab/</guid><description>The Matlab toolbox for linear spatio-temporal Vision Models VistaLab is a Matlab toolbox that provides the linear building-blocks to create spatio-temporal vision models and the tools to control the spatio-temporal properties of video sequences. These building blocks include the spatio-temporal receptive fields of LGN, V1, and MT cells, and the spatial and spatio-temporal Contrast Sensitivity Functions (CSFs). Additionally, VistaLab allows accurate spatio-temporal sampling, spatio-temporal Fourier domain visualization, and generation of video sequences with controlled texture and speed.</description></item><item><title>VistaModels: Computational models of Visual Neuroscience</title><link>https://juliocontrerash.github.io/try-isp-page/code/vision_and_color/colorlab/vistamodels/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://juliocontrerash.github.io/try-isp-page/code/vision_and_color/colorlab/vistamodels/</guid><description>The Toolboxes in the VistaModels site are organized in three categories of different nature: (a) Empirical-mechanistic Models, tuned to reproduce basic phenomena of color and texture perception, (b) Principled Models, derived from information theoretic arguments, and (c) Engineering-motivated Models, developed to address applied problems in image and video processing.
The algorithms in VistaModels require the standard building blocks provided in the (more basic) toolboxes VistaLab and ColorLab. However, the necessary functions from these more basic toolboxes are included in the packages listed below for the user convenience.</description></item><item><title>Visual Information Flow in Wilson Cowan Networks. GÃ³mez-Villa et al. Journal of Neurophysiology 2019.</title><link>https://juliocontrerash.github.io/try-isp-page/code/vision_and_color/colorlab/flow_wilson/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://juliocontrerash.github.io/try-isp-page/code/vision_and_color/colorlab/flow_wilson/</guid><description>The Wilson-Cowan interaction of wavelet-like visual neurons is analyzed in total correlation terms for the first time. Theoretical and empirical results show that a psychophysically-tuned interaction achieves the biggest efficiency in the most frequent region of the image space. This an original confirmation of the Efficient Coding Hypothesis and suggests that neural field models can be an alternative to Divisive Normalization in image compression.</description></item></channel></rss>