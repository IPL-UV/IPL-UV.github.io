<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>ISP - Image and Signal Processing group</title>
<link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer><link rel="shortcut icon" href=/images/isp_ico.webp type=image/x-icon><link href=https://fonts.cdnfonts.com/css/twentieth-century-for-kenmore rel=stylesheet><link rel=stylesheet href=/style/style.css><script src=/js/mode.js></script><script src=https://cdn.jsdelivr.net/npm/marked/marked.min.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-D8CVQKS51G"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-D8CVQKS51G")</script></head><body><nav class=custom-navbars><div class=custom-container><a href=/ class="custom-logo custom-hide-on-large"><img src=/images/isp_logo_sinfondo.webp alt="ISP Icon" class=custom-logo_nav>
<span class=custom-text-isp>ISP</span>
</a><button class=navbar-toggler aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class=custom-navbar-collapse><ul class=custom-navbar-nav><li class="custom-nav-item custom-desktop-only"><a class=custom-nav-link href=/>ISP</a></li><li class=custom-nav-item><a class=custom-nav-link href=/people/>People</a></li><li class="custom-nav-item custom-dropdown"><a class="custom-nav-link custom-dropdown-toggle" href=/research/philosophy/>Research</a><ul class=custom-dropdown-menu><li><a class=custom-dropdown-item href=/research/philosophy/>Philosophy</a></li><li><a class=custom-dropdown-item href=/research/machine_learning/>Machine learning</a></li><li><a class=custom-dropdown-item href=/research/visual_neuroscience/>Visual science</a></li><li><a class=custom-dropdown-item href=/research/visual_brain/>Image processing</a></li><li><a class=custom-dropdown-item href=/research/earth_science/>Earth science</a></li><li><a class=custom-dropdown-item href=/research/social_science>Social science</a></li></ul></li><li class=custom-nav-item><a class=custom-nav-link href=/projects/>Projects</a></li><li class=custom-nav-item><a class=custom-nav-link href=/facilities/>Facilities</a></li><li class="custom-nav-item custom-dropdown"><a class="custom-nav-link custom-dropdown-toggle" href=/publications/journals/>Publications</a><ul class=custom-dropdown-menu><li><a class=custom-dropdown-item href=/publications/journals/>Journals</a></li><li><a class=custom-dropdown-item href=/publications/conferences/>Conferences</a></li><li><a class=custom-dropdown-item href=/publications/books/>Books</a></li><li><a class=custom-dropdown-item href=/publications/talks/>Talks</a></li><li><a class=custom-dropdown-item href=/publications/theses/>Theses</a></li></ul></li><li class=custom-nav-item><a class=custom-nav-link href=/code/>Code</a></li><li class=custom-nav-item><a class=custom-nav-link href=/data/>Data</a></li><li class=custom-nav-item><a class=custom-nav-link href=/seminars/>Seminars</a></li><li class=custom-nav-item><a class=custom-nav-link href=/courses/>Courses</a></li><li class=custom-nav-item><a class=custom-nav-link href=/collaborators/>Collaborators</a></li><li class=custom-nav-item><a class=custom-nav-link href=/news/>News</a></li><li class=custom-nav-item><a class=custom-nav-link href=/contact/>Contact</a></li></ul></div></div></nav><main><div class=container><div class="content-container grid-layout"><div class=box-header><h1>Spatio-Chromatic Information available from different Neural Layers (J. Malo, Journal of Mathematical Neuroscience 2020)</h1></div><div class=box-abstract><p><p>The image representations along the retina-cortex pathway are analyzed in terms of their ability to capture information about the visual scenes. The considered series of representations includes: (1) the LMS retinal images, (2) their von-Kries adapted version, (3) the opponent images at LGN, (4) their nonlinear version after Weber-like saturation, (5) the cortical local-frequency representation filtered by achromatic and chromatic CSFs, and (6) the cortical representation after divisive normalization.</p><p>Assuming a single-step transform from the retinal input to each of these representations, and sensors of the same Signal-to-Noise quality in each representation, our estimations of transmitted information show that: (a) progressively deeper representations are better in terms of the amount of captured information, (b) the transmitted information up to the cortical representation follows the probability of natural scenes over the chromatic and achromatic dimensions of the stimulus space, (c) the contribution of spatial transforms to capture visual information is substantially greater (67%) than the contribution of chromatic transforms (33%), and (d) nonlinearities of the responses contribute substantially to the transmitted information (about 28%) but less than the linear transforms (72%).</p><p>The parameters of the model have psychophysical origin and they were not statistically optimized in any way. The information estimates were computed using our <a href=./../rbig4it>Gaussianization transform</a> (Laparra et al. IEEE TNN 11) over our database of colorimetrically-calibrated images (Laparra et al. Neur.Comp. 12), but equivalent results are obtained using other estimates (e.g. offset corrected Kozachenko-Leonenko [Marin et al. IEEE PAMI 13]) or other databases (e.g. Foster et al [Vis.Res.15,16]).</p></p></div><section class="box-references title2"><h2>References</h2><ul class=references-list><li><strong><a href=https://arxiv.org/abs/1910.01559 target=_blank class=references-name>Spatio-Chromatic Information available from different Neural Layers via Gaussianization</a></strong><br><span>J. Malo.</span><br><em><a href=https://doi.org/10.1186/s13408-020-00095-8>J. Mathematical Neuroscience (2020)</a></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/infoWC_JNP19.pdf target=_blank class=references-name>Visual information Flow in Wilson Cowan Networks</a></strong><br><span>A. Gómez-Villa, M. Bertalmío and J. Malo.</span><br><em>J. Neurophysiol. 123 (6): 2249-2268 (2020)</em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/Entropy_conf_2020.pdf target=_blank class=references-name>Information Flow in Color Appearance Neural Networks</a></strong><br><span>J. Malo.</span><br><em><a href=https://arxiv.org/abs/1912.12093>arXiv: Quantitative Biology, Neurons and Cognition (2019)</a></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/NOTES_info_flow_psycho_models.pdf target=_blank class=references-name>Visual information Flow in Psychophysical-Physiological networks</a></strong><br><span><a href=https://docs.google.com/document/d/14LvHeix6zE92e-T4w7e9ZmBqS6uVd4uOJ22N6-NFCc0/edit>J. Malo and Q. Li</a></span><br><em>Notebook (as of July 2021) Evolving Google Notebook</em></li></ul></section><section class="box-enlaces title2"><h2>Download</h2><ul class=enlaces-list><li><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/infoDivisiveNormalization.zip>Data and Code infoDivisiveNormalization.zip (17GB)</a></li></ul></section></div></div></main></body><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js integrity=sha384-C6RzsynM9kWDrMNeT87bh95OGNyZPhcTNXj1NW7RuBCsyN/o0jlpcV8Qyq46cDfL crossorigin=anonymous defer></script></html>