<!doctype html><html lang=en-us dir=ltr><head><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>ISP - Image and Signal Processing group</title>
<link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-T3c6CoIi6uLrA9TneNEoa7RxnatzjcDSCmG1MXxSR1GAsXEV/Dwwykc2MPK8M2HN crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css integrity="sha512-z3gLpd7yknf1YoNbCzqRKc4qyor8gaKU1qmn+CShxbuBusANI9QpRohGBreCFkKxLhei6S9CQXFEbbKuqLg0DA==" crossorigin=anonymous referrerpolicy=no-referrer><link rel="shortcut icon" href=http://isp.uv.es/favicon.ico type=image/x-icon><link rel=stylesheet href=/style/style.css><script src=/js/mode.js></script><script src=https://cdn.jsdelivr.net/npm/marked/marked.min.js></script></head></head><nav class="navbar navbar-expand-lg bg-body-tertiary fixed-top"><div class=container-fluid><a href=/ class="d-lg-none d-flex align-items-center a_logonav"><img src=/images/isp_logo_sinfondo.webp alt="ISP Icon" height=30 class=logo_nav>
<span class="ms-2 text-isp">ISP</span>
</a><button class="navbar-toggler ms-auto" type=button data-bs-toggle=collapse data-bs-target=#navbarTogglerDemo01 aria-controls=navbarTogglerDemo01 aria-expanded=false aria-label="Toggle navigation" style=height:40px>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarTogglerDemo01><ul class="navbar-nav mx-auto mb-lg-0"><li class="nav-item px-2 nav-item-highlight d-none d-lg-block"><a class="nav-link a" aria-current=page href=/>ISP</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/people/>People</a></li><li class="nav-item dropdown px-2 nav-item-highlight"><a class="nav-link dropdown-toggle a" href=/research/ id=navbarDropdownResearch role=button aria-expanded=false>Research</a><ul class=dropdown-menu aria-labelledby=navbarDropdownResearch><li><a class="dropdown-item a" href=/research/machine_learning/>Machine learning</a></li><li><a class="dropdown-item a" href=/research/visual_neuroscience/>Visual neuroscience</a></li><li><a class="dropdown-item a" href=/research/visual_brain/>Visual brain</a></li><li><a class="dropdown-item a" href=/research/earth_science/>Earth science</a></li><li><a class="dropdown-item a" href=/research/social_science>Social science</a></li></ul></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/projects/>Projects</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/facilities/>Facilities</a></li><li class="nav-item dropdown px-2 nav-item-highlight"><a class="nav-link dropdown-toggle a" href=/publications/journals/ id=navbarDropdownPublications role=button aria-expanded=false>Publications</a><ul class=dropdown-menu aria-labelledby=navbarDropdownPublications><li><a class="dropdown-item a" href=/publications/journals/>Journals</a></li><li><a class="dropdown-item a" href=/publications/conferences/>Conferences</a></li><li><a class="dropdown-item a" href=/publications/books/>Books</a></li><li><a class="dropdown-item a" href=/publications/talks/>Talks</a></li><li><a class="dropdown-item a" href=/publications/technical_reports/>Technical Reports</a></li><li><a class="dropdown-item a" href=/publications/theses/>Theses</a></li></ul></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/code/>Code</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/data/>Data</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/seminars/>Seminars</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/courses/>Courses</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/collaborators/>Collaborators</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/news/>News</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/contact/>Contact</a></li></ul></div></div></nav><style>.navbar{--bs-navbar-padding-y:0rem !important;background-color:#222!important}.a{color:#949494!important}.a:hover{color:#fff!important}.nav-item-highlight{padding:.4rem}.nav-item-highlight:hover{background-color:#2d70aa!important}.dropdown-menu{background-color:#333!important;color:#fff!important;display:block}.dropdown-item{color:#949494!important}.dropdown-item:hover{background-color:#2d70aa!important;color:#fff!important}.navbar-nav li.nav-item .nav-link,.dropdown-menu .dropdown-item{transition:color .3s,background-color .3s}.nav-link.active,.dropdown-item.active{color:#fff!important;background-color:#007bff!important}.navbar-toggler-icon{background-image:url("data:image/svg+xml,%3csvg viewBox='0 0 30 30' xmlns='http://www.w3.org/2000/svg'%3e%3cpath stroke='white' stroke-width='2' stroke-linecap='round' stroke-miterlimit='10' d='M4 7h22M4 15h22M4 23h22'/%3e%3c/svg%3e")}.navbar-toggler{border:none}.navbar-toggler:focus{outline:none}@media(min-width:992px){.dropdown-menu{display:none}.dropdown:hover .dropdown-menu{display:block}}@media(max-width:991px){.navbar-nav{max-height:calc(100vh - 56px);overflow-y:auto}.navbar-nav::-webkit-scrollbar{width:9px}.navbar-nav::-webkit-scrollbar-thumb{background-color:#888;border-radius:10px}.navbar-nav::-webkit-scrollbar-thumb:hover{background-color:#555}.navbar-nav .nav-link{font-size:1.2rem}.dropdown-menu .dropdown-item{font-size:1rem}.dropdown-toggle::after{display:inline-block;margin-left:.255em;vertical-align:.255em;content:"";border-top:.3em solid;border-right:.3em solid transparent;border-bottom:0;border-left:.3em solid transparent}.navbar .dropdown-toggle::after{content:none!important}.navbar-toggler-icon{width:1.2em;height:1.2em}.navbar-toggler{border:none!important}.navbar-toggler:focus{box-shadow:none!important}.d-flex{height:45px}.a_logonav{text-decoration:none!important}.text-isp{font-size:1.3rem;color:#9d9d9d;display:inline-block;vertical-align:middle;text-decoration:none!important}.navbar-nav{max-height:calc(50vh - 56px);overflow-y:auto}}</style><script>document.addEventListener("DOMContentLoaded",function(){let e=document.querySelectorAll(".navbar .dropdown");e.forEach(function(e){let t=e.querySelector(".dropdown-toggle");t.addEventListener("click",function(e){window.innerWidth<992&&e.target===t&&(window.location.href=t.href)});let n=e.querySelectorAll(".dropdown-item");n.forEach(function(e){e.addEventListener("click",function(){window.location.href=e.href})})}),document.addEventListener("click",function(t){window.innerWidth<992&&!t.target.closest(".navbar .dropdown")&&e.forEach(function(e){e.querySelector(".dropdown-menu").classList.remove("show")})})})</script><main><div class=container><div class="content-container grid-layout"><div class=box-header><h1>VistaModels: Computational models of Visual Neuroscience</h1></div><div class=box-abstract><p><p>The Toolboxes in the VistaModels site are organized in three categories of different nature: <a href=#a-empirical-mechanistic-models><strong>(a) Empirical-mechanistic Models</strong></a>, tuned to reproduce basic phenomena of color and texture perception, <a href=#b-efficient-coding-in-mechanistic-models><strong>(b) Principled Models</strong></a>, derived from information theoretic arguments, and <a href=#c-engineering-motivated-models><strong>(c) Engineering-motivated Models</strong></a>, developed to address applied problems in image and video processing.</p><p>The algorithms in <strong>VistaModels</strong> require the standard building blocks provided in the (more basic) toolboxes VistaLab and ColorLab. However, the necessary functions from these more basic toolboxes are included in the packages listed below for the user convenience.</p><h1 id=table-of-contents>Table of contents</h1><ul><li><a href=#a-empirical-mechanistic-models><strong>(A) Empirical-mechanistic Models</strong></a><ul><li><a href=#1995---2008-linear-opponent-color-channels-local-dct-and-divisive-normalization>1995 - 2008: Linear opponent color channels, local-DCT and Divisive Normalization</a></li><li><a href=#2009---2010-linear-opponent-color-channels-orthogonal-wavelet-and-divisive-normalization>2009 - 2010: Linear opponent color channels, Orthogonal Wavelet and Divisive Normalization</a></li><li><a href=#2013---2018-multi-layer-network-with-nonlinear-opponent-color-overcomplete-wavelet-and-divisive-normalization>2013 - 2018: Multi-Layer network with nonlinear opponent color, Overcomplete Wavelet and Divisive Normalization</a></li><li><a href=#2019---2021-convolutional-and-differentiable-implementations>2019 - 2021: Convolutional and differentiable implementations</a></li><li><a href=#psychophysical-test-bed-for-model-tuning-and-comparison>Psychophysical test-bed for model tuning and comparison</a></li><li><a href=#model-comparison>Model Comparison</a></li></ul></li><li><a href=#b-principled-models><strong>(B) Principled Models</strong></a><ul><li><a href=#efficient-coding-in-mechanistic-models>Efficient coding in mechanistic models</a></li><li><a href=#statistically-based-linear-receptive-fields>Statistically-based linear receptive fields</a></li><li><a href=#statistically-based-nonlinearities>Statistically-based nonlinearities</a></li></ul></li><li><a href=#c-engineering-motivated-models><strong>(C) Engineering-motivated Models</strong></a><ul><li><a href=#perceptually-weighted-motion-estimation-vistavideocoding>Perceptually-weighted motion estimation: VistaVideoCoding</a></li><li><a href=#image-coding-vistacore>Image Coding: VistaCoRe</a></li><li><a href=#image-and-video-quality-vistaqualitytools>Image and Video Quality: VistaQualityTools</a></li></ul></li></ul><h1 id=a-empirical-mechanistic-models>(A) Empirical-mechanistic Models</h1><p>Cascades of linear transforms and nonlinear saturations are ubiquitous in neuroscience and artificial intelligence ever since the [<a href=http://www.scholarpedia.org/article/Models_of_visual_cortex>McCulloch-Pitts model</a>]. More recently this has been exemplified in subtractive and divisive models of cortical interaction [Wilson & Cowan, Kybernetik 73; Carandini and Heeger, Nature Rev. Neurosci. 12].</p><p>Over the years, we have developed progressively better versions of such cascades to be applicable to color images and video sequences. These parametric models were empirically tuned to give a rough description of different color and texture perception phenomena (see the <a href=#psychophysical-test-bed-for-model-tuning-and-comparison>psychophysical test-bed</a> below for model tuning and comparison).</p><p>See a visual example of the effect of the local spatial-frequency transforms and the divisive normalization below (illustration of the 2018 model)</p><h2 id=1995---2008-linear-opponent-color-channels-local-dct-and-divisive-normalization>1995 - 2008: Linear opponent color channels, local-DCT and Divisive Normalization</h2><p>This model is invertible and was originally tuned to reproduce contrast response curves obtained from contrast incremental thresholds [Pons PhD Thesis, 1997]. It was applied to reproduce subjective distortion opinion [<a href=https://www.sciencedirect.com/science/article/abs/pii/S0262885696000042>Im.Vis.Comp.97</a>, <a href=https://www.sciencedirect.com/science/article/abs/pii/S0141938299000098>Displays 99</a>] and to improve the perceptual quality of JPEG and MPEG through (a) transform coding of the achromatic channel [<a href=https://www.uv.es/vista/vistavalencia/papers/ELECT95.PS.gz>Elect.Lett.95</a>, <a href=https://www.uv.es/vista/vistavalencia/papers/ELECT99.PS.gz>Elect.Lett.99</a>, <a href=https://www.uv.es/vista/vistavalencia/papers/ivc99.ps.gz>Im.Vis.Comp.00</a>, <a href=https://www.uv.es/vista/vistavalencia/papers/ieeeoct01.pdf>IEEE TIP 01</a>, <a href=https://www.uv.es/vista/vistavalencia/papers/patt_rec03.pdf>Patt.Recog.03</a>, <a href=https://www.uv.es/vista/vistavalencia/papers/SVM_JND8_ACCEPTED.pdf>IEEE TNN 05</a>, <a href=https://www.uv.es/vista/vistavalencia/papers/manuscript4.pdf>IEEE TIP 06a</a>, <a href=https://www.uv.es/vista/vistavalencia/papers/camps_JMLR_08.pdf>JMLR08</a>], (b) the color channels [<a href=https://www.eurekaselect.com/96168/article>RPSP12</a>], and (c) by improving the motion estimation [<a href=https://www.uv.es/vista/vistavalencia/papers/LNCS97.PS.gz>LNCS97</a>, <a href=https://www.uv.es/vista/vistavalencia/papers/ELECT98.PS.gz>Elect.Lett.98</a>, <a href=https://www.uv.es/vista/vistavalencia/papers/seg_ade2.ps>Elect.Lett.00a</a>, <a href=https://www.uv.es/vista/vistavalencia/papers/elect00.ps>Elect.Lett.00b</a>, <a href=https://www.uv.es/vista/vistavalencia/papers/ieeeoct01.pdf>IEEE TIP 01</a>].</p><ul><li><strong>Download the Toolbox!:</strong> <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/V1_model_DCT_DN_color.zip>V1_model_DCT_DN_color.zip (74MB)</a></li></ul><h2 id=2009---2010-linear-opponent-color-channels-orthogonal-wavelet-and-divisive-normalization>2009 - 2010: Linear opponent color channels, Orthogonal Wavelet and Divisive Normalization</h2><p>Even though we developed our own Matlab code for some specific overcomplete wavelets in the mid 90&rsquo;s [<a href=http://www.uv.es/vista/vistavalencia/papers/tesis/msc_jmalo.zip>MSc Thesis 95</a>, <a href=https://www.uv.es/vista/vistavalencia/papers/JMO97.PS.gz>J.Mod.Opt.97</a>], it took some time until we applied the Divisive Normalization interaction to Simoncelli&rsquo;s wavelets in MatlabPyrTools (which are substantially more efficient). The model was fitted to reproduce subjective image distortion opinion [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/Laparra_JOSA_10.pdf>JOSA A 10</a>] following exhaustive grid search as in [<a href=https://www.uv.es/vista/vistavalencia/papers/icip02.pdf>IEEE ICIP 02</a>]. This model (which relies on the orthogonal wavelets of the MatlabPyrTools) was found to have excellent redundancy reduction properties [<a href=https://link.springer.com/chapter/10.1007/978-3-642-11509-7_3>LNCS10</a>, <a href=https://www.uv.es/vista/vistavalencia/papers/Malo_Laparra_Neural_10b.pdf>Neur.Comp.10</a>].</p><ul><li><strong>Download the Toolbox!:</strong> <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/V1_model_wavelet_DN_color.zip>V1_model_wavelet_DN_color.zip (14MB)</a></li></ul><h2 id=2013---2018-multi-layer-network-with-nonlinear-opponent-color-overcomplete-wavelet-and-divisive-normalization>2013 - 2018: Multi-Layer network with nonlinear opponent color, Overcomplete Wavelet and Divisive Normalization</h2><p>Even though we developed a comprehensive color vision toolbox in the early 2000&rsquo;s (see <a href=./../content>ColorLab</a> ), it took some time until we included a fully adaptive chromatic front-end before the spatial processing models based on overcomplete wavelets. Note that the older toolboxes rely on (too rough) linear RGB to YUV transforms. This multi-layer model (or biologically-plausible deep network) performs the following chain of perceptually meaningful operations [<a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0201326">PLoS 18</a>].</p><p>The parameters of the different layers were fitted in different ways: while the 2nd and 3rd layers (contrast and CSF+masking) were determined using Maximum Differentiation [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/malo15a-reprint.pdf>Malo and Simoncelli SPIE 15</a>], layers 1st and 4th (chromatic front-end and wavelet layer) were fitted to reproduce subjective image distortion data [<a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0201326">PLoS 18</a>], and then fine-tuned to reproduce classical masking [<a href=https://www.frontiersin.org/articles/10.3389/fnins.2019.00008/full>Front. Neurosci. 19</a>].</p><ul><li><strong>Download the Toolbox!:</strong> <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/BioMultiLayer_L_NL_color.zip>BioMultiLayer_L_NL_color.zip (49MB)</a></li></ul><h2 id=2019---2021-convolutional-and-differentiable-implementations>2019 - 2021: Convolutional and differentiable implementations</h2><p>The matrix formulation developed in [<a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0201326">PLoS 18</a>, <a href=https://www.frontiersin.org/articles/10.3389/fnins.2019.00008/full>Front. Neurosci. 19</a>] and implemented in BioMultiLayer_L_NL_color is elegant but not applicable to large images nor appropriate to be included in python deep-learning schemes since it is implemented in Matlab. Recently we worked to solve these issues and confirm the choices of the chromatic part. This led to the deep Percepnet [<a href=https://ieeexplore.ieee.org/document/9190691>IEEE ICIP 20</a>], and to the convolutional version the above MultiLayer L+NL cascade [J.Vision, Proc. VSS 2021]. While Percepnet has the advantage of being implemented in python and hence ready for automatic differentiation (state-of-the-art in image quality), it has the disadvantage of being based on a restricted version of Divisive Normalization (no explicit interactions in space/scale) [<a href="https://openreview.net/forum?id=rJxdQ3jeg">ICLR 17</a>]. On the other hand, the BioMultiLayer_L_NL_color_convolutional has a more general and interpretable version of the Divisive Normalization (in includes full range of interactions in space/scale/orientation). Moreover, the color adaptation choices and the scaling of the achromatic and chromatic channels has been confirmed by positive psychophysical and statistical behaviors [<a href=https://journals.physiology.org/doi/abs/10.1152/jn.00487.2019>J. Neurophysiol.19</a>, <a href=https://mathematical-neuroscience.springeropen.com/articles/10.1186/s13408-020-00095-8>J. Math.Neurosci.20</a>]. However, derivatives are implemented in matlab, so it is not ready to be included in deep-learning schemes right away. There is a lot of room for improvement of its parameters!.</p><ul><li><p><strong>Download the Toolbox!:</strong> <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/BioMultiLayer_L_NL_color_convolutional.zip>BioMultiLayer_L_NL_color_convolutional.zip (76MB)</a></p></li><li><p><strong>Visit Github!:</strong> <a href=https://github.com/alexhepburn/perceptnet>Perceptnet</a></p></li><li><p><strong>Statistical and Psychophysical support for the chromatic choices (I/II):</strong> The small scale model in this paper uses the same chromatic choices as the BioMultiLayer_L_NL models. <a href=./../flow_wilson>Code and Data for small scale recurrent Wilson-Cowan network [J.Neurophysiol. 2020]</a></p></li><li><p><strong>Statistical and Psychophysical support for the chromatic choices (II/II):</strong> The small scale model in this paper uses the same chromatic choices as the BioMultiLayer_L_NL models. <a href=./../spatio_chromatic>Code and Data for small scale Div. Norm. [J.Math.Neurosci. 2020]</a></p></li></ul><h2 id=psychophysical-test-bed-for-model-tuning-and-comparison>Psychophysical test-bed for model tuning and comparison</h2><p>The figure below (computed using <a href=./../vistalab>VISTALAB</a> and <a href=./../content>ColorLab</a>) illustrates distinctive features of early vision: (a) the bandwidth of the achromatic and the chromatic channels is markedly different, (b) the response to contrast is a saturating nonlinearity, its slope (sensitivity) depends on the frequency and the response attenuates as a function of the properties of the background (note how the test is more salient -highlighted in green- on top of a very different background while it is masked -highlighted red- on top of similar backgrounds), and (c) the visibility of i.i.d. noise seen on top of a natural image is not uniform: e.g. visibility is smaller in high contrast regions.</p><p>These quite visible facts can be used to tune the parameters of the mechanistic models considered above. One could play with the parameters by hand until the response curves qualitatively reproduce what one actually sees. We suggested this idea to improve model fit in natural image databases [Front.Neurosci.18] and (for the first time!) here is data and code to perform such tune-it-yourself experiments: <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/experiments_VistaModels.zip>experiments_VistaModels.zip (400MB)</a>.</p><p>File is huge because it contains thousands of tests to compute detailed contrast response curves and distortion measures on the TD database. Moreover, it also has the corresponding responses of the three mechanistic models!.</p><p>Results below suggest that models are equivalent but the most recent displays better behavior (on top of having more plausible receptive fields [<a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0201326">PLoS 18</a>]). More importantly, while the results on Image Quality are way better than the popular Structural Similarity Index SSIM (see VistaQualityTools ) there is still a lot of room for improvement through these tune-it-yourself experiments!.</p><h2 id=model-comparison>Model Comparison</h2><h1 id=b-principled-models>(B) Principled Models</h1><h2 id=efficient-coding-in-mechanistic-models>Efficient coding in mechanistic models</h2><p>We have shown that models including point-wise Weber-like saturation for brightness lead to decreasing signal-to-noise ratio as a function of the luminance [J.Opt.95]. Moreover, taking into account more general cascades of linear+nonlinear layers (e.g. local-frequency transforms and divisive normalization after Weber-brightness) we have seen that the efficiency of such systems (in terms of redundancy reduction) decreases with luminance and contrast, which is consistent with the distribution of natural images in local frequency domains [PLoS 18]. We have seen that the discrimination ability of Local-DCT+Div.Norm. models is bigger in the more populated regions of the frequency-amplitude domain [Im.Vis.Comp.97]. Additionally, we have seen that the mutual information between the coefficients of the image representation progressively reduces from the retina to the normalized representation, both in the local-DCT + DN case [IEEE TIP 06] and in the Orthogonal wavelet+DN case <a href=https://www.uv.es/vista/vistavalencia/papers/Malo_Laparra_Neural_10b.pdf>Neur.Comp.10</a>.</p><p>The above body of results means that the Mechanistic Models considered above display remarkable adaptation to the natural image statistics.</p><p>In the same line, in collaboration with NYU (Balle and Simoncelli) we have optimized the described linear+nonlinear architectures for optimal autoencoding. By including both the linear and the nonlinear parts in the optimization we get unprecedented rate-distortion performances (see paper and code here [<a href=http://www.cns.nyu.edu/~lcv/iclr2017>ICLR 17</a>]), way better than our previous image coders based on V1 models with fixed linear stages (See the <a href=./../../../image_video_processing/vistacore/content>VistaCoRe</a> Toolbox).</p><h2 id=statistically-based-linear-receptive-fields>Statistically-based linear receptive fields</h2><p>Statistical goals such as decorrelation (Principal Component Analysis, PCA), and Independent Component Analysis (ICA) many time lead to sensible linear receptive fields when trained with natural scenes. For instance, spatio-spectral PCA leads to compact representations to disentangle reflectance and spectral illumination from retinal irradiance and lead to spatial-frequency sensors with smooth spectral response [IEEE TGRS 13] (see VistaSpatioSpectral). In collaboration with Helsinki University (Gutman and Hyvarinen) we explored ICA-related techniques. Complex ICA led to local and oriented receptive fields in phase quadrature [LNCS11] (download the <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/CICA_toolbox.zip>Complex ICA</a> Toolbox). Higher Order Canonical Correlation Analysis (HOCCA) combines the sparsity goal with optimal correspondence between identified features in domain adaptation problems leading to biologically plausible spatiochromatic receptive fields which adapt to changes in the illumination (PLoS 14, see the <a href=./../../../feature_extraction/hocca/content>HOCCA</a> Toolbox).</p><p>This analysis of ICA methods concluded with a refutation of a classical result in cortical organization based on Topographica ICA: in fact (as opposed to Hyvarinen & Hoyer Vis. Res. 2001) it does not lead to orientation domains [PLoS 17]. See code and results to analyze <a href=./../../tica/content>TICA</a> receptive fields.</p><h2 id=statistically-based-nonlinearities>Statistically-based nonlinearities</h2><p>Instead of optimizing the mechanistic models for efficient coding we tried a stronger approach to test the Efficient Coding Hypothesis: use pure data-driven techniques instead of assuming models which already have the right functional form. We developed a family of invertible techniques for manifold unfolding and for manifold Gaussianization.</p><p>The unfolding techniques identify nonlinear sensors that follow curved manifolds. These include Sequential Principal Curves Analysis <a href=./../../../feature_extraction/spca/content><strong>SPCA</strong></a> and sequels: Principal Polynomial Analysis <a href=./../../../feature_extraction/ppa/content><strong>PPA</strong></a> and Dimensionality Reduction based on Regression, <a href=./../../../feature_extraction/ddr/content><strong>DDR</strong></a>.</p><p>The Gaussianization technique (Rotation-Based Iterative Gaussianization, <a href=./../../../feature_extraction/rbig/content><strong>RBIG</strong></a>) does not identify sensors but it allows to compute the PDF. Therefore it is useful to define discrimination regions according to information maximization or error minimization. See the kind of predictions made by these unfolding techniques (SPCA [Network 06, NeCo12, Front. Human Neurosci.15, ArXiv 16, https://arxiv.org/pdf/1606.00856.pdf], and PPA-DRR [SPIE13, Int.J.Neur.Syst.14, IEEE Sel.Top.Sig.Proc.15]) and by the Gaussianization technique [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/2013_Courant_features_RBIG.pdf>Talk at LeCun Lab NYU 13</a>, IEEE TNN 11].</p><p>Closely related to optimal discrimination (or optimal metric) for error minimization is the concept of Fisher Information. Our lab has a tradition in the study of Riemannian metrics induced by nonlinear perception systems [J. Malo PhD 99, Displ.99]. Over the years, the ideas about the geometrical transforms induced by the system and their effect on information processing have evolved from distance computation to the consideration of the transformation of neural noise [Displ.99, Patt.Recog.03, IEEE TIP 06, JOSA A 10, SPIE 15, NIPS 17, PLoS 18].</p><h1 id=c-engineering-motivated>(C) Engineering-motivated</h1><h2 id=perceptually-weighted-motion-estimation-vistavideocoding>Perceptually-weighted motion estimation: VistaVideoCoding</h2><p>What can be predicted is not worth transmitting!. This simple idea is the core of predictive coding used in most successful video coders (e.g. MPEG). In predictive coding motion information is the key to predict future-from-past. MPEG-like coders first compute the optical flow (or displacement field) and encode the prediction error in a transformed domain which (not surprisingly!) is similar to the <a href=#a-empirical-mechanistic-models>V1 mechanistic models</a> described above.</p><p>In this video-coding context we improved motion estimation by connecting the optical flow computation with the perceptual relevance of the prediction error: we proposed to improve the resolution of the motion estimate only if the prediction error was hard to encode for our improved V1 models [LNCS97, Electr.Lett.98, J.Vis.01]. This gave rise to smoother motion flows more appropriate for motion-based segmentation [Electr.Lett.00a], and to better video coders [Electr.Lett.00b, IEEE TIP 01].</p><ul><li>Download the motion estimation and video coding toolbox! <a href=./../../../image_video_processing/videocodingtools/content>VistaVideoCoding</a>.</li></ul><h2 id=image-coding-vistacore>Image Coding: VistaCoRe</h2><p>Image compression requires vision models that rank visual features according to their perceptual relevance so that extra bits can be allocated to encode the subjectively important aspects of the image.</p><p>The vision model based on DCT and Divisive Normalization considered above leads to better decoded images at the same compression ratio than JPEG and variants based on simpler models of masking.</p><p>See the <a href=./../../../image_video_processing/vistacore/content>VistaCoRe</a> (Coding and Restoration Toolbox), and the references [Eletr.Lett95, Eletr.Lett99, Im.Vis.Comp.00 Patt.Recog.03, IEEE TNN 05, IEEE TIP 06a, IEEE TIP 06b, JMLR08].</p><h2 id=image-and-video-quality-vistaqualitytools>Image and Video Quality: VistaQualityTools</h2><p>Computing perceptual distances between images requires vision models that identify relevant and negligible visual features. Distortions in features that will be neglected by the observers should induce no effect in the distance. And the other way around for visually relevant features. The different models can be quantitatively compared by their accuracy in reproducing the opinion of viewers in subjectively rated databases.</p><p>The three vision models considered above (based on DCTs, orthonormal wavelets, and overcomplete wavelets) have been used to propose distortion metrics that overperform SSIM. See <a href=./../../../image_video_processing/vistaqualitytools/content>VistaQualityTools</a>, and the references [Im.Vis.Comp.97, Displays99, Patt.Recog.03, IEEE Trans.Im.Proc.06] for the DCT metric, [JOSA 10, Neur.Comp.10] for the orthogonal wavelet metric, and [PLoS 18, Frontiers Neurosci.18] for the metric based on overcomplete wavelets.</p></p></div><aside class=box-gallery><div class=gallery-grid><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/adicionales/VistaModels1.webp","Mechanistic Models","Following Hubel-Wiesel and McCulloch-Pitts, our models are cascades of two basic elements: (a) a linear transform (not necessarily convolutional set of receptive fields), and (b) a nonlinear saturation (either divisive or subtractive) describing the interactions between the linear units. We have played with different versions of such elements. For the linear part we explored center-surround units, local-DCTs, Orthonormal Wavelets, Overcomplete Wavelets and Laplacian Pyramids. For the nonlinear part played with different adaptive nonlinearities such as the Divisive Normalization and the subtractive Wilson-Cowan equations. See [[PLoS 2018](https://arxiv.org/abs/1711.00526)] for a comprehensive account of the maths, and [[ArXiV 2018](https://arxiv.org/abs/1804.05964)] for the equivalence between the considered nonlinear models. These models have been tuned to reproduce basic psychophysics such as contrast response curves and subjective image distortion.")'><img src=/images/adicionales/VistaModels1.webp alt="Mechanistic Models"></a><p class=gallery-title>Mechanistic Models</p><div class=gallery-description><p>Following Hubel-Wiesel and McCulloch-Pitts, our models are cascades of two basic elements: (a) a linear transform (not necessarily convolutional set of receptive fields), and (b) a nonlinear saturation (either divisive or subtractive) describing the interactions between the linear units. We have played with different versions of such elements. For the linear part we explored center-surround units, local-DCTs, Orthonormal Wavelets, Overcomplete Wavelets and Laplacian Pyramids. For the nonlinear part played with different adaptive nonlinearities such as the Divisive Normalization and the subtractive Wilson-Cowan equations. See [<a href=https://arxiv.org/abs/1711.00526>PLoS 2018</a>] for a comprehensive account of the maths, and [<a href=https://arxiv.org/abs/1804.05964>ArXiV 2018</a>] for the equivalence between the considered nonlinear models. These models have been tuned to reproduce basic psychophysics such as contrast response curves and subjective image distortion.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/adicionales/VistaModels2.webp","Statistical Principles","The emergence of (a) specific sensors (e.g. the red and green curves), or (b) specific discrimination properties (ellipsoids in gray) may be understood as an adaptation to the statistics of natural input (samples in blue). We have used these [Barlow-style information-theoretic priciples](https://www.youtube.com/watch?v=cv9hje42i_E) in two ways: unfolding the data manifolds [[Front. Human Neurosci. 15](https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/LaparraMalo15.pdf)], and Gaussianizing the data manifolds [IEEE Trans. Neur. Nets. 11]. Interestingly, nonlinearities of the Human Visual System (from retina [[J.Opt.95](https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/JOPT95.PS.gz)] to cortex [[Im.Vis.Comp.00](https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/ivc99.ps.gz), [Neural Comp.10](https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/Malo_Laparra_Neural_10b.pdf)]) have remarkable statistical effects too!.")'><img src=/images/adicionales/VistaModels2.webp alt="Statistical Principles"></a><p class=gallery-title>Statistical Principles</p><div class=gallery-description><p>The emergence of (a) specific sensors (e.g. the red and green curves), or (b) specific discrimination properties (ellipsoids in gray) may be understood as an adaptation to the statistics of natural input (samples in blue). We have used these <a href="https://www.youtube.com/watch?v=cv9hje42i_E">Barlow-style information-theoretic priciples</a> in two ways: unfolding the data manifolds [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/LaparraMalo15.pdf>Front. Human Neurosci. 15</a>], and Gaussianizing the data manifolds [IEEE Trans. Neur. Nets. 11]. Interestingly, nonlinearities of the Human Visual System (from retina [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/JOPT95.PS.gz>J.Opt.95</a>] to cortex [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/ivc99.ps.gz>Im.Vis.Comp.00</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/Malo_Laparra_Neural_10b.pdf>Neural Comp.10</a>]) have remarkable statistical effects too!.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/adicionales/modelB.webp","[Multi-Layer Network Model](#2013---2018-multi-layer-network-with-nonlinear-opponent-color-overcomplete-wavelet-and-divisive-normalization)","Multilayer network model that includes nonlinear chromatic processing and overcomplete wavelets.")'><img src=/images/adicionales/modelB.webp alt="[Multi-Layer Network Model](#2013---2018-multi-layer-network-with-nonlinear-opponent-color-overcomplete-wavelet-and-divisive-normalization)"></a><p class=gallery-title><a href=#2013---2018-multi-layer-network-with-nonlinear-opponent-color-overcomplete-wavelet-and-divisive-normalization>Multi-Layer Network Model</a></p><div class=gallery-description><p>Multilayer network model that includes nonlinear chromatic processing and overcomplete wavelets.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/adicionales/facts1.webp","[Facts of Vision - Achromatic and Chromatic Bandwidths](#psychophysical-test-bed-for-model-tuning-and-comparison)","Ilustración de características distintivas de la visión temprana, como las diferentes bandas de frecuencia para los canales acromáticos y cromáticos.")'><img src=/images/adicionales/facts1.webp alt="[Facts of Vision - Achromatic and Chromatic Bandwidths](#psychophysical-test-bed-for-model-tuning-and-comparison)"></a><p class=gallery-title><a href=#psychophysical-test-bed-for-model-tuning-and-comparison>Facts of Vision - Achromatic and Chromatic Bandwidths</a></p><div class=gallery-description><p>Ilustración de características distintivas de la visión temprana, como las diferentes bandas de frecuencia para los canales acromáticos y cromáticos.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/adicionales/visib_noise.webp","[Noise Visibility on Natural Images](#psychophysical-test-bed-for-model-tuning-and-comparison)","Visualization of noise visibility in natural images, showing lower visibility in high contrast regions.")'><img src=/images/adicionales/visib_noise.webp alt="[Noise Visibility on Natural Images](#psychophysical-test-bed-for-model-tuning-and-comparison)"></a><p class=gallery-title><a href=#psychophysical-test-bed-for-model-tuning-and-comparison>Noise Visibility on Natural Images</a></p><div class=gallery-description><p>Visualization of noise visibility in natural images, showing lower visibility in high contrast regions.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/adicionales/compCSFs.webp","[Model Comparison - CSFs](#b-principled-models)","Comparison of Contrast Sensitivity Functions (CSF) between different mechanistic models.")'><img src=/images/adicionales/compCSFs.webp alt="[Model Comparison - CSFs](#b-principled-models)"></a><p class=gallery-title><a href=#b-principled-models>Model Comparison - CSFs</a></p><div class=gallery-description><p>Comparison of Contrast Sensitivity Functions (CSF) between different mechanistic models.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/adicionales/compResponses.webp","[Model Comparison - Response Curves](#b-principled-models)","Response curves of different vision models, adjusted to reproduce psychophysical phenomena.")'><img src=/images/adicionales/compResponses.webp alt="[Model Comparison - Response Curves](#b-principled-models)"></a><p class=gallery-title><a href=#b-principled-models>Model Comparison - Response Curves</a></p><div class=gallery-description><p>Response curves of different vision models, adjusted to reproduce psychophysical phenomena.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/adicionales/compNoise.webp","[Model Comparison - Noise Visibility](#b-principled-models)","Comparison of noise visibility in images across various vision models.")'><img src=/images/adicionales/compNoise.webp alt="[Model Comparison - Noise Visibility](#b-principled-models)"></a><p class=gallery-title><a href=#b-principled-models>Model Comparison - Noise Visibility</a></p><div class=gallery-description><p>Comparison of noise visibility in images across various vision models.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/adicionales/principled.webp","[Principled Models - Efficient Coding](#efficient-coding-in-mechanistic-models)","Example of how mechanistic models are adapted to natural image statistics for redundancy reduction.")'><img src=/images/adicionales/principled.webp alt="[Principled Models - Efficient Coding](#efficient-coding-in-mechanistic-models)"></a><p class=gallery-title><a href=#efficient-coding-in-mechanistic-models>Principled Models - Efficient Coding</a></p><div class=gallery-description><p>Example of how mechanistic models are adapted to natural image statistics for redundancy reduction.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/adicionales/autoencoder.webp","[Autoencoder for Optimal Representation](#efficient-coding-in-mechanistic-models)","Representation of an autoencoder optimized for unprecedented performance in image coding.")'><img src=/images/adicionales/autoencoder.webp alt="[Autoencoder for Optimal Representation](#efficient-coding-in-mechanistic-models)"></a><p class=gallery-title><a href=#efficient-coding-in-mechanistic-models>Autoencoder for Optimal Representation</a></p><div class=gallery-description><p>Representation of an autoencoder optimized for unprecedented performance in image coding.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/adicionales/LinearStats.webp","[Statistically-based Linear Receptive Fields](#statistically-based-linear-receptive-fields)","Linear receptive fields derived from statistical techniques such as PCA and ICA trained on natural scenes.")'><img src=/images/adicionales/LinearStats.webp alt="[Statistically-based Linear Receptive Fields](#statistically-based-linear-receptive-fields)"></a><p class=gallery-title><a href=#statistically-based-linear-receptive-fields>Statistically-based Linear Receptive Fields</a></p><div class=gallery-description><p>Linear receptive fields derived from statistical techniques such as PCA and ICA trained on natural scenes.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/adicionales/ResponsesSPCA1.webp","[SPCA Responses 1](#statistically-based-nonlinearities)","Sensory response based on Sequential Principal Curves Analysis (SPCA), showing sensors adapted to the nonlinear properties of the visual system.")'><img src=/images/adicionales/ResponsesSPCA1.webp alt="[SPCA Responses 1](#statistically-based-nonlinearities)"></a><p class=gallery-title><a href=#statistically-based-nonlinearities>SPCA Responses 1</a></p><div class=gallery-description><p>Sensory response based on Sequential Principal Curves Analysis (SPCA), showing sensors adapted to the nonlinear properties of the visual system.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/adicionales/ResponsesSPCA2.webp","[SPCA Responses 2](#statistically-based-nonlinearities)","Another illustration of sensory responses using SPCA techniques.")'><img src=/images/adicionales/ResponsesSPCA2.webp alt="[SPCA Responses 2](#statistically-based-nonlinearities)"></a><p class=gallery-title><a href=#statistically-based-nonlinearities>SPCA Responses 2</a></p><div class=gallery-description><p>Another illustration of sensory responses using SPCA techniques.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/adicionales/neuro_rbig.webp","[Gaussianization of Nonlinear Manifolds](#statistically-based-nonlinearities)","Use of Gaussianization to define optimal discrimination regions based on information maximization or error minimization.")'><img src=/images/adicionales/neuro_rbig.webp alt="[Gaussianization of Nonlinear Manifolds](#statistically-based-nonlinearities)"></a><p class=gallery-title><a href=#statistically-based-nonlinearities>Gaussianization of Nonlinear Manifolds</a></p><div class=gallery-description><p>Use of Gaussianization to define optimal discrimination regions based on information maximization or error minimization.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/adicionales/metricFisher.webp","[Fisher Information Metric for Vision Models](#statistically-based-nonlinearities)","Representation of the Fisher information concept and its application in evaluating nonlinear perception systems.")'><img src=/images/adicionales/metricFisher.webp alt="[Fisher Information Metric for Vision Models](#statistically-based-nonlinearities)"></a><p class=gallery-title><a href=#statistically-based-nonlinearities>Fisher Information Metric for Vision Models</a></p><div class=gallery-description><p>Representation of the Fisher information concept and its application in evaluating nonlinear perception systems.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/adicionales/flow_800_10_excel.gif","[Optical Flow for Video Coding](#perceptually-weighted-motion-estimation-vistavideocoding)","Perceptually enhanced optical flow used for predictive coding in video compression.")'><img src=/images/adicionales/flow_800_10_excel.gif alt="[Optical Flow for Video Coding](#perceptually-weighted-motion-estimation-vistavideocoding)"></a><p class=gallery-title><a href=#perceptually-weighted-motion-estimation-vistavideocoding>Optical Flow for Video Coding</a></p><div class=gallery-description><p>Perceptually enhanced optical flow used for predictive coding in video compression.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/adicionales/coding.webp","[Image Coding and Restoration](#image-coding-vistacore)","Image coding using vision models based on DCT and Divisive Normalization to improve the quality of compressed images.")'><img src=/images/adicionales/coding.webp alt="[Image Coding and Restoration](#image-coding-vistacore)"></a><p class=gallery-title><a href=#image-coding-vistacore>Image Coding and Restoration</a></p><div class=gallery-description><p>Image coding using vision models based on DCT and Divisive Normalization to improve the quality of compressed images.</p></div></div><div id=imageModal class=modal><span class=modal-close onclick=closeModal()>&#215;</span>
<img class=modal-content id=modalImage><div id=modalCaption class=modal-caption></div></div></div></aside><section class="box-references title2"><h2>References</h2><ul class=references-list><li><strong><a href=# target=_blank class=references-name>VistaLab: The Matlab Toolbox for Spatio-Temporal Vision. Univ. Valencia 1997</a></strong><br><span>J. Malo & J. Gutierrez</span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/ELECT98.PS.gz target=_blank class=references-name>Perceptually Weighted Optical Flow for Motion-based Segmentation in MPEG-4</a></strong><br><span>J. Malo, et al.</span><br><em>Electronics Letters 36(20): 1693-1694 (2000)</em></li><li><strong><a href=https://www.frontiersin.org/articles/10.3389/fnhum.2015.00557/full target=_blank class=references-name>Visual aftereffects and sensory nonlinearities from a single statistical framework</a></strong><br><span>V. Laparra & J. Malo</span><br><em></em></li></ul></section><section class="box-enlaces title2"><h2>Download</h2><ul class=enlaces-list><li><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/Vistalab.zip>Updated Matlab Toolbox (VISTALAB 4.0)</a></li><li><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/BasicVideoTools_code.zip>Outdated toolbox (VISTALAB 1.0)</a></li><li><a href=https://www.frontiersin.org/articles/10.3389/fnhum.2015.00557/full>Front. Human Neurosci. 15 paper</a></li><li><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/VistaVideoCoding.zip>Extensions of VISTALAB I: VistaVideoCoding</a></li><li><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/BioMultiLayer_L_NL_color.zip>Extensions of VISTALAB II: VistaModels</a></li><li><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/Colorlab.zip>Extensions of VISTALAB III: ColorLab</a></li></ul></section></div></div></main></body><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js integrity=sha384-C6RzsynM9kWDrMNeT87bh95OGNyZPhcTNXj1NW7RuBCsyN/o0jlpcV8Qyq46cDfL crossorigin=anonymous defer></script><script type=text/javascript src=https://cdn.jsdelivr.net/gh/pcooksey/bibtex-js@1.0.0/src/bibtex_js.min.js defer></script></html>