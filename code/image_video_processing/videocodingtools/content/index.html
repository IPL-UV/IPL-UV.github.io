<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>ISP - Image and Signal Processing group</title>
<link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css integrity="sha512-z3gLpd7yknf1YoNbCzqRKc4qyor8gaKU1qmn+CShxbuBusANI9QpRohGBreCFkKxLhei6S9CQXFEbbKuqLg0DA==" crossorigin=anonymous referrerpolicy=no-referrer><link rel="shortcut icon" href=/images/isp_ico.webp type=image/x-icon><link rel=stylesheet href=/style/style.css><script src=/js/mode.js></script><script src=https://cdn.jsdelivr.net/npm/marked/marked.min.js></script></head><nav class=custom-navbars><div class=custom-container><a href=/ class="custom-logo custom-hide-on-large"><img src=/images/isp_logo_sinfondo.webp alt="ISP Icon" class=custom-logo_nav>
<span class=custom-text-isp>ISP</span>
</a><button class=navbar-toggler aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class=custom-navbar-collapse><ul class=custom-navbar-nav><li class="custom-nav-item custom-desktop-only"><a class=custom-nav-link href=/>ISP</a></li><li class=custom-nav-item><a class=custom-nav-link href=/people/>People</a></li><li class="custom-nav-item custom-dropdown"><a class="custom-nav-link custom-dropdown-toggle" href=/research/philosophy>Research</a><ul class=custom-dropdown-menu><li><a class=custom-dropdown-item href=/research/machine_learning/>Machine learning</a></li><li><a class=custom-dropdown-item href=/research/visual_neuroscience/>Visual science</a></li><li><a class=custom-dropdown-item href=/research/visual_brain/>Image processing</a></li><li><a class=custom-dropdown-item href=/research/earth_science/>Earth science</a></li><li><a class=custom-dropdown-item href=/research/social_science>Social science</a></li></ul></li><li class=custom-nav-item><a class=custom-nav-link href=/projects/>Projects</a></li><li class=custom-nav-item><a class=custom-nav-link href=/facilities/>Facilities</a></li><li class="custom-nav-item custom-dropdown"><a class="custom-nav-link custom-dropdown-toggle" href=/publications/journals/>Publications</a><ul class=custom-dropdown-menu><li><a class=custom-dropdown-item href=/publications/journals/>Journals</a></li><li><a class=custom-dropdown-item href=/publications/conferences/>Conferences</a></li><li><a class=custom-dropdown-item href=/publications/books/>Books</a></li><li><a class=custom-dropdown-item href=/publications/talks/>Talks</a></li><li><a class=custom-dropdown-item href=/publications/technical_reports/>Technical Reports</a></li><li><a class=custom-dropdown-item href=/publications/theses/>Theses</a></li></ul></li><li class=custom-nav-item><a class=custom-nav-link href=/code/>Code</a></li><li class=custom-nav-item><a class=custom-nav-link href=/data/>Data</a></li><li class=custom-nav-item><a class=custom-nav-link href=/seminars/>Seminars</a></li><li class=custom-nav-item><a class=custom-nav-link href=/courses/>Courses</a></li><li class=custom-nav-item><a class=custom-nav-link href=/collaborators/>Collaborators</a></li><li class=custom-nav-item><a class=custom-nav-link href=/news/>News</a></li><li class=custom-nav-item><a class=custom-nav-link href=/contact/>Contact</a></li></ul></div></div></nav><main><div class=container><div class="content-container grid-layout"><div class=box-header><h1>Motion Estimation and Video Coding Toolbox</h1></div><div class=box-abstract><p><h1 id=motion-estimation>Motion Estimation</h1><p>Our approach to motion estimation in video sequences was motivated by the general scheme of the current video coders with motion compensation (such as MPEG-X or H.26X [Musmann85, LeGall91, Tekalp95]).</p><p>In motion compensation video coders the input sequence, <strong>A(t)</strong>, is analized by a motion estimation system, M, that computes some description of the motion in the scene: typically the optical flow, <strong>DVF(t)</strong>. In the motion compensation module, <strong>P</strong>, this motion information can be used to predict the current frame, <strong>A(t)</strong>, from previous frames, <strong>A(t-1)</strong>. As the prediction, <strong>(t)</strong>, is not perfect, additional information is needed to reconstruct the sequence: the prediction error <strong>DFD(t)</strong>. This scheme is useful for video compression because the entropy of these two sources (motion, DVF, and errors, <strong>DFD</strong>) is significantly smaller than the entropy of the original sequence <strong>A(t)</strong>.</p><p>The coding gain can be even bigger if the error sequence is analyzed, and quantized, in an appropriate transform domain, as done in image compression procedures, using the transform <strong>T</strong> and the quantizer <strong>Q</strong>.</p><p>Conventional optical flow techniques (based in local maximization of the correlation by block matching) provide a motion description that may be redundant for a human viewer. Computational effort may be wasted describing &lsquo;perceptually irrelevant motions&rsquo;. This inefficient behavior may also give rise to false alarms and noisy flows. To solve this problem, hierarchical optical flow techniques have been proposed (as for instance in MPEG-4 and in H.263). They start from a low resolution motion estimate and new motion information is locally added only in certain regions. However, new motion information should be added only if it is &lsquo;perceptually relevant&rsquo;. Our contribution in motion estimation is a definition of &lsquo;perceptually relevant motion information&rsquo; [Malo98, Malo01a, Malo01b]. This definition is based on the entropy of the image representation in the human cortex (Watson JOSA 87, Daugman IEEE T.Biom.Eng. 89): an increment in motion information is perceptually relevant if it contributes to decrease the entropy of the cortex representation of the prediction error. Numerical experiments (optical flow computation and flow-based segmentation) show that applying this definition to a particular hierarchical motion estimation algorithm, more robust and meaningful flows are obtained [Malo00b, Malo01a, Malo01b].</p><h1 id=video-coding>Video Coding</h1><p>As stated in the above scheme, the basic ingredients of motion compensation video coders are the motion estimation module, M, and the transform and quantization module, <strong>T+Q</strong>. Given our work in motion estimation and in image representation for efficient quantization, the improvement of the current video coding standards is straightforward. See [Malo01b] for a comprehensive review, and [Malo97b, Malo00a] for the original formulation and specific analysis of the relative relevance of M and <strong>T+Q</strong> in the video coding process.</p><p>Here is an example [Malo00a, Malo01b] of the relative gain in the reconstructed sequence (0.27 bits/pix) obtained from isolated improvements in motion estimation (<strong>M</strong>) and/or image representation and quantization (<strong>T+Q</strong>).</p><p>In the above distortion-per-frame plot, thick lines correspond to algorithms with poor (linear) quantization schemes and thin lines correspond to improved (non-linear) quantization schemes. Dashed lines correspond to algorithms with improved motion estimation schemes. The conclusion is that at the current bit rates an appropriate image representation and quantization is quite more important than improvements in motion estimation.</p></p></div><aside class=box-gallery><div class=gallery-grid><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/code/coderde.webp","[Motion Estimation Scheme](#motion-estimation)","Illustration of the video coder scheme with motion estimation and prediction error quantization.")'><img src=/images/code/coderde.webp alt="[Motion Estimation Scheme](#motion-estimation)"></a><p class=gallery-title><a href=#motion-estimation>Motion Estimation Scheme</a></p><div class=gallery-description><p>Illustration of the video coder scheme with motion estimation and prediction error quantization.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/code/coding.webp","[Video Coding Example](#video-coding)","Comparison of video coding schemes with different levels of motion estimation and quantization efficiency.")'><img src=/images/code/coding.webp alt="[Video Coding Example](#video-coding)"></a><p class=gallery-title><a href=#video-coding>Video Coding Example</a></p><div class=gallery-description><p>Comparison of video coding schemes with different levels of motion estimation and quantization efficiency.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/code/distort.webp","[Distortion per Frame](#video-coding)","Distortion per frame plot comparing algorithms with improved motion estimation and non-linear quantization schemes.")'><img src=/images/code/distort.webp alt="[Distortion per Frame](#video-coding)"></a><p class=gallery-title><a href=#video-coding>Distortion per Frame</a></p><div class=gallery-description><p>Distortion per frame plot comparing algorithms with improved motion estimation and non-linear quantization schemes.</p></div></div><div id=imageModal class=modal><span class=modal-close onclick=closeModal()>&#215;</span>
<img class=modal-content id=modalImage><div id=modalCaption class=modal-caption></div></div></div></aside><section class="box-references title2"><h2>References</h2><ul class=references-list><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/seg_ade2.ps target=_blank class=references-name>Perceptually weighted optical flow for motion-based segmentation in MPEG-4 paradigm</a></strong><br><span>J. Malo, J. Gutierrez, I. Epifanio, F. Ferri</span><br><em>Electronics Letters, Vol. 36, 20, pp. 1693-1694 (2000)</em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/ieeeoct01.pdf target=_blank class=references-name>Perceptual feed-back in multigrid motion estimation using an improved DCT quantization</a></strong><br><span>J. Malo, J. Gutierrez, I. Epifanio, F. Ferri, J.M. Artigas</span><br><em>IEEE Transactions on Image Processing, Vol. 10, 10, pp. 1411-1427 (2001)</em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/elect00.ps target=_blank class=references-name>Importance of quantizer design compared to optimal multigrid motion estimation in video coding</a></strong><br><span>J. Malo, F. Ferri, J. Gutierrez, I. Epifanio</span><br><em>Electronics Letters, Vol. 36, 9, pp. 807-809 (2000)</em></li></ul></section><section class="box-enlaces title2"><h2>Download</h2><ul class=enlaces-list><li><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/code/soft_visioncolor/Motion_estimation_and_Video%20coding_code.zip>Motion_estimation_and_Video coding_code.zip</a></li></ul></section></div></div></main></body><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js integrity=sha384-C6RzsynM9kWDrMNeT87bh95OGNyZPhcTNXj1NW7RuBCsyN/o0jlpcV8Qyq46cDfL crossorigin=anonymous defer></script></html>