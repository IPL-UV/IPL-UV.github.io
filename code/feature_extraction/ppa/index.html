<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>ISP - Image and Signal Processing group</title>
<link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css integrity="sha512-z3gLpd7yknf1YoNbCzqRKc4qyor8gaKU1qmn+CShxbuBusANI9QpRohGBreCFkKxLhei6S9CQXFEbbKuqLg0DA==" crossorigin=anonymous referrerpolicy=no-referrer><link rel="shortcut icon" href=/images/isp_ico.webp type=image/x-icon><link rel=stylesheet href=/style/style.css><script src=/js/mode.js></script><script src=https://cdn.jsdelivr.net/npm/marked/marked.min.js></script></head><nav class=custom-navbars><div class=custom-container><a href=/ class="custom-logo custom-hide-on-large"><img src=/images/isp_logo_sinfondo.webp alt="ISP Icon" class=custom-logo_nav>
<span class=custom-text-isp>ISP</span>
</a><button class=navbar-toggler aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class=custom-navbar-collapse><ul class=custom-navbar-nav><li class="custom-nav-item custom-desktop-only"><a class=custom-nav-link href=/>ISP</a></li><li class=custom-nav-item><a class=custom-nav-link href=/people/>People</a></li><li class="custom-nav-item custom-dropdown"><a class="custom-nav-link custom-dropdown-toggle" href=/research/philosophy/>Research</a><ul class=custom-dropdown-menu><li><a class=custom-dropdown-item href=/research/philosophy/>Philosophy</a></li><li><a class=custom-dropdown-item href=/research/machine_learning/>Machine learning</a></li><li><a class=custom-dropdown-item href=/research/visual_neuroscience/>Visual science</a></li><li><a class=custom-dropdown-item href=/research/visual_brain/>Image processing</a></li><li><a class=custom-dropdown-item href=/research/earth_science/>Earth science</a></li><li><a class=custom-dropdown-item href=/research/social_science>Social science</a></li></ul></li><li class=custom-nav-item><a class=custom-nav-link href=/projects/>Projects</a></li><li class=custom-nav-item><a class=custom-nav-link href=/facilities/>Facilities</a></li><li class="custom-nav-item custom-dropdown"><a class="custom-nav-link custom-dropdown-toggle" href=/publications/journals/>Publications</a><ul class=custom-dropdown-menu><li><a class=custom-dropdown-item href=/publications/journals/>Journals</a></li><li><a class=custom-dropdown-item href=/publications/conferences/>Conferences</a></li><li><a class=custom-dropdown-item href=/publications/books/>Books</a></li><li><a class=custom-dropdown-item href=/publications/talks/>Talks</a></li><li><a class=custom-dropdown-item href=/publications/theses/>Theses</a></li></ul></li><li class=custom-nav-item><a class=custom-nav-link href=/code/>Code</a></li><li class=custom-nav-item><a class=custom-nav-link href=/data/>Data</a></li><li class=custom-nav-item><a class=custom-nav-link href=/seminars/>Seminars</a></li><li class=custom-nav-item><a class=custom-nav-link href=/courses/>Courses</a></li><li class=custom-nav-item><a class=custom-nav-link href=/collaborators/>Collaborators</a></li><li class=custom-nav-item><a class=custom-nav-link href=/news/>News</a></li><li class=custom-nav-item><a class=custom-nav-link href=/contact/>Contact</a></li></ul></div></div></nav><main><div class=container><div class=content-container><div class=grid-container id=grid-container><div class=grid-item><a href><img src=/images/code alt="Principal Polynomial Analysis (PPA)"></a><div class=text><a href class=nameLink_a>Principal Polynomial Analysis (PPA)</a><p>This paper (and toolbox) presents a new framework for manifold learning based on the use of a sequence of principal polynomials that capture the eventually nonlinear nature of the data. The proposed Principal Polynomial Analysis (PPA) is shown to generalize PCA by admitting curves instead of straight lines. As opposed to previous approaches following the same rationale, PPA reduces to performing canonical, univariate regressions which make it computationally feasible and easy to interpret analytically. We show that the PPA transform is a volume-preserving map, which guarantees the existence of the inverse since the determinant of the Jacobian is bounded. We propose a closed-form solution for the inverse map. Invertibility is an important advantage over other nonlinear dimensionality reduction methods because it permits to understand the identified features in the input domain where data have physical meaning. Moreover, invertibility allows to evaluate the dimensionality reduction performance in sensible units. Preserving the volume also allows to compute the reduction in multi-information achieved by the transform using only marginal operations. Additionally, PPA leads to a clear geometrical interpretation of the manifold: the computation of Frenet-Serret frames along the identified curves allow us to obtain generalized curvature and torsion of the manifold. Moreover, the analytical expression of the Jacobian simplifies the computation of the metric induced by the data. Performance in dimensionality reduction and redundancy reduction, as well as the theoretical properties of PPA, are experimentally tested in datasets from the UCI machine learning repository.</p></div></div></div><p>Principal Polynomial Analysis (PPA) is a manifold learning technique that generalizes PCA by using principal polynomials to capture nonlinear data patterns. It improves PCAâ€™s energy compaction ability, reducing dimensionality reduction errors. PPA defines a manifold-dependent metric that generalizes Mahalanobis distance for curved manifolds.</p></div></div></main></body><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js integrity=sha384-C6RzsynM9kWDrMNeT87bh95OGNyZPhcTNXj1NW7RuBCsyN/o0jlpcV8Qyq46cDfL crossorigin=anonymous defer></script></html>