<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>ISP - Image and Signal Processing group</title>
<link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css integrity="sha512-z3gLpd7yknf1YoNbCzqRKc4qyor8gaKU1qmn+CShxbuBusANI9QpRohGBreCFkKxLhei6S9CQXFEbbKuqLg0DA==" crossorigin=anonymous referrerpolicy=no-referrer><link rel="shortcut icon" href=/images/isp_ico.webp type=image/x-icon><link rel=stylesheet href=/style/style.css><script src=/js/mode.js></script><script src=https://cdn.jsdelivr.net/npm/marked/marked.min.js></script></head><nav class=custom-navbars><div class=custom-container><a href=/ class="custom-logo custom-hide-on-large"><img src=/images/isp_logo_sinfondo.webp alt="ISP Icon" class=custom-logo_nav>
<span class=custom-text-isp>ISP</span>
</a><button class=navbar-toggler aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class=custom-navbar-collapse><ul class=custom-navbar-nav><li class="custom-nav-item custom-desktop-only"><a class=custom-nav-link href=/>ISP</a></li><li class=custom-nav-item><a class=custom-nav-link href=/people/>People</a></li><li class="custom-nav-item custom-dropdown"><a class="custom-nav-link custom-dropdown-toggle" href=/research/philosophy>Research</a><ul class=custom-dropdown-menu><li><a class=custom-dropdown-item href=/research/machine_learning/>Machine learning</a></li><li><a class=custom-dropdown-item href=/research/visual_neuroscience/>Visual science</a></li><li><a class=custom-dropdown-item href=/research/visual_brain/>Image processing</a></li><li><a class=custom-dropdown-item href=/research/earth_science/>Earth science</a></li><li><a class=custom-dropdown-item href=/research/social_science>Social science</a></li></ul></li><li class=custom-nav-item><a class=custom-nav-link href=/projects/>Projects</a></li><li class=custom-nav-item><a class=custom-nav-link href=/facilities/>Facilities</a></li><li class="custom-nav-item custom-dropdown"><a class="custom-nav-link custom-dropdown-toggle" href=/publications/journals/>Publications</a><ul class=custom-dropdown-menu><li><a class=custom-dropdown-item href=/publications/journals/>Journals</a></li><li><a class=custom-dropdown-item href=/publications/conferences/>Conferences</a></li><li><a class=custom-dropdown-item href=/publications/books/>Books</a></li><li><a class=custom-dropdown-item href=/publications/talks/>Talks</a></li><li><a class=custom-dropdown-item href=/publications/technical_reports/>Technical Reports</a></li><li><a class=custom-dropdown-item href=/publications/theses/>Theses</a></li></ul></li><li class=custom-nav-item><a class=custom-nav-link href=/code/>Code</a></li><li class=custom-nav-item><a class=custom-nav-link href=/data/>Data</a></li><li class=custom-nav-item><a class=custom-nav-link href=/seminars/>Seminars</a></li><li class=custom-nav-item><a class=custom-nav-link href=/courses/>Courses</a></li><li class=custom-nav-item><a class=custom-nav-link href=/collaborators/>Collaborators</a></li><li class=custom-nav-item><a class=custom-nav-link href=/news/>News</a></li><li class=custom-nav-item><a class=custom-nav-link href=/contact/>Contact</a></li></ul></div></div></nav><main><div class=container><div class="content-container grid-layout"><div class=box-header><h1>Image and Video Processing: Scene Statistics and Visual Neuroscience at work!</h1></div><div class=box-abstract><p><p>Efficient coding of visual information and efficient inference of missing information in images depend on two factors:</p><ul><li>The statistical structure of photographic images, and</li><li>The nature of the observer that will analyze the result.</li></ul><p>Interestingly, these two factors (image regularities and human vision) are deeply related since the evolution of biological sensors seems to be guided by statistical learning (see our work on the <em>Efficient Coding Hypothesis</em> in <a href=neuro.html>Visual Neuroscience</a>). However, the simultaneous consideration of these two factors is unusual in the image processing community, particularly beyond Gaussian image models and linear models of the observer.<br>Our work in image and video processing has been parallel to our investigation in describing the non-Gaussian nature of visual scenes and the nonlinear behavior of visual cortex. This parallel approach is sensible since these are two sides of the same issue in vision (<a href=https://en.wikipedia.org/wiki/Efficient_coding_hypothesis>the Efficient Coding Hypothesis again!</a>). Specifically, the core algorithm used in many applications has been the <a href=https://en.wikipedia.org/wiki/Normalization_model>Divisive Normalization</a>, a canonical computation in sensory neurons with interesting statistical effects (see <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Malo_Laparra_Neural_10b.pdf>Neur.Comp.10</a>).</p><p>We have used this perceptual (and also statistical) model to propose novel solutions in bit allocation, to identify perceptually relevant motion, to smooth image representations, and to compute distances between images.</p><h1 id=image-and-video-processing>Image and Video Processing</h1><p>Low level Image Processing (coding, restoration, synthesis, white balance, color and texture edition, etc&mldr;) is all about <em>image statistics</em> in a domain where <em>the metric is non-Euclidean</em> (i.e. induced by the data or the observer).</p><p>We proposed original image processing techniques using both perception models and image statistics including:</p><p>(i) improvements of JPEG standard for <strong>image coding</strong> through nonlinear texture vision models [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/ELECT95.PS.gz>Electr.Lett.95</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/ELECT99.PS.gz>Electr.Lett.99</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Gomez-Perez05_IEEETNN.pdf>IEEE TNN05</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/manuscript4.pdf>IEEE TIP06a</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Camps-Valls08_JMLR.pdf>JMLR08</a>,<a href=http://www.uv.es/gcamps/papers/paper_patent_6_review.pdf>RPSP12</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/patente_v5_jesus.pdf>Patent08</a>], (ii) improvements of MPEG standard for <strong>video coding</strong> with new perceptual quantization scheme and new motion estimation focused on perceptually relevant <strong>optical flow</strong> [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/LNCS97.PS.gz>LNCS97</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/ELECT98.PS.gz>Electr.Lett.98</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/elect00.ps>Electr.Lett.00a</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/seg_ade2.ps>Electr.Lett.00b</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/ieeeoct01.pdf>IEEE TIP01</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Redundancy_Reduction_Malo_99.pdf>Redund.Reduct.99</a>], (iii) new <strong>image restoration</strong> techniques based on nonlinear contrast perception models and the image statistics in local frequency domains [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/manuscript_TIP_00864_2004_R2.pdf>IEEE TIP 06b</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/laparra10a.pdf>JMLR10</a>], (iv) new approaches to <strong>color constancy</strong> either based on relative chromatic descriptors<br>[<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/VISRES97.PS.gz>Vis.Res.97</a>,<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/JOPT96.PS.gz>J.Opt.96</a>, statistically-based chromatic adaptation models <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Neco_accepted_2012.pdf>Neur.Comp.12</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Gutmann_PLOS_ONE_2014.pdf>PLoS-ONE14</a>, or Bayesian estimation of surface reflectance <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/manuscr_TGRS_2012_00431.pdf>IEEE-TGRS14</a>], (v) new subjective <strong>image and video distortion measures</strong> using nonlinear perception models [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/IVC97.PS.gz>Im.Vis.Comp.97</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/displays_99.pdf>Disp.99</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/icip02.pdf>IEEE ICIP02</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Laparra_JOSA_10.pdf>JOSA10</a>,<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/malo15a-reprint.pdf>Proc.SPIE15</a>], (vi) <strong>image classification</strong> and <strong>knowledge extraction</strong> (or regression) based on our feature extraction techniques [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Laparra11.pdf>IEEE-TNN11</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/AdaptVQ_ieeetgars_2012.pdf>IEEE-TGRS13</a>,<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/IJNS_Laparra14_accepted_v5.pdf>Int.J.Neur.Syst.14</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/drr_jstsp2014_final.pdf>IEEE-JSTSP15</a>]. See CODE for image and video processing applications <a href=../../../code/image_video_processing/>here</a>.</p></p></div><aside class=box-gallery><div class=gallery-grid><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/research/image_processing.webp","Image Processing","Controversies around using Mean Squared Error and images like &#39;Lena Sölderberg&#39;. Learn more about the MSE issue [here](../../../code/image_video_processing/vistaqualitytools/content/).")'><img src=/images/research/image_processing.webp alt="Image Processing"></a><p class=gallery-title>Image Processing</p><div class=gallery-description><p>Controversies around using Mean Squared Error and images like &lsquo;Lena Sölderberg&rsquo;. Learn more about the MSE issue <a href=../../../code/image_video_processing/vistaqualitytools/content/>here</a>.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/research/animated_coder.gif","Video Compression Model","MPEG-like video compression involves motion compensation and residual quantization. Vision Science and Statistical Learning can enhance these predictive coding methods.")'><img src=/images/research/animated_coder.gif alt="Video Compression Model"></a><p class=gallery-title>Video Compression Model</p><div class=gallery-description><p>MPEG-like video compression involves motion compensation and residual quantization. Vision Science and Statistical Learning can enhance these predictive coding methods.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/research/animated_video_coding.gif","Motion Estimation and Residual Quantization","Decoded sequences under different settings of Motion Estimation and Residual Quantization. Examples in [Electr.Lett.00a](https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/elect00.ps), [IEEE TIP01](https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/ieeeoct01.pdf).")'><img src=/images/research/animated_video_coding.gif alt="Motion Estimation and Residual Quantization"></a><p class=gallery-title>Motion Estimation and Residual Quantization</p><div class=gallery-description><p>Decoded sequences under different settings of Motion Estimation and Residual Quantization. Examples in <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/elect00.ps>Electr.Lett.00a</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/ieeeoct01.pdf>IEEE TIP01</a>.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/research/im_coding.webp","Image Coding","Using nonlinear perceptual image representations is critical to improving JPEG compression.")'><img src=/images/research/im_coding.webp alt="Image Coding"></a><p class=gallery-title>Image Coding</p><div class=gallery-description><p>Using nonlinear perceptual image representations is critical to improving JPEG compression.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/research/video_coding.webp","Video Coding","Improved bit allocation in MPEG video coding through nonlinear perception models.")'><img src=/images/research/video_coding.webp alt="Video Coding"></a><p class=gallery-title>Video Coding</p><div class=gallery-description><p>Improved bit allocation in MPEG video coding through nonlinear perception models.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/research/ruidos_great.webp","Image Restoration","Image restoration using regularization functionals based on nonlinear perception models and image smoothing in the wavelet domain.")'><img src=/images/research/ruidos_great.webp alt="Image Restoration"></a><p class=gallery-title>Image Restoration</p><div class=gallery-description><p>Image restoration using regularization functionals based on nonlinear perception models and image smoothing in the wavelet domain.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/research/flor1.webp","Color Constancy - Adaptation","Color constancy addressed through linear and nonlinear solutions to the geometric problem of manifold matching under different illumination conditions.")'><img src=/images/research/flor1.webp alt="Color Constancy - Adaptation"></a><p class=gallery-title>Color Constancy - Adaptation</p><div class=gallery-description><p>Color constancy addressed through linear and nonlinear solutions to the geometric problem of manifold matching under different illumination conditions.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/research/metrics.webp","Subjective Image/Video Metrics","Observer&#39;s opinion is better correlated with our Euclidean distance in nonlinear perceptual domains than with Structural Similarity Index.")'><img src=/images/research/metrics.webp alt="Subjective Image/Video Metrics"></a><p class=gallery-title>Subjective Image/Video Metrics</p><div class=gallery-description><p>Observer&rsquo;s opinion is better correlated with our Euclidean distance in nonlinear perceptual domains than with Structural Similarity Index.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/research/clasi1.webp","Image Classification - Feature Adaptation","Classifiers using RBIG, SPCA, PPA, DRR features are robust to changes in acquisition conditions.")'><img src=/images/research/clasi1.webp alt="Image Classification - Feature Adaptation"></a><p class=gallery-title>Image Classification - Feature Adaptation</p><div class=gallery-description><p>Classifiers using RBIG, SPCA, PPA, DRR features are robust to changes in acquisition conditions.</p></div></div><div id=imageModal class=modal><span class=modal-close onclick=closeModal()>&#215;</span>
<img class=modal-content id=modalImage><div id=modalCaption class=modal-caption></div></div></div></aside><section class="box-enlaces title2"><h2>Download</h2><ul class=enlaces-list><li><a href=../../code/image_video_processing/vistacore/content/>Vista Toolbox</a></li><li><a href=https://en.wikipedia.org/wiki/Efficient_coding_hypothesis>Efficient Coding Hypothesis</a></li><li><a href="https://www.acim.lafe.san.gva.es/acim/?page_id=1229">NeuroImage Unit</a></li><li><a href=../../code/vision_and_color/>Vision and Color Processing Software</a></li></ul></section></div></div></main></body><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js integrity=sha384-C6RzsynM9kWDrMNeT87bh95OGNyZPhcTNXj1NW7RuBCsyN/o0jlpcV8Qyq46cDfL crossorigin=anonymous defer></script><script type=text/javascript src=https://cdn.jsdelivr.net/gh/pcooksey/bibtex-js@1.0.0/src/bibtex_js.min.js defer></script></html>