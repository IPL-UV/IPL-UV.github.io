<!doctype html><html lang=en-us dir=ltr><head><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>ISP - Image and Signal Processing group</title>
<link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-T3c6CoIi6uLrA9TneNEoa7RxnatzjcDSCmG1MXxSR1GAsXEV/Dwwykc2MPK8M2HN crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css integrity="sha512-z3gLpd7yknf1YoNbCzqRKc4qyor8gaKU1qmn+CShxbuBusANI9QpRohGBreCFkKxLhei6S9CQXFEbbKuqLg0DA==" crossorigin=anonymous referrerpolicy=no-referrer><link rel="shortcut icon" href=http://isp.uv.es/favicon.ico type=image/x-icon><link rel=stylesheet href=/style/style.css><script src=/js/mode.js></script><script src=https://cdn.jsdelivr.net/npm/marked/marked.min.js></script></head></head><nav class="navbar navbar-expand-lg bg-body-tertiary fixed-top"><div class=container-fluid><a href=/ class="d-lg-none d-flex align-items-center a_logonav"><img src=/images/isp_logo_sinfondo.webp alt="ISP Icon" height=30 class=logo_nav>
<span class="ms-2 text-isp">ISP</span>
</a><button class="navbar-toggler ms-auto" type=button data-bs-toggle=collapse data-bs-target=#navbarTogglerDemo01 aria-controls=navbarTogglerDemo01 aria-expanded=false aria-label="Toggle navigation" style=height:40px>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarTogglerDemo01><ul class="navbar-nav mx-auto mb-lg-0"><li class="nav-item px-2 nav-item-highlight d-none d-lg-block"><a class="nav-link a" aria-current=page href=/>ISP</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/people/>People</a></li><li class="nav-item dropdown px-2 nav-item-highlight"><a class="nav-link dropdown-toggle a" href=/research/philosophy id=navbarDropdownResearch role=button aria-expanded=false>Research</a><ul class=dropdown-menu aria-labelledby=navbarDropdownResearch><li><a class="dropdown-item a" href=/research/machine_learning/>Machine learning</a></li><li><a class="dropdown-item a" href=/research/visual_neuroscience/>Visual neuroscience</a></li><li><a class="dropdown-item a" href=/research/visual_brain/>Visual brain</a></li><li><a class="dropdown-item a" href=/research/earth_science/>Earth science</a></li><li><a class="dropdown-item a" href=/research/social_science>Social science</a></li></ul></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/projects/>Projects</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/facilities/>Facilities</a></li><li class="nav-item dropdown px-2 nav-item-highlight"><a class="nav-link dropdown-toggle a" href=/publications/journals/ id=navbarDropdownPublications role=button aria-expanded=false>Publications</a><ul class=dropdown-menu aria-labelledby=navbarDropdownPublications><li><a class="dropdown-item a" href=/publications/journals/>Journals</a></li><li><a class="dropdown-item a" href=/publications/conferences/>Conferences</a></li><li><a class="dropdown-item a" href=/publications/books/>Books</a></li><li><a class="dropdown-item a" href=/publications/talks/>Talks</a></li><li><a class="dropdown-item a" href=/publications/technical_reports/>Technical Reports</a></li><li><a class="dropdown-item a" href=/publications/theses/>Theses</a></li></ul></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/code/>Code</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/data/>Data</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/seminars/>Seminars</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/courses/>Courses</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/collaborators/>Collaborators</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/news/>News</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/contact/>Contact</a></li></ul></div></div></nav><style>.navbar{--bs-navbar-padding-y:0rem !important;background-color:#222!important}.a{color:#949494!important}.a:hover{color:#fff!important}.nav-item-highlight{padding:.4rem}.nav-item-highlight:hover{background-color:#2d70aa!important}.dropdown-menu{background-color:#333!important;color:#fff!important;display:block}.dropdown-item{color:#949494!important}.dropdown-item:hover{background-color:#2d70aa!important;color:#fff!important}.navbar-nav li.nav-item .nav-link,.dropdown-menu .dropdown-item{transition:color .3s,background-color .3s}.nav-link.active,.dropdown-item.active{color:#fff!important;background-color:#007bff!important}.navbar-toggler-icon{background-image:url("data:image/svg+xml,%3csvg viewBox='0 0 30 30' xmlns='http://www.w3.org/2000/svg'%3e%3cpath stroke='white' stroke-width='2' stroke-linecap='round' stroke-miterlimit='10' d='M4 7h22M4 15h22M4 23h22'/%3e%3c/svg%3e")}.navbar-toggler{border:none}.navbar-toggler:focus{outline:none}@media(min-width:992px){.dropdown-menu{display:none}.dropdown:hover .dropdown-menu{display:block}}@media(max-width:991px){.navbar-nav{max-height:calc(100vh - 56px);overflow-y:auto}.navbar-nav::-webkit-scrollbar{width:9px}.navbar-nav::-webkit-scrollbar-thumb{background-color:#888;border-radius:10px}.navbar-nav::-webkit-scrollbar-thumb:hover{background-color:#555}.navbar-nav .nav-link{font-size:1.2rem}.dropdown-menu .dropdown-item{font-size:1rem}.dropdown-toggle::after{display:inline-block;margin-left:.255em;vertical-align:.255em;content:"";border-top:.3em solid;border-right:.3em solid transparent;border-bottom:0;border-left:.3em solid transparent}.navbar .dropdown-toggle::after{content:none!important}.navbar-toggler-icon{width:1.2em;height:1.2em}.navbar-toggler{border:none!important}.navbar-toggler:focus{box-shadow:none!important}.d-flex{height:45px}.a_logonav{text-decoration:none!important}.text-isp{font-size:1.3rem;color:#9d9d9d;display:inline-block;vertical-align:middle;text-decoration:none!important}.navbar-nav{max-height:calc(50vh - 56px);overflow-y:auto}}</style><script>document.addEventListener("DOMContentLoaded",function(){let e=document.querySelectorAll(".navbar .dropdown");e.forEach(function(e){let t=e.querySelector(".dropdown-toggle");t.addEventListener("click",function(e){window.innerWidth<992&&e.target===t&&(window.location.href=t.href)});let n=e.querySelectorAll(".dropdown-item");n.forEach(function(e){e.addEventListener("click",function(){window.location.href=e.href})})}),document.addEventListener("click",function(t){window.innerWidth<992&&!t.target.closest(".navbar .dropdown")&&e.forEach(function(e){e.querySelector(".dropdown-menu").classList.remove("show")})})})</script><main><div class=container><div class="content-container grid-layout"><div class=box-header><h1>Image and Video Processing: Scene Statistics and Visual Neuroscience at work!</h1></div><div class=box-abstract><p><p>Efficient coding of visual information and efficient inference of missing information in images depend on two factors:</p><ol><li>The statistical structure of photographic images, and</li><li>The nature of the observer that will analyze the result.</li></ol><p>Interestingly, these two factors (image regularities and human vision) are deeply related since the evolution of biological sensors seems to be guided by statistical learning (see our work on the <em>Efficient Coding Hypothesis</em> in <a href=neuro.html>Visual Neuroscience</a>). However, the simultaneous consideration of these two factors is unusual in the image processing community, particularly beyond Gaussian image models and linear models of the observer.<br>Our work in image and video processing has been parallel to our investigation in describing the non-Gaussian nature of visual scenes and the nonlinear behavior of visual cortex. This parallel approach is sensible since these are two sides of the same issue in vision (<a href=https://en.wikipedia.org/wiki/Efficient_coding_hypothesis>the Efficient Coding Hypothesis again!</a>). Specifically, the core algorithm used in many applications has been the <a href=https://en.wikipedia.org/wiki/Normalization_model>Divisive Normalization</a>, a canonical computation in sensory neurons with interesting statistical effects (see <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Malo_Laparra_Neural_10b.pdf>Neur.Comp.10</a>).</p><p>We have used this perceptual (and also statistical) model to propose novel solutions in bit allocation, to identify perceptually relevant motion, to smooth image representations, and to compute distances between images.</p><h1 id=image-and-video-processing>Image and Video Processing</h1><p>Low level Image Processing (coding, restoration, synthesis, white balance, color and texture edition, etc&mldr;) is all about <em>image statistics</em> in a domain where <em>the metric is non-Euclidean</em> (i.e. induced by the data or the observer).</p><p>We proposed original image processing techniques using both perception models and image statistics including:</p><p>(i) improvements of JPEG standard for <strong>image coding</strong> through nonlinear texture vision models <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/ELECT95.PS.gz>Electr.Lett.95</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/ELECT99.PS.gz>Electr.Lett.99</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Gomez-Perez05_IEEETNN.pdf>IEEE TNN05</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/manuscript4.pdf>IEEE TIP06a</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Camps-Valls08_JMLR.pdf>JMLR08</a>,<a href=http://www.uv.es/gcamps/papers/paper_patent_6_review.pdf>RPSP12</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/patente_v5_jesus.pdf>Patent08</a>, (ii) improvements of MPEG standard for <strong>video coding</strong> with new perceptual quantization scheme and new motion estimation focused on perceptually relevant <strong>optical flow</strong> <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/LNCS97.PS.gz>LNCS97</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/ELECT98.PS.gz>Electr.Lett.98</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/elect00.ps>Electr.Lett.00a</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/seg_ade2.ps>Electr.Lett.00b</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/ieeeoct01.pdf>IEEE TIP01</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Redundancy_Reduction_Malo_99.pdf>Redund.Reduct.99</a>, (iii) new <strong>image restoration</strong> techniques based on nonlinear contrast perception models and the image statistics in local frequency domains <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/manuscript_TIP_00864_2004_R2.pdf>IEEE TIP 06b</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/laparra10a.pdf>JMLR10</a>, (iv) new approaches to <strong>color constancy</strong> either based on relative chromatic descriptors<br><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/VISRES97.PS.gz>Vis.Res.97</a>,<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/JOPT96.PS.gz>J.Opt.96</a>, statistically-based chromatic adaptation models <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Neco_accepted_2012.pdf>Neur.Comp.12</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Gutmann_PLOS_ONE_2014.pdf>PLoS-ONE14</a>, or Bayesian estimation of surface reflectance <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/manuscr_TGRS_2012_00431.pdf>IEEE-TGRS14</a>, (v) new subjective <strong>image and video distortion measures</strong> using nonlinear perception models <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/IVC97.PS.gz>Im.Vis.Comp.97</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/displays_99.pdf>Disp.99</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/icip02.pdf>IEEE ICIP02</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Laparra_JOSA_10.pdf>JOSA10</a>,<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/malo15a-reprint.pdf>Proc.SPIE15</a>, (vi) <strong>image classification</strong> and <strong>knowledge extraction</strong> (or regression) based on our feature extraction techniques <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Laparra11.pdf>IEEE-TNN11</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/AdaptVQ_ieeetgars_2012.pdf>IEEE-TGRS13</a>,<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/IJNS_Laparra14_accepted_v5.pdf>Int.J.Neur.Syst.14</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/drr_jstsp2014_final.pdf>IEEE-JSTSP15</a>. See CODE for image and video processing applications <a href=/old_pages/code/soft_imvideo/ISP%20-%20Image%20and%20Video%20processing%20software.html>here</a>.</p></p></div><aside class=box-gallery><div class=gallery-grid><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/research/image_processing.webp","Image Processing","Controversies around using Mean Squared Error and images like &#39;Lena Sölderberg&#39;. Learn more about the MSE issue [here](/old_pages/code/soft_imvideo/subpages/vista_toolbox.html).")'><img src=/images/research/image_processing.webp alt="Image Processing"></a><p class=gallery-title>Image Processing</p><div class=gallery-description><p>Controversies around using Mean Squared Error and images like &lsquo;Lena Sölderberg&rsquo;. Learn more about the MSE issue <a href=/old_pages/code/soft_imvideo/subpages/vista_toolbox.html>here</a>.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/research/animated_coder.gif","Video Compression Model","MPEG-like video compression involves motion compensation and residual quantization. Vision Science and Statistical Learning can enhance these predictive coding methods.")'><img src=/images/research/animated_coder.gif alt="Video Compression Model"></a><p class=gallery-title>Video Compression Model</p><div class=gallery-description><p>MPEG-like video compression involves motion compensation and residual quantization. Vision Science and Statistical Learning can enhance these predictive coding methods.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/research/animated_video_coding.gif","Motion Estimation and Residual Quantization","Decoded sequences under different settings of Motion Estimation and Residual Quantization. Examples in [Electr.Lett.00a](https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/elect00.ps), [IEEE TIP01](https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/ieeeoct01.pdf).")'><img src=/images/research/animated_video_coding.gif alt="Motion Estimation and Residual Quantization"></a><p class=gallery-title>Motion Estimation and Residual Quantization</p><div class=gallery-description><p>Decoded sequences under different settings of Motion Estimation and Residual Quantization. Examples in <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/elect00.ps>Electr.Lett.00a</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/ieeeoct01.pdf>IEEE TIP01</a>.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/research/im_coding.webp","Image Coding","Using nonlinear perceptual image representations is critical to improving JPEG compression.")'><img src=/images/research/im_coding.webp alt="Image Coding"></a><p class=gallery-title>Image Coding</p><div class=gallery-description><p>Using nonlinear perceptual image representations is critical to improving JPEG compression.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/research/video_coding.webp","Video Coding","Improved bit allocation in MPEG video coding through nonlinear perception models.")'><img src=/images/research/video_coding.webp alt="Video Coding"></a><p class=gallery-title>Video Coding</p><div class=gallery-description><p>Improved bit allocation in MPEG video coding through nonlinear perception models.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/research/ruidos_great.webp","Image Restoration","Image restoration using regularization functionals based on nonlinear perception models and image smoothing in the wavelet domain.")'><img src=/images/research/ruidos_great.webp alt="Image Restoration"></a><p class=gallery-title>Image Restoration</p><div class=gallery-description><p>Image restoration using regularization functionals based on nonlinear perception models and image smoothing in the wavelet domain.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/research/flor1.webp","Color Constancy - Adaptation","Color constancy addressed through linear and nonlinear solutions to the geometric problem of manifold matching under different illumination conditions.")'><img src=/images/research/flor1.webp alt="Color Constancy - Adaptation"></a><p class=gallery-title>Color Constancy - Adaptation</p><div class=gallery-description><p>Color constancy addressed through linear and nonlinear solutions to the geometric problem of manifold matching under different illumination conditions.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/research/metrics.webp","Subjective Image/Video Metrics","Observer&#39;s opinion is better correlated with our Euclidean distance in nonlinear perceptual domains than with Structural Similarity Index.")'><img src=/images/research/metrics.webp alt="Subjective Image/Video Metrics"></a><p class=gallery-title>Subjective Image/Video Metrics</p><div class=gallery-description><p>Observer&rsquo;s opinion is better correlated with our Euclidean distance in nonlinear perceptual domains than with Structural Similarity Index.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/research/clasi1.webp","Image Classification - Feature Adaptation","Classifiers using RBIG, SPCA, PPA, DRR features are robust to changes in acquisition conditions.")'><img src=/images/research/clasi1.webp alt="Image Classification - Feature Adaptation"></a><p class=gallery-title>Image Classification - Feature Adaptation</p><div class=gallery-description><p>Classifiers using RBIG, SPCA, PPA, DRR features are robust to changes in acquisition conditions.</p></div></div><div id=imageModal class=modal><span class=modal-close onclick=closeModal()>&#215;</span>
<img class=modal-content id=modalImage><div id=modalCaption class=modal-caption></div></div></div></aside><section class="box-references title2"><h2>References</h2><ul class=references-list><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/ELECT95.PS.gz target=_blank class=references-name>Electr.Lett.95</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/ELECT99.PS.gz target=_blank class=references-name>Electr.Lett.99</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Gomez-Perez05_IEEETNN.pdf target=_blank class=references-name>IEEE TNN05</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/manuscript4.pdf target=_blank class=references-name>IEEE TIP06a</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Camps-Valls08_JMLR.pdf target=_blank class=references-name>JMLR08</a></strong><br><span></span><br><em></em></li><li><strong><a href=http://www.uv.es/gcamps/papers/paper_patent_6_review.pdf target=_blank class=references-name>RPSP12</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/patente_v5_jesus.pdf target=_blank class=references-name>Patent08</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/LNCS97.PS.gz target=_blank class=references-name>LNCS97</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/ELECT98.PS.gz target=_blank class=references-name>Electr.Lett.98</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/elect00.ps target=_blank class=references-name>Electr.Lett.00a</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/ieeeoct01.pdf target=_blank class=references-name>IEEE TIP01</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Redundancy_Reduction_Malo_99.pdf target=_blank class=references-name>Redund.Reduct.99</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/manuscript_TIP_00864_2004_R2.pdf target=_blank class=references-name>IEEE TIP 06b</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/laparra10a.pdf target=_blank class=references-name>JMLR10</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/VISRES97.PS.gz target=_blank class=references-name>Vis.Res.97</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/JOPT96.PS.gz target=_blank class=references-name>J.Opt.96</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Neco_accepted_2012.pdf target=_blank class=references-name>Neur.Comp.12</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Gutmann_PLOS_ONE_2014.pdf target=_blank class=references-name>PLoS-ONE14</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/manuscr_TGRS_2012_00431.pdf target=_blank class=references-name>IEEE-TGRS14</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/IVC97.PS.gz target=_blank class=references-name>Im.Vis.Comp.97</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/displays_99.pdf target=_blank class=references-name>Disp.99</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/icip02.pdf target=_blank class=references-name>IEEE ICIP02</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Laparra_JOSA_10.pdf target=_blank class=references-name>JOSA10</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/malo15a-reprint.pdf target=_blank class=references-name>Proc.SPIE15</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Laparra11.pdf target=_blank class=references-name>IEEE-TNN11</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/AdaptVQ_ieeetgars_2012.pdf target=_blank class=references-name>IEEE-TGRS13</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/IJNS_Laparra14_accepted_v5.pdf target=_blank class=references-name>Int.J.Neur.Syst.14</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/drr_jstsp2014_final.pdf target=_blank class=references-name>IEEE-JSTSP15</a></strong><br><span></span><br><em></em></li></ul></section><section class="box-enlaces title2"><h2>Download</h2><ul class=enlaces-list><li><a href=../../code/image_video_processing/vistacore/content/>Vista Toolbox</a></li><li><a href=https://en.wikipedia.org/wiki/Efficient_coding_hypothesis>Efficient Coding Hypothesis</a></li><li><a href="https://www.acim.lafe.san.gva.es/acim/?page_id=1229">NeuroImage Unit</a></li><li><a href=../../code/vision_and_color/>Vision and Color Processing Software</a></li></ul></section></div></div></main></body><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js integrity=sha384-C6RzsynM9kWDrMNeT87bh95OGNyZPhcTNXj1NW7RuBCsyN/o0jlpcV8Qyq46cDfL crossorigin=anonymous defer></script><script type=text/javascript src=https://cdn.jsdelivr.net/gh/pcooksey/bibtex-js@1.0.0/src/bibtex_js.min.js defer></script></html>