<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>ISP - Image and Signal Processing group</title>
<link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css integrity="sha512-z3gLpd7yknf1YoNbCzqRKc4qyor8gaKU1qmn+CShxbuBusANI9QpRohGBreCFkKxLhei6S9CQXFEbbKuqLg0DA==" crossorigin=anonymous referrerpolicy=no-referrer><link rel="shortcut icon" href=/github/images/favicon.ico type=image/x-icon><link rel=stylesheet href=/github/style/style.css><script src=/github/js/mode.js></script><script src=https://cdn.jsdelivr.net/npm/marked/marked.min.js></script></head><nav class=custom-navbars><div class=custom-container><a href=/ class="custom-logo custom-hide-on-large"><img src=/github/images/isp_logo_sinfondo.webp alt="ISP Icon" class=custom-logo_nav>
<span class=custom-text-isp>ISP</span>
</a><button class=navbar-toggler aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class=custom-navbar-collapse><ul class=custom-navbar-nav><li class="custom-nav-item custom-desktop-only"><a class=custom-nav-link href=/github/>ISP</a></li><li class=custom-nav-item><a class=custom-nav-link href=/github/people/>People</a></li><li class="custom-nav-item custom-dropdown"><a class="custom-nav-link custom-dropdown-toggle" href=/github/research/philosophy>Research</a><ul class=custom-dropdown-menu><li><a class=custom-dropdown-item href=/github/research/machine_learning/>Machine learning</a></li><li><a class=custom-dropdown-item href=/github/research/visual_neuroscience/>Visual neuroscience</a></li><li><a class=custom-dropdown-item href=/github/research/visual_brain/>Visual brain</a></li><li><a class=custom-dropdown-item href=/github/research/earth_science/>Earth science</a></li><li><a class=custom-dropdown-item href=/github/research/social_science>Social science</a></li></ul></li><li class=custom-nav-item><a class=custom-nav-link href=/github/projects/>Projects</a></li><li class=custom-nav-item><a class=custom-nav-link href=/github/facilities/>Facilities</a></li><li class="custom-nav-item custom-dropdown"><a class="custom-nav-link custom-dropdown-toggle" href=/github/publications/journals/>Publications</a><ul class=custom-dropdown-menu><li><a class=custom-dropdown-item href=/github/publications/journals/>Journals</a></li><li><a class=custom-dropdown-item href=/github/publications/conferences/>Conferences</a></li><li><a class=custom-dropdown-item href=/github/publications/books/>Books</a></li><li><a class=custom-dropdown-item href=/github/publications/talks/>Talks</a></li><li><a class=custom-dropdown-item href=/github/publications/technical_reports/>Technical Reports</a></li><li><a class=custom-dropdown-item href=/github/publications/theses/>Theses</a></li></ul></li><li class=custom-nav-item><a class=custom-nav-link href=/github/code/>Code</a></li><li class=custom-nav-item><a class=custom-nav-link href=/github/data/>Data</a></li><li class=custom-nav-item><a class=custom-nav-link href=/github/seminars/>Seminars</a></li><li class=custom-nav-item><a class=custom-nav-link href=/github/courses/>Courses</a></li><li class=custom-nav-item><a class=custom-nav-link href=/github/collaborators/>Collaborators</a></li><li class=custom-nav-item><a class=custom-nav-link href=/github/news/>News</a></li><li class=custom-nav-item><a class=custom-nav-link href=/github/contact/>Contact</a></li></ul></div></div></nav><style>.custom-navbars{background-color:#222;position:fixed;top:0;width:100%;display:flex;align-items:center;justify-content:space-between;padding:0 1rem;z-index:1000;height:3rem}.custom-container{display:flex;width:100%;height:100%;justify-content:center;align-items:center}.custom-logo{display:flex;align-items:center}.custom-logo_nav{height:30px}.custom-hide-on-large{display:none}.navbar-toggler{display:none}.navbar-toggler-icon{background-image:url("data:image/svg+xml,%3csvg viewBox='0 0 30 30' xmlns='http://www.w3.org/2000/svg'%3e%3cpath stroke='white' stroke-width='2' stroke-linecap='round' stroke-miterlimit='10' d='M4 7h22M4 15h22M4 23h22'/%3e%3c/svg%3e")}.custom-navbar-collapse{flex-grow:.95;height:100%;align-content:center}.custom-navbar-nav{list-style:none;display:flex;flex-direction:row;padding:0;margin:0;height:100%}.custom-nav-item{flex:auto;text-align:center;border:2px solid transparent;border-radius:8px;height:100%;display:flex;align-items:center;justify-content:center}.custom-nav-item:hover{background-color:#a0a0a0;border-color:#fff}.custom-nav-link{text-decoration:none;color:#f8f8f8;font-size:clamp(1rem,1.1vw,1.1rem);position:relative;display:block}.custom-dropdown-menu{position:absolute;background-color:rgba(46,46,46,.9);text-decoration:none;display:none;list-style:none;padding:.5rem 0;margin:0;border-radius:5px;font-size:clamp(1rem,1.1vw,1.1rem);top:3rem}.custom-dropdown-toggle::after{content:"â–¼";font-size:.5rem;margin-left:.3rem;color:#f8f8f8;display:inline-block;vertical-align:middle}.custom-dropdown-item{padding:0 1rem;color:#f8f8f8;text-decoration:none;margin:0;display:block;border:2px solid transparent;border-radius:8px}.custom-dropdown-item:hover{background-color:#a0a0a0;border-color:#fff}.custom-dropdown:hover .custom-dropdown-menu{display:block}.custom-nav-item.active{background-color:#464646}.custom-dropdown-item.active{background-color:#535353}@media(max-width:1070px){.custom-container{display:flex;width:100%;justify-content:space-between}.navbar-toggler{display:block;position:absolute;right:1rem;top:.5rem}.custom-hide-on-large{display:flex;align-items:center;text-decoration:none}.custom-navbar-collapse{display:none;flex-direction:column;width:100%;background-color:rgba(46,46,46,.9);position:absolute;top:3rem;left:0;z-index:999;height:auto;overflow:hidden;max-height:0;transition:max-height .3s ease-out;padding:.5rem 0;border-bottom-left-radius:.5rem;border-bottom-right-radius:.5rem}.custom-navbar-collapse.show{display:flex;max-height:100vh;overflow-y:auto}.custom-navbar-nav{flex-direction:column;align-items:center;width:100%}.custom-nav-item{width:100%;margin:0;display:block}.custom-nav-item a{text-align:left;margin-left:10%}.custom-text-isp{margin-left:.5rem;font-size:1rem;text-decoration:none;color:#f8f8f8}.custom-nav-link{width:100%;text-align:center}.custom-dropdown-menu{position:static;display:none;width:100%;background-color:#333}.custom-dropdown:hover .custom-dropdown-menu,.custom-dropdown .custom-dropdown-menu.show{display:block}}</style><script>document.addEventListener("DOMContentLoaded",function(){const n=document.querySelector(".navbar-toggler"),t=document.querySelector(".custom-navbar-collapse"),e=window.location.pathname;n.addEventListener("click",function(){t.classList.toggle("show")}),document.addEventListener("click",function(e){e.target.closest(".custom-navbars")||t.classList.remove("show")});const s=document.querySelectorAll(".custom-nav-link");s.forEach(t=>{const n=new URL(t.href).pathname;n===e&&e!=="/"&&t.closest(".custom-nav-item").classList.add("active")});const o=document.querySelectorAll(".custom-dropdown-item");o.forEach(t=>{const n=new URL(t.href).pathname;if(n===e){t.classList.add("active");const e=t.closest(".custom-dropdown");e&&e.classList.add("active")}});const i=document.querySelectorAll(".custom-dropdown");i.forEach(t=>{const n=t.querySelectorAll(".custom-dropdown-item");n.forEach(n=>{const s=new URL(n.href).pathname;s===e&&t.classList.add("active")})})})</script><main><div class=container><div class="content-container grid-layout"><div class=box-header><h1>Image and Video Processing: Scene Statistics and Visual Neuroscience at work!</h1></div><div class=box-abstract><p><p>Efficient coding of visual information and efficient inference of missing information in images depend on two factors:</p><ol><li>The statistical structure of photographic images, and</li><li>The nature of the observer that will analyze the result.</li></ol><p>Interestingly, these two factors (image regularities and human vision) are deeply related since the evolution of biological sensors seems to be guided by statistical learning (see our work on the <em>Efficient Coding Hypothesis</em> in <a href=neuro.html>Visual Neuroscience</a>). However, the simultaneous consideration of these two factors is unusual in the image processing community, particularly beyond Gaussian image models and linear models of the observer.<br>Our work in image and video processing has been parallel to our investigation in describing the non-Gaussian nature of visual scenes and the nonlinear behavior of visual cortex. This parallel approach is sensible since these are two sides of the same issue in vision (<a href=https://en.wikipedia.org/wiki/Efficient_coding_hypothesis>the Efficient Coding Hypothesis again!</a>). Specifically, the core algorithm used in many applications has been the <a href=https://en.wikipedia.org/wiki/Normalization_model>Divisive Normalization</a>, a canonical computation in sensory neurons with interesting statistical effects (see <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Malo_Laparra_Neural_10b.pdf>Neur.Comp.10</a>).</p><p>We have used this perceptual (and also statistical) model to propose novel solutions in bit allocation, to identify perceptually relevant motion, to smooth image representations, and to compute distances between images.</p><h1 id=image-and-video-processing>Image and Video Processing</h1><p>Low level Image Processing (coding, restoration, synthesis, white balance, color and texture edition, etc&mldr;) is all about <em>image statistics</em> in a domain where <em>the metric is non-Euclidean</em> (i.e. induced by the data or the observer).</p><p>We proposed original image processing techniques using both perception models and image statistics including:</p><p>(i) improvements of JPEG standard for <strong>image coding</strong> through nonlinear texture vision models <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/ELECT95.PS.gz>Electr.Lett.95</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/ELECT99.PS.gz>Electr.Lett.99</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Gomez-Perez05_IEEETNN.pdf>IEEE TNN05</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/manuscript4.pdf>IEEE TIP06a</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Camps-Valls08_JMLR.pdf>JMLR08</a>,<a href=http://www.uv.es/gcamps/papers/paper_patent_6_review.pdf>RPSP12</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/patente_v5_jesus.pdf>Patent08</a>, (ii) improvements of MPEG standard for <strong>video coding</strong> with new perceptual quantization scheme and new motion estimation focused on perceptually relevant <strong>optical flow</strong> <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/LNCS97.PS.gz>LNCS97</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/ELECT98.PS.gz>Electr.Lett.98</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/elect00.ps>Electr.Lett.00a</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/seg_ade2.ps>Electr.Lett.00b</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/ieeeoct01.pdf>IEEE TIP01</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Redundancy_Reduction_Malo_99.pdf>Redund.Reduct.99</a>, (iii) new <strong>image restoration</strong> techniques based on nonlinear contrast perception models and the image statistics in local frequency domains <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/manuscript_TIP_00864_2004_R2.pdf>IEEE TIP 06b</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/laparra10a.pdf>JMLR10</a>, (iv) new approaches to <strong>color constancy</strong> either based on relative chromatic descriptors<br><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/VISRES97.PS.gz>Vis.Res.97</a>,<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/JOPT96.PS.gz>J.Opt.96</a>, statistically-based chromatic adaptation models <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Neco_accepted_2012.pdf>Neur.Comp.12</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Gutmann_PLOS_ONE_2014.pdf>PLoS-ONE14</a>, or Bayesian estimation of surface reflectance <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/manuscr_TGRS_2012_00431.pdf>IEEE-TGRS14</a>, (v) new subjective <strong>image and video distortion measures</strong> using nonlinear perception models <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/IVC97.PS.gz>Im.Vis.Comp.97</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/displays_99.pdf>Disp.99</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/icip02.pdf>IEEE ICIP02</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Laparra_JOSA_10.pdf>JOSA10</a>,<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/malo15a-reprint.pdf>Proc.SPIE15</a>, (vi) <strong>image classification</strong> and <strong>knowledge extraction</strong> (or regression) based on our feature extraction techniques <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Laparra11.pdf>IEEE-TNN11</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/AdaptVQ_ieeetgars_2012.pdf>IEEE-TGRS13</a>,<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/IJNS_Laparra14_accepted_v5.pdf>Int.J.Neur.Syst.14</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/drr_jstsp2014_final.pdf>IEEE-JSTSP15</a>. See CODE for image and video processing applications <a href=../../../code/image_video_processing/>here</a>.</p></p></div><aside class=box-gallery><div class=gallery-grid><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/github/images/research/image_processing.webp","Image Processing","Controversies around using Mean Squared Error and images like &#39;Lena SÃ¶lderberg&#39;. Learn more about the MSE issue [here](../../../code/image_video_processing/vistaqualitytools/content/).")'><img src=/github/images/research/image_processing.webp alt="Image Processing"></a><p class=gallery-title>Image Processing</p><div class=gallery-description><p>Controversies around using Mean Squared Error and images like &lsquo;Lena SÃ¶lderberg&rsquo;. Learn more about the MSE issue <a href=../../../code/image_video_processing/vistaqualitytools/content/>here</a>.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/github/images/research/animated_coder.gif","Video Compression Model","MPEG-like video compression involves motion compensation and residual quantization. Vision Science and Statistical Learning can enhance these predictive coding methods.")'><img src=/github/images/research/animated_coder.gif alt="Video Compression Model"></a><p class=gallery-title>Video Compression Model</p><div class=gallery-description><p>MPEG-like video compression involves motion compensation and residual quantization. Vision Science and Statistical Learning can enhance these predictive coding methods.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/github/images/research/animated_video_coding.gif","Motion Estimation and Residual Quantization","Decoded sequences under different settings of Motion Estimation and Residual Quantization. Examples in [Electr.Lett.00a](https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/elect00.ps), [IEEE TIP01](https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/ieeeoct01.pdf).")'><img src=/github/images/research/animated_video_coding.gif alt="Motion Estimation and Residual Quantization"></a><p class=gallery-title>Motion Estimation and Residual Quantization</p><div class=gallery-description><p>Decoded sequences under different settings of Motion Estimation and Residual Quantization. Examples in <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/elect00.ps>Electr.Lett.00a</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/ieeeoct01.pdf>IEEE TIP01</a>.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/github/images/research/im_coding.webp","Image Coding","Using nonlinear perceptual image representations is critical to improving JPEG compression.")'><img src=/github/images/research/im_coding.webp alt="Image Coding"></a><p class=gallery-title>Image Coding</p><div class=gallery-description><p>Using nonlinear perceptual image representations is critical to improving JPEG compression.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/github/images/research/video_coding.webp","Video Coding","Improved bit allocation in MPEG video coding through nonlinear perception models.")'><img src=/github/images/research/video_coding.webp alt="Video Coding"></a><p class=gallery-title>Video Coding</p><div class=gallery-description><p>Improved bit allocation in MPEG video coding through nonlinear perception models.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/github/images/research/ruidos_great.webp","Image Restoration","Image restoration using regularization functionals based on nonlinear perception models and image smoothing in the wavelet domain.")'><img src=/github/images/research/ruidos_great.webp alt="Image Restoration"></a><p class=gallery-title>Image Restoration</p><div class=gallery-description><p>Image restoration using regularization functionals based on nonlinear perception models and image smoothing in the wavelet domain.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/github/images/research/flor1.webp","Color Constancy - Adaptation","Color constancy addressed through linear and nonlinear solutions to the geometric problem of manifold matching under different illumination conditions.")'><img src=/github/images/research/flor1.webp alt="Color Constancy - Adaptation"></a><p class=gallery-title>Color Constancy - Adaptation</p><div class=gallery-description><p>Color constancy addressed through linear and nonlinear solutions to the geometric problem of manifold matching under different illumination conditions.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/github/images/research/metrics.webp","Subjective Image/Video Metrics","Observer&#39;s opinion is better correlated with our Euclidean distance in nonlinear perceptual domains than with Structural Similarity Index.")'><img src=/github/images/research/metrics.webp alt="Subjective Image/Video Metrics"></a><p class=gallery-title>Subjective Image/Video Metrics</p><div class=gallery-description><p>Observer&rsquo;s opinion is better correlated with our Euclidean distance in nonlinear perceptual domains than with Structural Similarity Index.</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/github/images/research/clasi1.webp","Image Classification - Feature Adaptation","Classifiers using RBIG, SPCA, PPA, DRR features are robust to changes in acquisition conditions.")'><img src=/github/images/research/clasi1.webp alt="Image Classification - Feature Adaptation"></a><p class=gallery-title>Image Classification - Feature Adaptation</p><div class=gallery-description><p>Classifiers using RBIG, SPCA, PPA, DRR features are robust to changes in acquisition conditions.</p></div></div><div id=imageModal class=modal><span class=modal-close onclick=closeModal()>&#215;</span>
<img class=modal-content id=modalImage><div id=modalCaption class=modal-caption></div></div></div></aside><section class="box-references title2"><h2>References</h2><ul class=references-list><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/ELECT95.PS.gz target=_blank class=references-name>Electr.Lett.95</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/ELECT99.PS.gz target=_blank class=references-name>Electr.Lett.99</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Gomez-Perez05_IEEETNN.pdf target=_blank class=references-name>IEEE TNN05</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/manuscript4.pdf target=_blank class=references-name>IEEE TIP06a</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Camps-Valls08_JMLR.pdf target=_blank class=references-name>JMLR08</a></strong><br><span></span><br><em></em></li><li><strong><a href=http://www.uv.es/gcamps/papers/paper_patent_6_review.pdf target=_blank class=references-name>RPSP12</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/patente_v5_jesus.pdf target=_blank class=references-name>Patent08</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/LNCS97.PS.gz target=_blank class=references-name>LNCS97</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/ELECT98.PS.gz target=_blank class=references-name>Electr.Lett.98</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/elect00.ps target=_blank class=references-name>Electr.Lett.00a</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/ieeeoct01.pdf target=_blank class=references-name>IEEE TIP01</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Redundancy_Reduction_Malo_99.pdf target=_blank class=references-name>Redund.Reduct.99</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/manuscript_TIP_00864_2004_R2.pdf target=_blank class=references-name>IEEE TIP 06b</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/laparra10a.pdf target=_blank class=references-name>JMLR10</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/VISRES97.PS.gz target=_blank class=references-name>Vis.Res.97</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/JOPT96.PS.gz target=_blank class=references-name>J.Opt.96</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Neco_accepted_2012.pdf target=_blank class=references-name>Neur.Comp.12</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Gutmann_PLOS_ONE_2014.pdf target=_blank class=references-name>PLoS-ONE14</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/manuscr_TGRS_2012_00431.pdf target=_blank class=references-name>IEEE-TGRS14</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/IVC97.PS.gz target=_blank class=references-name>Im.Vis.Comp.97</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/displays_99.pdf target=_blank class=references-name>Disp.99</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/icip02.pdf target=_blank class=references-name>IEEE ICIP02</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Laparra_JOSA_10.pdf target=_blank class=references-name>JOSA10</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/malo15a-reprint.pdf target=_blank class=references-name>Proc.SPIE15</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/Laparra11.pdf target=_blank class=references-name>IEEE-TNN11</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/AdaptVQ_ieeetgars_2012.pdf target=_blank class=references-name>IEEE-TGRS13</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/IJNS_Laparra14_accepted_v5.pdf target=_blank class=references-name>Int.J.Neur.Syst.14</a></strong><br><span></span><br><em></em></li><li><strong><a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_brain/drr_jstsp2014_final.pdf target=_blank class=references-name>IEEE-JSTSP15</a></strong><br><span></span><br><em></em></li></ul></section><section class="box-enlaces title2"><h2>Download</h2><ul class=enlaces-list><li><a href=../../code/image_video_processing/vistacore/content/>Vista Toolbox</a></li><li><a href=https://en.wikipedia.org/wiki/Efficient_coding_hypothesis>Efficient Coding Hypothesis</a></li><li><a href="https://www.acim.lafe.san.gva.es/acim/?page_id=1229">NeuroImage Unit</a></li><li><a href=../../code/vision_and_color/>Vision and Color Processing Software</a></li></ul></section></div></div></main></body><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js integrity=sha384-C6RzsynM9kWDrMNeT87bh95OGNyZPhcTNXj1NW7RuBCsyN/o0jlpcV8Qyq46cDfL crossorigin=anonymous defer></script><script type=text/javascript src=https://cdn.jsdelivr.net/gh/pcooksey/bibtex-js@1.0.0/src/bibtex_js.min.js defer></script></html>