<!doctype html><html lang=en-us dir=ltr><head><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>ISP - Image and Signal Processing group</title>
<link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-T3c6CoIi6uLrA9TneNEoa7RxnatzjcDSCmG1MXxSR1GAsXEV/Dwwykc2MPK8M2HN crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css integrity="sha512-z3gLpd7yknf1YoNbCzqRKc4qyor8gaKU1qmn+CShxbuBusANI9QpRohGBreCFkKxLhei6S9CQXFEbbKuqLg0DA==" crossorigin=anonymous referrerpolicy=no-referrer><link rel="shortcut icon" href=http://isp.uv.es/favicon.ico type=image/x-icon><link rel=stylesheet href=/style/style.css><script src=/js/mode.js></script><script src=https://cdn.jsdelivr.net/npm/marked/marked.min.js></script></head></head><nav class="navbar navbar-expand-lg bg-body-tertiary fixed-top"><div class=container-fluid><a href=/ class="d-lg-none d-flex align-items-center a_logonav"><img src=/images/isp_logo_sinfondo.webp alt="ISP Icon" height=30 class=logo_nav>
<span class="ms-2 text-isp">ISP</span>
</a><button class="navbar-toggler ms-auto" type=button data-bs-toggle=collapse data-bs-target=#navbarTogglerDemo01 aria-controls=navbarTogglerDemo01 aria-expanded=false aria-label="Toggle navigation" style=height:40px>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarTogglerDemo01><ul class="navbar-nav mx-auto mb-lg-0"><li class="nav-item px-2 nav-item-highlight d-none d-lg-block"><a class="nav-link a" aria-current=page href=/>ISP</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/people/>People</a></li><li class="nav-item dropdown px-2 nav-item-highlight"><a class="nav-link dropdown-toggle a" href=/research/ id=navbarDropdownResearch role=button aria-expanded=false>Research</a><ul class=dropdown-menu aria-labelledby=navbarDropdownResearch><li><a class="dropdown-item a" href=/research/machine_learning/>Machine learning</a></li><li><a class="dropdown-item a" href=/research/visual_neuroscience/>Visual neuroscience</a></li><li><a class="dropdown-item a" href=/research/visual_brain/>Visual brain</a></li><li><a class="dropdown-item a" href=/research/earth_science/>Earth science</a></li><li><a class="dropdown-item a" href=/research/social_science>Social science</a></li></ul></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/projects/>Projects</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/facilities/>Facilities</a></li><li class="nav-item dropdown px-2 nav-item-highlight"><a class="nav-link dropdown-toggle a" href=/publications/journals/ id=navbarDropdownPublications role=button aria-expanded=false>Publications</a><ul class=dropdown-menu aria-labelledby=navbarDropdownPublications><li><a class="dropdown-item a" href=/publications/journals/>Journals</a></li><li><a class="dropdown-item a" href=/publications/conferences/>Conferences</a></li><li><a class="dropdown-item a" href=/publications/books/>Books</a></li><li><a class="dropdown-item a" href=/publications/talks/>Talks</a></li><li><a class="dropdown-item a" href=/publications/technical_reports/>Technical Reports</a></li><li><a class="dropdown-item a" href=/publications/theses/>Theses</a></li></ul></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/code/>Code</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/data/>Data</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/seminars/>Seminars</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/courses/>Courses</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/collaborators/>Collaborators</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/news/>News</a></li><li class="nav-item px-2 nav-item-highlight"><a class="nav-link a" aria-current=page href=/contact/>Contact</a></li></ul></div></div></nav><style>.navbar{--bs-navbar-padding-y:0rem !important;background-color:#222!important}.a{color:#949494!important}.a:hover{color:#fff!important}.nav-item-highlight{padding:.4rem}.nav-item-highlight:hover{background-color:#2d70aa!important}.dropdown-menu{background-color:#333!important;color:#fff!important;display:block}.dropdown-item{color:#949494!important}.dropdown-item:hover{background-color:#2d70aa!important;color:#fff!important}.navbar-nav li.nav-item .nav-link,.dropdown-menu .dropdown-item{transition:color .3s,background-color .3s}.nav-link.active,.dropdown-item.active{color:#fff!important;background-color:#007bff!important}.navbar-toggler-icon{background-image:url("data:image/svg+xml,%3csvg viewBox='0 0 30 30' xmlns='http://www.w3.org/2000/svg'%3e%3cpath stroke='white' stroke-width='2' stroke-linecap='round' stroke-miterlimit='10' d='M4 7h22M4 15h22M4 23h22'/%3e%3c/svg%3e")}.navbar-toggler{border:none}.navbar-toggler:focus{outline:none}@media(min-width:992px){.dropdown-menu{display:none}.dropdown:hover .dropdown-menu{display:block}}@media(max-width:991px){.navbar-nav{max-height:calc(100vh - 56px);overflow-y:auto}.navbar-nav::-webkit-scrollbar{width:9px}.navbar-nav::-webkit-scrollbar-thumb{background-color:#888;border-radius:10px}.navbar-nav::-webkit-scrollbar-thumb:hover{background-color:#555}.navbar-nav .nav-link{font-size:1.2rem}.dropdown-menu .dropdown-item{font-size:1rem}.dropdown-toggle::after{display:inline-block;margin-left:.255em;vertical-align:.255em;content:"";border-top:.3em solid;border-right:.3em solid transparent;border-bottom:0;border-left:.3em solid transparent}.navbar .dropdown-toggle::after{content:none!important}.navbar-toggler-icon{width:1.2em;height:1.2em}.navbar-toggler{border:none!important}.navbar-toggler:focus{box-shadow:none!important}.d-flex{height:45px}.a_logonav{text-decoration:none!important}.text-isp{font-size:1.3rem;color:#9d9d9d;display:inline-block;vertical-align:middle;text-decoration:none!important}.navbar-nav{max-height:calc(50vh - 56px);overflow-y:auto}}</style><script>document.addEventListener("DOMContentLoaded",function(){let e=document.querySelectorAll(".navbar .dropdown");e.forEach(function(e){let t=e.querySelector(".dropdown-toggle");t.addEventListener("click",function(e){window.innerWidth<992&&e.target===t&&(window.location.href=t.href)});let n=e.querySelectorAll(".dropdown-item");n.forEach(function(e){e.addEventListener("click",function(){window.location.href=e.href})})}),document.addEventListener("click",function(t){window.innerWidth<992&&!t.target.closest(".navbar .dropdown")&&e.forEach(function(e){e.querySelector(".dropdown-menu").classList.remove("show")})})})</script><main><div class=container><div class=text-center><p><h1 style=font-size:1.7em>Vision Science:</h1></p><p><h1 style=font-style:italic;margin-top:-20px>from Optics to Neuroscience and Statistical Learning</h1></p></div><div class=row><div class="col-md-2 uno"><img alt=Jesus class=img-responsive src=/images/research/visual_neuroscience/vision_science.webp><p align=justify><small>The PSF of this <span style=font-style:italic>nice</span> human eye can be measured <span style=font-style:italic>in-vivo</span> [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/OPH97.PS.gz>Opth.Phys.Opt.97</a>].</small></p><div><img class=img-responsive src=/images/adicionales/anim_gif_v1_resp.gif style=width:130%;margin-left:-50px><small>Linear V1 cells
(see model here [<a href=/old_pages/data/after_effects/>Front.Neurosci.15</a>]).</small><br><br><img class=img-responsive src=/images/adicionales/anim_gif_mt_resp.gif style=width:130%;margin-left:-50px><small>Linear response of MT cells<br>(and see the CODE for V1 and MT cells <a href=/old_pages/code/soft_visioncolor/subpages/virtual_labs.html><span style=font-style:italic>HERE</span></a>)</small></div></div><div class="col-md-8 dos"><p align=justify><strong>Vision</strong> is the ability to interpret the surrounding environment by analyzing the measurements drawn by imaging systems.
This ability is particularly impressive in <span style=font-style:italic>humans</span> when compared to the current state of the art in <span style=font-style:italic>computers</span> .<br>The study of all the phenomena related to <span style=font-style:italic>vision in biological systems</span> (and particularly in humans) is usually referred to as <strong>Vision Science</strong>.
It addresses a variety of issues ranging from the formation of the visual signal (e.g. the Physics of the imaging process that includes Radiometry and <strong>Physiological Optics</strong>),
to the analysis of the visual signal (of interest for Neuroscience and Psychology).
This analysis involves the extraction of visual primitives through basic computations in the retina-cortex neural pathway and the information processing leading to scene descriptors of higher abstraction level (<a href=http://www.scholarpedia.org/article/Models_of_visual_cortex>see elsewhere</a>). These problems may be addressed from a <span style=font-style:italic>mechanistic perspective</span> focused on describing the empirical behavior of the system; or from a <span style=font-style:italic>normative perspective</span> that looks for the functional reasons (organization principles) that explain the behavior. While the mechanistic perspective is based in experimental recordings from <strong>Psychophysics</strong> and <strong>Neurophysiology</strong>, the normative perspective is based on the study of <strong>Image Statistics</strong> and the use of concepts from <strong>Information Theory</strong> and <strong>Statistical Learning</strong>.
The latter is known as the <a href=https://en.wikipedia.org/wiki/Efficient_coding_hypothesis>Efficient Coding Hypothesis</a>.<br>Over the years we have done original work in <span style=font-style:italic>all</span> the above subdisciplines related to (low-level) Vision Science. Now we are shifting to more abstract visual functions.</p><div class=list-group><a class=list-group-item href=#Experiments><h4 class=list-group-item-heading style=color:#0096c8>Experiments in Vision Science:</h4><ul><li>Physiological Optics</li><li>Psychophysics</li><li>Neural Response (fMRI)</li><li>Image Statistics (spectrally, chromatically and spatio-temporally calibrated scenes)</li></ul></a><a class=list-group-item href=#Empirical><h4 class=list-group-item-heading style=color:#0096c8>Theory. Empirical models in Vision Science:</h4><ul><li><span style=font-style:italic>Texture Vision</span> models based on contrast discrimination.</li><li><span style=font-style:italic>Color Vision</span> models: tristimulus colorimetry and color adaptation and appearance.</li><li><span style=font-style:italic>Motion Perception</span>: speed sensitive neurons and optical flow.</li><li><span style=font-style:italic>Mathematical Techniques:</span> linear filter banks, wavelets and divisive normalization.</li></ul></a><a class=list-group-item href=#Principled><h4 class=list-group-item-heading style=color:#0096c8>Theory. Principled models in Vision Science:</h4><ul><li>Receptive fields from linear infomax and manifold matching.</li><li>Adaptation, Saturation and After-effects from nonlinear infomax and quantization.</li></ul></a><a class=list-group-item href=#Statistics><h4 class=list-group-item-heading style=color:#0096c8>Theory. Statistical Learning for Vision Science:</h4><ul><li>Linear and nonlinear manifold learning: feature extraction and dimens. reduction.</li><li>Probability Density Estimation.</li><li>Metric Learning.</li><li>Multi-information and Redundancy.</li></ul></a></div></div><div class="col-md-2 tres"><div><img class=img-responsive src=/images/adicionales/anim_gif.gif style=width:135%;margin-left:-10px><p><small>fMRI recordings at Hospital La Fe <a href="https://www.acim.lafe.san.gva.es/acim/?page_id=1229">NeuroImage Unit</a></small>.</p></div><br><div><img class=img-responsive src=/images/adicionales/anim_gif_texture.gif style=width:100%><p><small>Texture Vision [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/JMO97.PS.gz>J.Mod.Opt.97</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/Malo_Laparra_Neural_10b.pdf>Neur.Comp.10</a>]</small></p></div><div><img class=img-responsive src=/images/adicionales/anim_gif_color.gif style=width:100%><p><small>Color vision [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/VISRES97.PS.gz>Vis.Res.97</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/josa_04.pdf>JOSA04</a>]</small></p></div><div><img class=img-responsive src=/images/adicionales/anim_gif_motion2.gif style=width:100%><p><small>Motion vision [<a href=http://journalofvision.org/1/3/309/>J.Vis.01</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/ieeeoct01.pdf>IEEE TIP01</a> ]</small></p><br></div></div></div><div><div class=content-container style=width:78%><p align=justify><h3>Experiments in Vision Science</h3></p><p align=justify>I made experimental contributions in three aspects: <span style=font-style:italic>Physiological Optics, Psychophysics</span> and <span style=font-style:italic>Image Statistics</span>. (i) In the field of <strong>Physiological Optics</strong>, we measured the optical transfer function of the lens+cornea system in-vivo [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/OPH97.PS.gz>Opth.Phys.Opt.97</a>]. This work received the European Vistakon Research Award 94'. (ii) In <strong>Psychophysics</strong>, we proposed simplified methods to measure the Contrast Sensitivity Function in all the frequency domain [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/JOPT94.PS.gz>J.Opt.94</a>], and a fast and accurate method to measure the parameters of multi-stage linear+nonlinear vision models [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/malo15a-reprint.pdf>Proc.SPIE15</a>]. Finally, (iii) in <strong>Image Statistics</strong> we gathered spatially and spectrally calibrated image samples to determine the properties of these signals and their variation under changes in illumination, contrast and motion [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/ivc99.ps.gz>Im.Vis.Comp.00</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/Neco_accepted_2012.pdf>Neur.Comp.12</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/manuscr_TGRS_2012_00431.pdf>IEEE-TGRS14</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/Gutmann_PLOS_ONE_2014.pdf>PLoS-ONE14</a>, <a href=http://isp.uv.es/files/rem_sens_im_proc_12_ch02.pdf>Rem.Sens.Im.Proc.11</a>, <a href=/old_pages/data/after_effects/>Front.Neurosci.15</a>].</p><div><img alt=DoblePaso class=img-responsive src=/images/research/visual_neuroscience/experiment1.webp style=width:47%>
<img alt=IllumD class=img-responsive src=/images/research/visual_neuroscience/method1.webp style=width:26%>
<img alt=IllumA class=img-responsive src=/images/research/visual_neuroscience/method2.webp style=width:26%></div><br><p align=justify style=font-size:85%><strong style=font-weight:700;color:#00a000>Illustrative experimental equipment:</strong>
<small><span style=font-style:italic>Left:</span> double-pass setting for the measurement of the Modulation Transfer Function of the human eye [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/OPH97.PS.gz>Opth.Phys.Opt.97</a>]. <span style=font-style:italic>Right:</span> Spectrally calibrated light sources, image colorimeter and spectroradiometer to gather accurate color image statistics, see the available <a href=/old_pages/data/calibrated_color_image/subpages/data_color.html>color image database</a> [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/Neco_accepted_2012.pdf>Neur.Comp.12</a>,
<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/Gutmann_PLOS_ONE_2014.pdf>PLoS-ONE14</a>], and <a href=/old_pages/data/after_effects//>texture and motion datasets</a> [<a href=/old_pages/data/after_effects/>Front.Neurosci.15</a>], with samples ready to be processed.</small></p><hr><br><p align=justify><h3>Theory: empirical models in Vision Science</h3></p><p align=justify>We proposed mathematical descriptions of different visual dimensions: <span style=font-style:italic>Texture, <span style=font-style:italic>Color, and <span style=font-style:italic>Motion. (i) we used wavelet representations to propose nonstationary <strong>Texture Vision</strong> models [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/JMO97.PS.gz>J.Mod.Opt.97</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/msc_jmalo.pdf>MScThesis95</a>], (ii) we developed <strong>Color Vision</strong> models with illumination invariance that allow the reproduction of chromatic anomalies, adaptation and aftereffects [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/VISRES97.PS.gz>Vis.Res.97</a>,
<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/JOPT96.PS.gz>J.Opt.96</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/JOPT98.PS.gz>J. Opt.98</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/josa_04.pdf>JOSA04</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/Neco_accepted_2012.pdf>Neur.Comp.12</a>], and (iii) <strong>Motion Vision</strong> models [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/Malo_Alheteia_08.pdf>Alheteia08</a>] that focus the optical flow computation in perceptually relevant moving regions [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/vss_poster.eps>J.Vis.01</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/Redundancy_Reduction_Malo_99.pdf>PhDThesis99</a>], and explain the <span style=font-style:italic>static motion aftereffect [<span lang=EN-US style=font-size:11pt;line-height:115%;font-family:Arial,sans-serif><a href=/old_pages/data/after_effects/>Front.Neurosci.15</a>]. All these psychophysical and physiological models have a parallel <span style=font-style:italic>linear+nonlinear structure where <strong>receptive fields</strong> and <strong>surround-dependent normalization</strong> play an important role.</span></span></span></span></span></span></p></div><div class=content-container style=width:85%;margin-top:-10px><div class=row><div class=col-md-6><p align=justify style=font-size:85%><strong><span style=color:#00a000>Empirical motion model at work:</span></strong>
<small>Waving hands sequence recorded at my lab (just a remake of the original movie from <a href=http://vision.arc.nasa.gov/publications/ModelHumanVisualMotion.pdf>Watson & Ahumada</a>), linear filter model of MT neurons as an aggregate of spatio-temporal wavelet-like filters (bottom left) tuned to certain speed for optical flow computation (example at bottom right for a later frame).
<span style=font-style:italic>Note that our remake improved the original by including a striped costume for Fourier-obsessed freaks!</span></small></p><img class=img-responsive src=/images/research/visual_neuroscience/motion.webp style=max-width:100%></div><div class=col-md-6><img class=img-responsive src=/images/research/visual_neuroscience/dicromat.webp style=max-width:100%><p align=justify style=font-size:85%><strong><span style=color:#00a000>See how it feels to be color blind!:Â </span></strong>
<small>We proposed a way to simulate the perception of color blinds (here with Picasso's <a href=https://en.wikipedia.org/wiki/Dora_Maar>Dora Maar</a>). As you see, dichromats are not color blind at all: <span style=font-style:italic>they simply see different colors</span>. Moreover, this simulation can be used to discriminate between color theories just by asking your dichromat friend which image is more similar to him.</small></p></div></div><p align=center><small>See additional CODE for <a href="/old_pages/code/soft_visioncolor/ISP - Vision and color processing software.html"><span style=font-style:italic>Texture</span>, <span style=font-style:italic>Color</span> and <span style=font-style:italic>Motion</span> perception</a></small></p><hr id=principled><br></div><div class=content-container style=width:78%><p align=justify><h3>Theory: principled models in Vision Science</h3></p><p align=justify>This category refers to the proposition of organization laws of sensory systems that explain the empirical phenomena. These principles show that neural function has been adapted to (or is determined by) the statistics of visual stimuli. In this regard, (i) we worked on the <strong>derivation of the linear properties</strong> of the sensors, and we found that their spatio-chromatic sensitivity, the way the receptive fields change, and their phase properties, come from optimal solutions to the adaptation problem under noise constraints and manifold matching [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/Gutmann_PLOS_ONE_2014.pdf>PLoS-ONE14</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/AdaptVQ_ieeetgars_2012.pdf>IEEE-TGRS13</a>], from statistical independence requirements [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/ICANN_2011_v7.pdf>LNCS11</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/SlidesNeuroImageMeeting11.pdf>NeuroImag.Meeting11</a>], and from optimal estimation of object reflectance [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/manuscr_TGRS_2012_00431.pdf>IEEE TGRS14</a>].
(ii) We also worked on the <strong>derivation of the non-linear behavior</strong> for a variety of visual sensors (chromatic, texture, and motion sensors). We found that in all cases the nonlinearities are related to optimal information transmission (entropy maximization) and/or to error minimization in noisy systems (optimal vector quantization).
We studied this relation in the classical <span style=font-style:italic>statistics-to-perception</span> direction (deriving the nonlinearity from the regularities in the scene)
[<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/V1_from_non_linear_ICA.pdf>Network06</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/Neco_accepted_2012.pdf>Neur.Comp.12</a>, <a href=/old_pages/data/after_effects/>Front.Neurosci.15</a>], as well as in the (more novel) <span style=font-style:italic>perception-to-statistics</span> direction, i.e. by looking at the statistical effect of perceptually motivated nonlinearities [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/JOPT95.PS.gz>J.Opt.95</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/ivc99.ps.gz>Im.Vis.Comp.00</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/spr00.ps>LNCS00</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/patt_rec03.pdf>Patt.Recog.03</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/Malo_Laparra_Neural_10b.pdf>Neur.Comp.10</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/LNAI10_malo_laparra.pdf>LNCS10</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/SlidesNeuroImageMeeting11.pdf>NeuroImag.Meeting11</a>].<div class=content-container style=width:80%;margin-top:-10px><p style=font-size:85%><strong><span style=color:#00a000>Illustrative organization principle: </span></strong><small>Optimal adaptation and information transmission with noise constraints (Higher Order Canonical Correlation) predicts shifts in oscillatory responses of gabor-like opponent spatio-chromatic receptive fields when adapted to visual scenes under different illumination (similarly to V1 neurons). See CODE <a href=/old_pages/code/soft_feature/subpages/hocca.html>here</a>.</small></p><img class=img-responsive src=/images/research/visual_neuroscience/estimulac.webp style=max-width:100%><div class=row><div class=col-md-6><img class=img-responsive src=/images/research/visual_neuroscience/resp1.webp style=max-width:100%></div><div class=col-md-6><img class=img-responsive src=/images/research/visual_neuroscience/resp2.webp style=max-width:100%></div></div></div><hr><br><p align=justify><h3>Theory: Statistical Learning for Vision Science</h3></p><p align=justify>In theoretical neuroscience the derivation of properties of biological sensors from the regularities visual scenes requires novel tools for statistical learning. In this field, we developed new techniques for <strong>unsupervised manifold learning</strong>, <strong>feature extraction</strong> (or symmetry detection in datasets), <strong>dimensionality reduction</strong>, <strong>probability density estimation</strong>, <strong>multi-information estimation</strong>, <strong>distance learning</strong>, and automatic <strong>adaptation</strong> from optimal dataset matching. Given my interest in applicability in Vision Science problems, I focused on techniques that can be explicitly represented in the image domain to be compared with receptive fields of visual neurons, as opposed to the usual practice in the<span style=font-style:italic> Machine Learning</span> community. Techniques include Rotation-based Iterative Gaussianization -RBIG- [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/Laparra11.pdf>IEEE TNN 11</a>], Sequential Principal Curves Analysis -SPCA- [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/V1_from_non_linear_ICA.pdf>Network06</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/Neco_accepted_2012.pdf>Neur.Comp.12</a>, <a href=/old_pages/data/after_effects/>Front. Neurosci.15</a>], Principal
Polynomial Analysis -PPA- [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/IJNS_Laparra14_accepted_v5.pdf>Int.J.Neur.Syst.14</a>], Dimensionality Reduction based on Regression -DRR- [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/drr_jstsp2014_final.pdf>IEEE JSTSP15</a>], and Graph Matching for Adaptation [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/AdaptVQ_ieeetgars_2012.pdf>IEEE TGRS13</a>].<p align=justify style=width:85%;margin-left:5%;font-size:88%><span style=color:#00a000><strong>Illustrative learning technique:</strong></span><small> data induced metric and identified features using nonlinear feature extraction (in this case, the Principal Polynomial Analysis, PPA). The scatter plot (at the top) shows the training set, the 2nd row shows the features (curves in black) and the discrimination ellipsoids (in yellow) in different representation domains: the original domain (left), the data-unfolded domain after PPA (center), and the whitened PPA domain (right). Principal Polynomial Analysis looks for the intrinsic curvilinear coordinates generalizing the linear directions of PCA (principal polynomials instead of eigen directions). Similarly to Mahalanobis distance when using PCA, the Jacobian of PPA can be used to define data dependent measures. Similarly to what is done in (linear) Independent Component Analysis, the Jacobian of PPA can be used to identify intrinsic features in the input domain.
As opposed to the linear counterparts, metric and features are local.
Different sets of local features (local basis vectors) in the input domain (3rd row) or in the PPA-transformed domain (4th row) can be obtained from the Jacobian or the jacobian-related metric matrix. See available CODE for this and other feature extraction techniques <a href="/old_pages/code/soft_feature/ISP - Feature extraction software.html">here</a>.</small></p><img class=img-responsive src=/images/research/visual_neuroscience/data_metric.webp style=max-width:90%><div class=row><div class=col-md-6><img class=img-responsive src=/images/research/visual_neuroscience/features_1.webp style=max-width:140%></div><div class=col-md-6><img class=img-responsive src=/images/research/visual_neuroscience/features_2.webp style=max-width:75%;margin-left:150px;margin-top:10px></div></div><hr><br><p align=justify><h3><span class="label label-warning">References</span></h3></p><ul><li>[<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/OPH97.PS.gz>Opth.Phys.Opt.97</a>].<li>[<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/JOPT94.PS.gz>J.Opt.94</a>]<li>[<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/malo15a-reprint.pdf>Proc.SPIE15</a>]<li>[<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/ivc99.ps.gz>Im.Vis.Comp.00</a>]<li>[<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/Neco_accepted_2012.pdf>Neur.Comp.12</a>]<li>[<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/manuscr_TGRS_2012_00431.pdf>IEEE-TGRS14</a>]<li>[<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/Gutmann_PLOS_ONE_2014.pdf>PLoS-ONE14</a>]<li>[<a href=http://isp.uv.es/files/rem_sens_im_proc_12_ch02.pdf>Rem.Sens.Im.Proc.11</a>]<li>[<a href=/old_pages/data/after_effects/>Front.Neurosci.15</a>]<li>[<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/JMO97.PS.gz>J.Mod.Opt.97</a>]<li>[<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/Malo_Laparra_Neural_10b.pdf>Neur.Comp.10</a>]<li>[<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/msc_jmalo.pdf>MScThesis95</a>]<li>[<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/VISRES97.PS.gz>Vis.Res.97</a>]<li>[<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/JOPT96.PS.gz>J.Opt.96</a>]<li>[<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/JOPT98.PS.gz>J. Opt.98</a>]<li>[<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/josa_04.pdf>JOSA04</a>]<li>[<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/Malo_Alheteia_08.pdf>Alheteia08</a>]<li>[<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/vss_poster.eps>J.Vis.01</a>]<li>[<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/Redundancy_Reduction_Malo_99.pdf>PhDThesis99</a>],<li>[<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/ieeeoct01.pdf>IEEE TIP01</a>]<li>[<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/AdaptVQ_ieeetgars_2012.pdf>IEEE-TGRS13</a>]<li>[<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/ICANN_2011_v7.pdf>LNCS11</a>]<li>[<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/SlidesNeuroImageMeeting11.pdf>NeuroImag.Meeting11</a>],<li>[<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/manuscr_TGRS_2012_00431.pdf>IEEE TGRS14</a>].<li>[<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/V1_from_non_linear_ICA.pdf>Network06</a>]<li>[<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/JOPT95.PS.gz>J.Opt.95</a>]<li>[<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/ivc99.ps.gz>Im.Vis.Comp.00</a>]<li>[<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/spr00.ps>LNCS00</a>]<li>[<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/patt_rec03.pdf>Patt.Recog.03</a>]<li>[<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/LNAI10_malo_laparra.pdf>LNCS10</a>]<li>[<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/Laparra11.pdf>IEEE TNN 11</a>]<li>[<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/IJNS_Laparra14_accepted_v5.pdf>Int.J.Neur.Syst.14</a>]<li>[<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/drr_jstsp2014_final.pdf>IEEE JSTSP15</a>]<li>[<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/AdaptVQ_ieeetgars_2012.pdf>IEEE TGRS13</a>].<li>[<a href="/old_pages/code/soft_feature/ISP - Feature extraction software.html">ML CODE</a>]</li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></ul></p></p></div></div></div></main></body><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js integrity=sha384-C6RzsynM9kWDrMNeT87bh95OGNyZPhcTNXj1NW7RuBCsyN/o0jlpcV8Qyq46cDfL crossorigin=anonymous defer></script><script type=text/javascript src=https://cdn.jsdelivr.net/gh/pcooksey/bibtex-js@1.0.0/src/bibtex_js.min.js defer></script></html>