<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>ISP - Image and Signal Processing group</title>
<link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer><link rel="shortcut icon" href=/images/isp_ico.webp type=image/x-icon><link href=https://fonts.cdnfonts.com/css/twentieth-century-for-kenmore rel=stylesheet><link rel=stylesheet href=/style/style.css><script src=/js/mode.js></script><script src=https://cdn.jsdelivr.net/npm/marked/marked.min.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-D8CVQKS51G"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-D8CVQKS51G")</script></head><nav class=custom-navbars><div class=custom-container><a href=/ class="custom-logo custom-hide-on-large"><img src=/images/isp_logo_sinfondo.webp alt="ISP Icon" class=custom-logo_nav>
<span class=custom-text-isp>ISP</span>
</a><button class=navbar-toggler aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class=custom-navbar-collapse><ul class=custom-navbar-nav><li class="custom-nav-item custom-desktop-only"><a class=custom-nav-link href=/>ISP</a></li><li class=custom-nav-item><a class=custom-nav-link href=/people/>People</a></li><li class="custom-nav-item custom-dropdown"><a class="custom-nav-link custom-dropdown-toggle" href=/research/philosophy/>Research</a><ul class=custom-dropdown-menu><li><a class=custom-dropdown-item href=/research/philosophy/>Philosophy</a></li><li><a class=custom-dropdown-item href=/research/machine_learning/>Machine learning</a></li><li><a class=custom-dropdown-item href=/research/visual_neuroscience/>Visual science</a></li><li><a class=custom-dropdown-item href=/research/visual_brain/>Image processing</a></li><li><a class=custom-dropdown-item href=/research/earth_science/>Earth science</a></li><li><a class=custom-dropdown-item href=/research/social_science>Social science</a></li></ul></li><li class=custom-nav-item><a class=custom-nav-link href=/projects/>Projects</a></li><li class=custom-nav-item><a class=custom-nav-link href=/facilities/>Facilities</a></li><li class="custom-nav-item custom-dropdown"><a class="custom-nav-link custom-dropdown-toggle" href=/publications/journals/>Publications</a><ul class=custom-dropdown-menu><li><a class=custom-dropdown-item href=/publications/journals/>Journals</a></li><li><a class=custom-dropdown-item href=/publications/conferences/>Conferences</a></li><li><a class=custom-dropdown-item href=/publications/books/>Books</a></li><li><a class=custom-dropdown-item href=/publications/talks/>Talks</a></li><li><a class=custom-dropdown-item href=/publications/theses/>Theses</a></li></ul></li><li class=custom-nav-item><a class=custom-nav-link href=/code/>Code</a></li><li class=custom-nav-item><a class=custom-nav-link href=/data/>Data</a></li><li class=custom-nav-item><a class=custom-nav-link href=/seminars/>Seminars</a></li><li class=custom-nav-item><a class=custom-nav-link href=/courses/>Courses</a></li><li class=custom-nav-item><a class=custom-nav-link href=/collaborators/>Collaborators</a></li><li class=custom-nav-item><a class=custom-nav-link href=/news/>News</a></li><li class=custom-nav-item><a class=custom-nav-link href=/contact/>Contact</a></li></ul></div></div></nav><main><div class=container><div class="content-container grid-layout"><div class=box-header><h1>Vision Science: from Optics to Neuroscience and Statistical Learning</h1></div><div class=box-abstract><p><p><strong>Vision</strong> is the ability to interpret the surrounding environment by analyzing the measurements drawn by imaging systems. This ability is particularly impressive in <em>humans</em> compared to the current state of the art in <em>computers</em>.</p><p>The study of all phenomena related to <em>vision in biological systems</em> (particularly in humans) is usually referred to as <strong>Vision Science</strong>. It addresses a variety of issues ranging from the formation of the visual signal, such as the physics of the imaging process, which includes Radiometry and <strong>Physiological Optics</strong>â€”to the analysis of the visual signal, which is of interest for Neuroscience and Psychology.</p><p>This analysis involves the extraction of visual primitives through basic computations in the retina-cortex neural pathway and the subsequent information processing that leads to scene descriptors of higher abstraction levels (<a href=http://www.scholarpedia.org/article/Models_of_visual_cortex>see elsewhere</a>). These problems can be approached from different perspectives:</p><ul><li>A <em>mechanistic perspective</em>, which focuses on describing the empirical behavior of the system, based on experimental recordings from <strong>Psychophysics</strong> and <strong>Neurophysiology</strong>.</li><li>A <em>normative perspective</em>, which looks for the functional reasons (organization principles) that explain the behavior. This perspective relies on the study of <strong>Image Statistics</strong> and the use of concepts from <strong>Information Theory</strong> and <strong>Statistical Learning</strong>.</li></ul><p>The latter is known as the [<a href=https://en.wikipedia.org/wiki/Efficient_coding_hypothesis>Efficient Coding Hypothesis</a>].</p><p>Over the years, we have made original contributions in <em>all</em> of the above subdisciplines related to (low-level) Vision Science. Currently, we are shifting our focus to more abstract visual functions.</p><h1 id=experiments-in-vision-science>Experiments in Vision Science</h1><p>I made experimental contributions in three aspects: <em>Physiological Optics, Psychophysics</em>, and <em>Image Statistics</em>.</p><ol><li><p>In the field of <strong>Physiological Optics</strong>, we measured the optical transfer function of the lens+cornea system in-vivo [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/OPH97.PS.gz>Opth.Phys.Opt.97</a>]. This work received the European Vistakon Research Award in 1994.</p></li><li><p>In <strong>Psychophysics</strong>, we proposed simplified methods to measure the Contrast Sensitivity Function across the entire frequency domain [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/JOPT94.PS.gz>J.Opt.94</a>], and developed a fast and accurate method to measure the parameters of multi-stage linear+nonlinear vision models [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/malo15a-reprint.pdf>Proc.SPIE15</a>].</p></li><li><p>In <strong>Image Statistics</strong>, we gathered spatially and spectrally calibrated image samples to determine the properties of these signals and their variations under changes in illumination, contrast, and motion [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/ivc99.ps.gz>Im.Vis.Comp.00</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/Neco_accepted_2012.pdf>Neur.Comp.12</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/manuscr_TGRS_2012_00431.pdf>IEEE-TGRS14</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/Gutmann_PLOS_ONE_2014.pdf>PLoS-ONE14</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/people/malo/rem_sens_im_proc_12_ch02.pdf>Rem.Sens.Im.Proc.11</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/after_effects>Front.Neurosci.15</a>].</p></li></ol><h1 id=theory-empirical-models-in-vision-science>Theory: empirical models in Vision Science</h1><p>We proposed mathematical descriptions of different visual dimensions: <em>Texture</em>, <em>Color</em>, and <em>Motion</em>.</p><ol><li><p>We used wavelet representations to propose nonstationary <strong>Texture Vision</strong> models [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/JMO97.PS.gz>J.Mod.Opt.97</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/msc_jmalo.pdf>MScThesis95</a>].</p></li><li><p>We developed <strong>Color Vision</strong> models with illumination invariance, which allow for the reproduction of chromatic anomalies, adaptation, and aftereffects [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/VISRES97.PS.gz>Vis.Res.97</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/JOPT96.PS.gz>J.Opt.96</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/JOPT98.PS.gz>J.Opt.98</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/josa_04.pdf>JOSA04</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/Neco_accepted_2012.pdf>Neur.Comp.12</a>].</p></li><li><p>We created <strong>Motion Vision</strong> models [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/Malo_Alheteia_08.pdf>Alheteia08</a>] that focus on optical flow computation in perceptually relevant moving regions [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/vss_poster.eps>J.Vis.01</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/Redundancy_Reduction_Malo_99.pdf>PhDThesis99</a>], and explain the <em>static motion aftereffect</em> [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/after_effects>Front.Neurosci.15</a>].</p></li></ol><p>All these psychophysical and physiological models have a parallel <em>linear+nonlinear</em> structure where <strong>receptive fields</strong> and <strong>surround-dependent normalization</strong> play an important role.</p><h1 id=theory-principled-models-in-vision-science>Theory: principled models in Vision Science</h1><p>This category refers to the proposition of organizational laws of sensory systems that explain empirical phenomena. These principles demonstrate that neural function has been adapted to (or is determined by) the statistics of visual stimuli.</p><ol><li><p><strong>Derivation of Linear Properties</strong>: We worked on deriving the linear properties of the sensors and found that their spatio-chromatic sensitivity, changes in receptive fields, and phase properties arise from optimal solutions to the adaptation problem under noise constraints and manifold matching [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/Gutmann_PLOS_ONE_2014.pdf>PLoS-ONE14</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/AdaptVQ_ieeetgars_2012.pdf>IEEE-TGRS13</a>]. These properties are also derived from statistical independence requirements [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/ICANN_2011_v7.pdf>LNCS11</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/SlidesNeuroImageMeeting11.pdf>NeuroImag.Meeting11</a>], and from optimal estimation of object reflectance [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/manuscr_TGRS_2012_00431.pdf>IEEE TGRS14</a>].</p></li><li><p><strong>Derivation of Non-Linear Behavior</strong>: We also derived the non-linear behavior for a variety of visual sensors (chromatic, texture, and motion sensors). We found that these nonlinearities are linked to optimal information transmission (entropy maximization) and/or error minimization in noisy systems (optimal vector quantization).</p></li></ol><ul><li><p>We studied this relationship in the traditional <em>statistics-to-perception</em> direction, deriving the nonlinearity from regularities in the scene [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/V1_from_non_linear_ICA.pdf>Network06</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/Neco_accepted_2012.pdf>Neur.Comp.12</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/after_effects>Front.Neurosci.15</a>].</p></li><li><p>We also explored the (more novel) <em>perception-to-statistics</em> direction, examining the statistical effects of perceptually motivated nonlinearities [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/JOPT95.PS.gz>J.Opt.95</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/ivc99.ps.gz>Im.Vis.Comp.00</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/spr00.ps>LNCS00</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/patt_rec03.pdf>Patt.Recog.03</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/Malo_Laparra_Neural_10b.pdf>Neur.Comp.10</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/LNAI10_malo_laparra.pdf>LNCS10</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/SlidesNeuroImageMeeting11.pdf>NeuroImag.Meeting11</a>].</p></li></ul><h1 id=theory-statistical-learning-for-vision-science>Theory: Statistical Learning for Vision Science</h1><p>In theoretical neuroscience, deriving properties of biological sensors from the regularities in visual scenes requires novel tools for statistical learning. In this field, we developed new techniques for <strong>unsupervised manifold learning</strong>, <strong>feature extraction</strong> (or symmetry detection in datasets), <strong>dimensionality reduction</strong>, <strong>probability density estimation</strong>, <strong>multi-information estimation</strong>, <strong>distance learning</strong>, and automatic <strong>adaptation</strong> from optimal dataset matching.</p><p>Given my interest in applicability to Vision Science problems, I focused on techniques that can be explicitly represented in the image domain, to be compared with receptive fields of visual neurons, as opposed to the usual practice in the <em>Machine Learning</em> community. Techniques include:</p><ul><li><strong>Rotation-based Iterative Gaussianization (RBIG)</strong> [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/Laparra11.pdf>IEEE TNN 11</a>].</li><li><strong>Sequential Principal Curves Analysis (SPCA)</strong> [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/V1_from_non_linear_ICA.pdf>Network06</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/Neco_accepted_2012.pdf>Neur.Comp.12</a>, <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/after_effects>Front. Neurosci.15</a>].</li><li><strong>Principal Polynomial Analysis (PPA)</strong> <a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/IJNS_Laparra14_accepted_v5.pdf>Int.J.Neur.Syst.14</a></li><li><strong>Dimensionality Reduction based on Regression (DRR)</strong> [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/drr_jstsp2014_final.pdf>IEEE JSTSP15</a>].</li><li><strong>Graph Matching for Adaptation</strong> [<a href=https://huggingface.co/datasets/isp-uv-es/Web_site_legacy/resolve/main/research/visual_neuroscience/AdaptVQ_ieeetgars_2012.pdf>IEEE TGRS13</a>.]</li></ul></p></div><aside class=box-gallery><div class=gallery-grid><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/research/vision_science.webp","Vision Science","The PSF of this nice human eye can be measured in-vivo. Related publication: Opth.Phys.Opt.97")'><img src=/images/research/vision_science.webp alt="Vision Science"></a><p class=gallery-title>Vision Science</p><div class=gallery-description><p>The PSF of this nice human eye can be measured in-vivo. Related publication: Opth.Phys.Opt.97</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/research/anim_gif_v1_resp.gif","Linear V1 Cells","Linear response of V1 cells (see model here: Front.Neurosci.15)")'><img src=/images/research/anim_gif_v1_resp.gif alt="Linear V1 Cells"></a><p class=gallery-title>Linear V1 Cells</p><div class=gallery-description><p>Linear response of V1 cells (see model here: Front.Neurosci.15)</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/research/anim_gif_mt_resp.gif","Linear MT Cells","Linear response of MT cells (see the CODE for V1 and MT cells HERE)")'><img src=/images/research/anim_gif_mt_resp.gif alt="Linear MT Cells"></a><p class=gallery-title>Linear MT Cells</p><div class=gallery-description><p>Linear response of MT cells (see the CODE for V1 and MT cells HERE)</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/research/anim_gif.gif","fMRI Recordings","fMRI recordings at Hospital La Fe (NeuroImage Unit)")'><img src=/images/research/anim_gif.gif alt="fMRI Recordings"></a><p class=gallery-title>fMRI Recordings</p><div class=gallery-description><p>fMRI recordings at Hospital La Fe (NeuroImage Unit)</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/research/anim_gif_texture.gif","Texture Vision","Texture Vision analysis. Related publications: J.Mod.Opt.97, Neur.Comp.10")'><img src=/images/research/anim_gif_texture.gif alt="Texture Vision"></a><p class=gallery-title>Texture Vision</p><div class=gallery-description><p>Texture Vision analysis. Related publications: J.Mod.Opt.97, Neur.Comp.10</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/research/anim_gif_color.gif","Color Vision","Color vision study involving chromatic anomalies, adaptation, and aftereffects. Related publications: Vis.Res.97, JOSA04")'><img src=/images/research/anim_gif_color.gif alt="Color Vision"></a><p class=gallery-title>Color Vision</p><div class=gallery-description><p>Color vision study involving chromatic anomalies, adaptation, and aftereffects. Related publications: Vis.Res.97, JOSA04</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/research/anim_gif_motion2.gif","Motion Vision","Motion vision study, focusing on the computation of optical flow in perceptually relevant moving regions. Related   publications: J.Vis.01, IEEE TIP01")'><img src=/images/research/anim_gif_motion2.gif alt="Motion Vision"></a><p class=gallery-title>Motion Vision</p><div class=gallery-description><p>Motion vision study, focusing on the computation of optical flow in perceptually relevant moving regions. Related publications: J.Vis.01, IEEE TIP01</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/research/experiment1.webp","[Double-pass Measurement Setup](#experiments-in-vision-science)","Double-pass setting for measuring the Modulation Transfer Function of the human eye. Related publication: Opth.Phys.Opt.97")'><img src=/images/research/experiment1.webp alt="[Double-pass Measurement Setup](#experiments-in-vision-science)"></a><p class=gallery-title><a href=#experiments-in-vision-science>Double-pass Measurement Setup</a></p><div class=gallery-description><p>Double-pass setting for measuring the Modulation Transfer Function of the human eye. Related publication: Opth.Phys.Opt.97</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/research/method1.webp","[Spectrally Calibrated Light Sources](#experiments-in-vision-science)","Spectrally calibrated light sources used for gathering accurate color image statistics")'><img src=/images/research/method1.webp alt="[Spectrally Calibrated Light Sources](#experiments-in-vision-science)"></a><p class=gallery-title><a href=#experiments-in-vision-science>Spectrally Calibrated Light Sources</a></p><div class=gallery-description><p>Spectrally calibrated light sources used for gathering accurate color image statistics</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/research/method2.webp","[Image Colorimeter and Spectroradiometer](#experiments-in-vision-science)","Image colorimeter and spectroradiometer used for accurate measurements in visual experiments")'><img src=/images/research/method2.webp alt="[Image Colorimeter and Spectroradiometer](#experiments-in-vision-science)"></a><p class=gallery-title><a href=#experiments-in-vision-science>Image Colorimeter and Spectroradiometer</a></p><div class=gallery-description><p>Image colorimeter and spectroradiometer used for accurate measurements in visual experiments</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/research/motion.webp","[Empirical Motion Model](#theory-empirical-models-in-vision-science)","Waving hands sequence demonstrating the empirical motion model based on spatio-temporal wavelet-like filters")'><img src=/images/research/motion.webp alt="[Empirical Motion Model](#theory-empirical-models-in-vision-science)"></a><p class=gallery-title><a href=#theory-empirical-models-in-vision-science>Empirical Motion Model</a></p><div class=gallery-description><p>Waving hands sequence demonstrating the empirical motion model based on spatio-temporal wavelet-like filters</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/research/dicromat.webp","[Color Blind Simulation](#theory-empirical-models-in-vision-science)","Simulation of color blindness with Picasso&#39;s Dora Maar to illustrate how dichromats perceive colors differently")'><img src=/images/research/dicromat.webp alt="[Color Blind Simulation](#theory-empirical-models-in-vision-science)"></a><p class=gallery-title><a href=#theory-empirical-models-in-vision-science>Color Blind Simulation</a></p><div class=gallery-description><p>Simulation of color blindness with Picasso&rsquo;s Dora Maar to illustrate how dichromats perceive colors differently</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/research/estimulac.webp","[Optimal Adaptation and Information Transmission](#theory-principled-models-in-vision-science)","Illustrative organization principle: optimal adaptation and information transmission under noise constraints")'><img src=/images/research/estimulac.webp alt="[Optimal Adaptation and Information Transmission](#theory-principled-models-in-vision-science)"></a><p class=gallery-title><a href=#theory-principled-models-in-vision-science>Optimal Adaptation and Information Transmission</a></p><div class=gallery-description><p>Illustrative organization principle: optimal adaptation and information transmission under noise constraints</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/research/resp1.webp","[Response Example 1](#theory-principled-models-in-vision-science)","Illustration of response shifts in V1 neurons under different visual scene illumination conditions")'><img src=/images/research/resp1.webp alt="[Response Example 1](#theory-principled-models-in-vision-science)"></a><p class=gallery-title><a href=#theory-principled-models-in-vision-science>Response Example 1</a></p><div class=gallery-description><p>Illustration of response shifts in V1 neurons under different visual scene illumination conditions</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/research/resp2.webp","[Response Example 2](#theory-principled-models-in-vision-science)","Illustration of response changes in V1 neurons due to optimal adaptation to varying visual stimuli")'><img src=/images/research/resp2.webp alt="[Response Example 2](#theory-principled-models-in-vision-science)"></a><p class=gallery-title><a href=#theory-principled-models-in-vision-science>Response Example 2</a></p><div class=gallery-description><p>Illustration of response changes in V1 neurons due to optimal adaptation to varying visual stimuli</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/research/data_metric.webp","[Principal Polynomial Analysis Example](#theory-statistical-learning-for-vision-science)","Illustrative example showing PPA application in feature extraction and metric definition in the dataset")'><img src=/images/research/data_metric.webp alt="[Principal Polynomial Analysis Example](#theory-statistical-learning-for-vision-science)"></a><p class=gallery-title><a href=#theory-statistical-learning-for-vision-science>Principal Polynomial Analysis Example</a></p><div class=gallery-description><p>Illustrative example showing PPA application in feature extraction and metric definition in the dataset</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/research/features_1.webp","[PPA Features - Input Domain](#theory-statistical-learning-for-vision-science)","Local features obtained in the input domain using Principal Polynomial Analysis (PPA)")'><img src=/images/research/features_1.webp alt="[PPA Features - Input Domain](#theory-statistical-learning-for-vision-science)"></a><p class=gallery-title><a href=#theory-statistical-learning-for-vision-science>PPA Features - Input Domain</a></p><div class=gallery-description><p>Local features obtained in the input domain using Principal Polynomial Analysis (PPA)</p></div></div><div class=gallery-item><a href=javascript:void(0); class=gallery-thumbnail onclick='openModal("/images/research/features_2.webp","[PPA Features - Transformed Domain](#theory-statistical-learning-for-vision-science)","Local features visualized in the PPA-transformed domain")'><img src=/images/research/features_2.webp alt="[PPA Features - Transformed Domain](#theory-statistical-learning-for-vision-science)"></a><p class=gallery-title><a href=#theory-statistical-learning-for-vision-science>PPA Features - Transformed Domain</a></p><div class=gallery-description><p>Local features visualized in the PPA-transformed domain</p></div></div><div id=imageModal class=modal><span class=modal-close onclick=closeModal()>&#215;</span>
<img class=modal-content id=modalImage><div id=modalCaption class=modal-caption></div></div></div></aside><section class="box-enlaces title2"><h2>Download</h2><ul class=enlaces-list><li><a href=http://www.scholarpedia.org/article/Models_of_visual_cortex>Scholarpedia: Models of Visual Cortex</a></li><li><a href=https://en.wikipedia.org/wiki/Efficient_coding_hypothesis>Efficient Coding Hypothesis</a></li><li><a href="https://www.acim.lafe.san.gva.es/acim/?page_id=1229">NeuroImage Unit</a></li><li><a href=../../code/vision_and_color/aftereffects/content/>Texture Vision Dataset</a></li><li><a href=../../code/vision_and_color/>Vision and Color Processing Software</a></li></ul></section></div></div></main></body><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js integrity=sha384-C6RzsynM9kWDrMNeT87bh95OGNyZPhcTNXj1NW7RuBCsyN/o0jlpcV8Qyq46cDfL crossorigin=anonymous defer></script></html>